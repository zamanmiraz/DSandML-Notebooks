{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zamanmiraz/DSandML-Notebooks/blob/main/Foundational_Math_for_Generative_AI/Chapter1Demo2_AttentionSentiment_pt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUXyqlUmcwpC"
      },
      "outputs": [],
      "source": [
        "! pip install gensim\n",
        "! pip install torch==2.3.0 torchtext==0.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "769JinVVjf6y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I1zZ5U_b3RA"
      },
      "source": [
        "# Loading the dataset from Hugging Face\n",
        "*imdb* dataset features contain 'text' and 'label'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xpe3SUEmrvO"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('imdb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6YscD0Hd9hu"
      },
      "source": [
        "## 1. Train Word2Vec embeddings\n",
        " - *word2Vec* is a model that learns to represents the word as vectors\n",
        " - Similar words end up with similar vectors\n",
        " - Parameters: sentences - a list of tokenized sentences, vector_size - # of dimensions of word vectors (higher dimension leads to heavier to compute), window - how many words before and after a target word to look at, min_count - Ignores word that appear fewer than this number of times, workers - # of cpu core to train\n",
        "\n",
        " ## 2. Create embedding matrix and Turn it into embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60qfcG1Pm00r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "max_vocab_size = 25000\n",
        "max_seq_len = 50\n",
        "embedding_dim = 150\n",
        "latent_dim = 512\n",
        "output_dim = 2\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Train Word2Vec embeddings\n",
        "# -------------------------------\n",
        "sentences = [text.split() for text in dataset['train']['text']]\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=embedding_dim,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Build vocabulary with torchtext\n",
        "# -------------------------------\n",
        "def yield_tokens(sentences):\n",
        "    for sent in sentences:\n",
        "        yield sent\n",
        "\n",
        "# build the vocab\n",
        "vocab = build_vocab_from_iterator(\n",
        "    yield_tokens(sentences),\n",
        "    max_tokens=max_vocab_size,\n",
        "    specials=['<unk>', '<pad>']\n",
        ")\n",
        "\n",
        "# set default index\n",
        "unk_idx = vocab['<unk>']\n",
        "pad_idx = vocab['<pad>']\n",
        "vocab.set_default_index(unk_idx)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Create embedding matrix\n",
        "# -------------------------------\n",
        "def create_embedding_matrix(vocab, word2vec_model, embedding_dim):\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
        "    for word, idx in vocab.get_stoi().items():\n",
        "        if word in word2vec_model.wv:\n",
        "            embedding_matrix[idx] = word2vec_model.wv[word]\n",
        "        else:\n",
        "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = create_embedding_matrix(vocab, word2vec_model, embedding_dim)\n",
        "\n",
        "# Convert to torch tensor for nn.Embedding\n",
        "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Define Embedding Layer\n",
        "# -------------------------------\n",
        "embedding_layer = nn.Embedding.from_pretrained(\n",
        "    embeddings=embedding_matrix,\n",
        "    freeze=False,          # True = keep pretrained fixed, False = allow fine-tuning\n",
        "    padding_idx=pad_idx\n",
        ")\n",
        "\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "print(\"Embedding layer shape:\", embedding_layer.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "139d0cc5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Convert the dataset to map-style\n",
        "train_dataset = to_map_style_dataset(dataset['train'])\n",
        "test_dataset = to_map_style_dataset(dataset['test'])\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "# Numericalization and padding function\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for sample in batch:  # sample is a dict like {'text': ..., 'label': ...}\n",
        "        _text = sample['text']\n",
        "        _label = sample['label']\n",
        "\n",
        "        # Convert label (assumes numeric labels 0/1)\n",
        "        label_list.append(int(_label))\n",
        "\n",
        "        processed_text = torch.tensor(\n",
        "            [vocab[token] for token in tokenize_text(_text)[:max_seq_len]],\n",
        "            dtype=torch.int64\n",
        "        )\n",
        "        text_list.append(processed_text)\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_text_list = torch.nn.utils.rnn.pad_sequence(\n",
        "        text_list, batch_first=True, padding_value=pad_idx\n",
        "    )\n",
        "\n",
        "    return torch.tensor(label_list, dtype=torch.int64), padded_text_list\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "print(\"Train DataLoader created.\")\n",
        "print(\"Test DataLoader created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmfqqnCVtEq-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Define EncoderGRU\n",
        "# -------------------------------\n",
        "\n",
        "class EncoderGRU(nn.Module):\n",
        "    def __init__(self, embedding_layer, latent_dim, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = embedding_layer # embedding matrix into embedding layer\n",
        "        self.dropout = nn.Dropout(dropout_rate) # Added Dropout\n",
        "        self.rnn = nn.GRU(\n",
        "            embedding_layer.embedding_dim,\n",
        "            latent_dim,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text shape: (batch_size, seq_len)\n",
        "        embedded = self.embedding(text)\n",
        "        embedded = self.dropout(embedded) # Apply dropout after embedding\n",
        "        # embedded shape: (batch_size, seq_len, embedding_dim)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        # output = self.dropout(output) # Apply dropout after RNN\n",
        "        # output shape: (batch_size, seq_len, latent_dim)\n",
        "        # hidden shape: (1, batch_size, latent_dim)\n",
        "        # hidden.squeeze: (Batch Size and Latent Dimension)\n",
        "        return output, hidden.squeeze(0)\n",
        "\n",
        "encoder = EncoderGRU(embedding_layer, latent_dim)\n",
        "print(\"Encoder model created with GRU and Dropout.\")\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Define CrossAttention\n",
        "# -------------------------------\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, latent_dim, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=latent_dim, num_heads=1, batch_first=True)\n",
        "        self.normalization = nn.LayerNorm(latent_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate) # Added Dropout\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        attn_output, attn_score = self.mha(x, context, context)\n",
        "        # Dimension:\n",
        "        x = x + attn_output\n",
        "        x = self.normalization(x)\n",
        "        x = self.dropout(x) # Apply dropout after normalization\n",
        "        return x\n",
        "\n",
        "cross_attention = CrossAttention(latent_dim)\n",
        "print(\"CrossAttention model created with Dropout.\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Define Classifier (Decoder)\n",
        "# -------------------------------\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, latent_dim, output_dim, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(latent_dim, output_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        hidden_state = self.dropout(hidden_state)\n",
        "        return self.fc(hidden_state)   # raw logits\n",
        "\n",
        "classifier = Classifier(latent_dim, output_dim)\n",
        "print(\"Classifier model created with Dropout.\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Combine Encoder and Classifier\n",
        "# -------------------------------\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, encoder, cross_attention, classifier): # Added cross_attention as parameter\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.cross_attention = cross_attention # Use the passed in cross_attention module\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def forward(self, text):\n",
        "        encoder_outputs, encoder_state = self.encoder(text)\n",
        "        query = encoder_state.unsqueeze(1)    # (batch, 1, latent_dim)\n",
        "        context = encoder_outputs             # (batch, seq_len, latent_dim)\n",
        "        attn_output = self.cross_attention(query, context) # Pass through the cross_attention module\n",
        "        attn_output = attn_output[:, 0, :]   # (batch, latent_dim)\n",
        "        prediction = self.classifier(attn_output)\n",
        "        return prediction\n",
        "\n",
        "model = SentimentClassifier(encoder, cross_attention, classifier) # Pass cross_attention instance\n",
        "model.to(device) # Move model to the appropriate device (CPU or GPU)\n",
        "print(\"SentimentClassifier model created and moved to device.\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Define Loss Function and Optimizer\n",
        "# -------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)\n",
        "print(\"Loss function and Optimizer defined.\")\n",
        "\n",
        "# -------------------------------\n",
        "# 9. Training Loop\n",
        "# -------------------------------\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for labels, text in tqdm(dataloader, desc=\"Training\"):\n",
        "        labels = labels.to(device)\n",
        "        text = text.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "# -------------------------------\n",
        "# 10. Evaluation Loop\n",
        "# -------------------------------\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for labels, text in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            labels = labels.to(device)\n",
        "            text = text.to(device)\n",
        "\n",
        "            predictions = model(text)\n",
        "            loss = criterion(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(predictions, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    return epoch_loss / len(dataloader), correct_predictions / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 11. Train the model\n",
        "# -------------------------------\n",
        "N_EPOCHS = 30 # You can adjust the number of epochs\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(model, test_dataloader, criterion, device)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "        print(f\"Epoch {epoch+1}: Validation loss improved. Saving model.\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.3f}, Val Loss: {valid_loss:.3f}, Val Acc: {valid_acc:.3f}\")\n",
        "\n",
        "print(\"\\nTraining finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu6pDqXXZZXe"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Load the best trained model\n",
        "# ---------------------------\n",
        "model.load_state_dict(torch.load(\"best-model.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# ---------------------------\n",
        "# Example samples\n",
        "# ---------------------------\n",
        "samples = {\n",
        "    \"Positive\": \"This was the best movie I have ever seen.\",\n",
        "    \"Negative\": \"This was the worst movie I have ever watched.\",\n",
        "    \"Neutral\": \"The movie was okay, not great but not terrible.\",\n",
        "    \"Sarcasm\": \"Wow, this was such a masterpiece... the actors, the screenplay, I could stay for hours if it wasn't for how bad it was.\",\n",
        "    \"Irony\": \"The plot was so riveting, I couldn’t stop yawning.\"\n",
        "}\n",
        "\n",
        "# ---------------------------\n",
        "# Helper function to convert text to tensor\n",
        "# ---------------------------\n",
        "def text_to_tensor(text, vocab, max_seq_len, pad_idx):\n",
        "    tokens = text.split()\n",
        "    indexed_tokens = [vocab[token] for token in tokens[:max_seq_len]]\n",
        "    # Pad the sequence\n",
        "    padded_sequence = indexed_tokens + [pad_idx] * (max_seq_len - len(indexed_tokens))\n",
        "    return torch.tensor(padded_sequence, dtype=torch.int64)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Predict function\n",
        "# ---------------------------\n",
        "def predict(text, model, vocab, max_seq_len, pad_idx, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        tensor = text_to_tensor(text, vocab, max_seq_len, pad_idx).unsqueeze(0).to(device) # shape: (1, seq_len)\n",
        "        prediction = model(tensor)               # shape: (1, num_classes)\n",
        "        predicted_label = torch.argmax(prediction, dim=1).item()\n",
        "    return predicted_label, prediction\n",
        "\n",
        "# ---------------------------\n",
        "# Run predictions\n",
        "# ---------------------------\n",
        "for label, text in samples.items():\n",
        "    # Pass vocab, max_seq_len, and pad_idx to the predict function\n",
        "    pred_label, raw_logits = predict(text, model, vocab, max_seq_len, pad_idx, device)\n",
        "    print(f\"{label} → Predicted class: {pred_label}, Raw logits: {raw_logits}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUGDsA6IAEG6Llh5xVQplE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}