{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aed1c96",
   "metadata": {},
   "source": [
    "**Source**: [Neural Networks and Deep Learning - Chapter 1](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "\n",
    "### 🧠 Sigmoid Neuron\n",
    "\n",
    "A **sigmoid neuron** takes inputs in the range [0, 1] and outputs a value between 0 and 1, using the sigmoid activation function.\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 Neural Network Layers\n",
    "\n",
    "- **Input Layer**: Contains *input neurons*\n",
    "- **Hidden (Middle) Layer**: Contains *hidden neurons*\n",
    "- **Output Layer**: Contains *output neurons*\n",
    "\n",
    "---\n",
    "\n",
    "### 🔻 Gradient Descent\n",
    "\n",
    "Let:\n",
    "- $x$ be the training input  \n",
    "- $y = y(x)$ be the corresponding desired output  \n",
    "- $a$ be the actual output of the network  \n",
    "\n",
    "The **cost function** is defined as:\n",
    "\n",
    "$$\n",
    "C(w, b) = \\frac{1}{2n} \\sum_x \\| y(x) - a \\|^2\n",
    "$$\n",
    "\n",
    "Suppose we simplify notation by writing $(w, b) \\rightarrow (v_1, v_2)$.  \n",
    "Then the change in cost is approximately:\n",
    "\n",
    "$$\n",
    "\\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 + \\frac{\\partial C}{\\partial v_2} \\Delta v_2\n",
    "$$\n",
    "\n",
    "We want to choose $\\Delta v_1$ and $\\Delta v_2$ such that $\\Delta C < 0$ (i.e., the cost decreases).\n",
    "\n",
    "Define:\n",
    "- $\\Delta v \\equiv \\begin{pmatrix} \\Delta v_1 \\\\ \\Delta v_2 \\end{pmatrix}$\n",
    "- $\\nabla C \\equiv \\begin{pmatrix} \\frac{\\partial C}{\\partial v_1} \\\\ \\frac{\\partial C}{\\partial v_2} \\end{pmatrix}$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\Delta C \\approx \\nabla C \\cdot \\Delta v\n",
    "$$\n",
    "\n",
    "To ensure the cost decreases, we choose:\n",
    "\n",
    "$$\n",
    "\\Delta v = -\\eta \\nabla C\n",
    "$$\n",
    "\n",
    "where $\\eta$ is a small positive number called the *learning rate*. Substituting, we get:\n",
    "\n",
    "$$\n",
    "\\Delta C \\approx -\\eta \\nabla C \\cdot \\nabla C = -\\eta \\| \\nabla C \\|^2\n",
    "$$\n",
    "\n",
    "Since $\\| \\nabla C \\|^2 \\geq 0$, this guarantees:\n",
    "\n",
    "$$\n",
    "\\Delta C \\leq 0\n",
    "$$\n",
    "\n",
    "So, the cost function $C$ always decreases (or remains the same), and the update rule becomes:\n",
    "\n",
    "$$\n",
    "v \\rightarrow v' = v - \\eta \\nabla C\n",
    "$$\n",
    "\n",
    "This can be extended to functions with any number of variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9e3a3",
   "metadata": {},
   "source": [
    "Excercise 1: The goal is to minimize the cost $C$ in such a way that the cost $C$ goes down as much as possible. Let's the limit the size of the change to a fixed value, $ \\|\\Delta v\\| = \\epsilon$. As a first order of approximation $\\Delta C \\approx  \\nabla C \\cdot \\Delta v$. Now the objective is to choose a vector $\\Delta v$ of fixed length $\\epsilon$ that minimizes $\\nabla C \\cdot \\Delta v$. The dot product of $\\nabla C \\cdot \\Delta v$ is $\\|\\nabla C\\|\\cdot \\|\\Delta v\\| \\cos(\\theta)$. To minimize this, $\\cos(\\theta) = -1$ since $ \\|\\Delta v\\| = \\epsilon$, the smallest value of the dot product is $-\\|\\nabla C\\|\\cdot \\epsilon$, hence $\\eta = \\epsilon / \\|\\nabla C\\|$.\n",
    "\n",
    "Excecise 2: For $1D$, the \"gradient\" is just the slope of the line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a5277",
   "metadata": {},
   "source": [
    "### 🔁 Gradient Descent (Component-wise Update)\n",
    "\n",
    "When using gradient descent to minimize a cost function $C$, we update each **weight** $w_k$ and **bias** $b_l$ individually using the gradients of the cost function:\n",
    "\n",
    "$$\n",
    "w_k \\rightarrow w_k' = w_k - \\eta \\frac{\\partial C}{\\partial w_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_l \\rightarrow b_l' = b_l - \\eta \\frac{\\partial C}{\\partial b_l}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\eta$ is the **learning rate**,\n",
    "- $\\frac{\\partial C}{\\partial w_k}$ and $\\frac{\\partial C}{\\partial b_l}$ are the **partial derivatives** of the cost function with respect to each parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎲 Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Instead of computing the full gradient $\\nabla C$ over the entire dataset (which can be computationally expensive), **Stochastic Gradient Descent** estimates it using a small, randomly selected subset (mini-batch) of the training data.\n",
    "\n",
    "We approximate the true gradient:\n",
    "\n",
    "$$\n",
    "\\nabla C \\approx \\frac{1}{m} \\sum_{j=1}^{m} \\nabla C_{x_j}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $m$ is the **mini-batch size**,\n",
    "- $x_j$ is the $j$-th example in the mini-batch,\n",
    "- $\\nabla C_{x_j}$ is the gradient computed for the individual training example $x_j$.\n",
    "\n",
    "This is an approximation to the full batch gradient:\n",
    "\n",
    "$$\n",
    "\\nabla C = \\frac{1}{n} \\sum_{x} \\nabla C_x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 SGD Component-wise Update Rules\n",
    "\n",
    "Using the estimated gradient from the mini-batch, we update the weights and biases as:\n",
    "\n",
    "$$\n",
    "w_k \\rightarrow w_k' = w_k - \\frac{\\eta}{m} \\sum_{j=1}^{m} \\frac{\\partial C_{x_j}}{\\partial w_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_l \\rightarrow b_l' = b_l - \\frac{\\eta}{m} \\sum_{j=1}^{m} \\frac{\\partial C_{x_j}}{\\partial b_l}\n",
    "$$\n",
    "\n",
    "This speeds up training and helps the model generalize better due to the noise introduced by random sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23697ada",
   "metadata": {},
   "source": [
    "Backpropagation Algorithm: \n",
    "\n",
    "- Input $x$: Set the corresponding activation $a^1$ for the input layer \n",
    "- Feedforward: For each layer $l \\in [2, L]$ (layer 1 is for input) compute $z^l = w^la^{l - 1} + b^l$ and $a^l = \\sigma(z^l)$\n",
    "- Ouput Error $\\delta^L$: Compute, $\\delta^L = \\nabla_aC \\odot \\sigma'(z^L)$\n",
    "- Backpropagate the error: For each $l$ in $\\{L-1, L-2, \\cdots 2\\}$ compute $\\delta^l = ((w^{l+1})^T\\delta^{l + 1}) \\odot \\sigma'(z^l)$\n",
    "-  The gradient of the cost function is given by \n",
    "$\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l \\quad \\text{and} \\quad \\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l$\n",
    "- Gradient Descent: Update the $w^l$ and $b^l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca397b2e",
   "metadata": {},
   "source": [
    "Cross Entropy Cost Function: Four regularization methods-\n",
    "- L1 & L2 regularization \n",
    "- Dropout \n",
    "- Artificial expansion of    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548234b0",
   "metadata": {},
   "source": [
    "L2 Regularization: The idea of L2 regularization is to add an extra term to the cost function, a term called the regularization term. Here is the regularized cross-entropy:\n",
    "\n",
    "$C = \\frac{1}{n} \\sum_{xj}[y_j \\ln a_j^L + (1 - y_j) \\ln(a_j^L)] + \\frac{\\lambda}{2n}\\sum_w w^2$\n",
    "\n",
    "- First term is the usual expression for the cross entropy\n",
    "- $C = C_0 + \\frac{\\lambda}{2n}\\sum_w w^2$\n",
    "- Regularization does not chose biases\n",
    "- $W \\leftarrow (1- \\frac{\\eta\\lambda}{n})w - \\eta\\frac{\\partial C_0}{\\partial w}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feea6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import network2\n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data, monitor_evaluation_accuracy=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
