{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aed1c96",
   "metadata": {},
   "source": [
    "Sigmoid Neuron: It can takes input from 0 to 1. write the equation \n",
    "\n",
    "Input Layer: Input Neurons\n",
    "\n",
    "Hidden (Middle) Layer: Hidden Neurons\n",
    "\n",
    "Output Layer: Output Neurons\n",
    "\n",
    "Gradient Descent: x -> Traning Input, y = y(x) corresponding desired output\n",
    "    Cost function $$ C(w, b) = \\frac{1}{2n} \\sum_x \\| y(x) - a \\|^2 $$\n",
    "    Lets think $(w,b) -> (v1, v2)$\n",
    "    Change in cost function: $$ \\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 + \\frac{\\partial C}{\\partial v_2} \\Delta v_2 $$\n",
    "    Need to find a way of choosing $\\Delta v_1$ & $\\Delta v_2$ so that $\\Delta C$ is negative. Let, $\\Delta v \\equiv (\\Delta v_1, \\Delta v_2)^T $. Let also denote the gradient vector $$\\nabla C \\equiv (\\frac{\\partial C}{\\partial v_1}, \\frac{\\partial C}{\\partial v_2})$$. So, $\\Delta C \\approx  \\nabla C \\cdot \\Delta v$. As we need to get, $\\Delta C \\lt 0$, we pick $\\Delta v=-\\eta \\nabla C$, where $\\eta$ is learning rate. Hence, $\\Delta C \\approx (-\\eta \\nabla C)\\cdot \\nabla C = -\\eta \\|\\nabla C\\|^2 $ i.e., $C$ will decrease. So, $v \\rightarrow v' = v - \\eta \\nabla C$. This can be extended for any number of variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9e3a3",
   "metadata": {},
   "source": [
    "Excercise 1: The goal is to minimize the cost $C$ in such a way that the cost $C$ goes down as much as possible. Let's the limit the size of the change to a fixed value, $ \\|\\Delta v\\| = \\epsilon$. As a first order of approximation $\\Delta C \\approx  \\nabla C \\cdot \\Delta v$. Now the objective is to choose a vector $\\Delta v$ of fixed length $\\epsilon$ that minimizes $\\nabla C \\cdot \\Delta v$. The dot product of $\\nabla C \\cdot \\Delta v$ is $\\|\\nabla C\\|\\cdot \\|\\Delta v\\| \\cos(\\theta)$. To minimize this, $\\cos(\\theta) = -1$ since $ \\|\\Delta v\\| = \\epsilon$, the smallest value of the dot product is $-\\|\\nabla C\\|\\cdot \\epsilon$, hence $\\eta = \\epsilon / \\|\\nabla C\\|$.\n",
    "\n",
    "Excecise 2: For $1D$, the \"gradient\" is just the slope of the line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a5277",
   "metadata": {},
   "source": [
    "Writing out the gradient descent update rule in terms of components, we have:\n",
    "\n",
    "$$\n",
    "w_k \\rightarrow w_k' = w_k - \\eta \\frac{\\partial C}{\\partial w_k} \n",
    "$$\n",
    "\n",
    "$$\n",
    "b_l \\rightarrow b_l' = b_l - \\eta \\frac{\\partial C}{\\partial b_l} \n",
    "$$\n",
    "\n",
    "**Stochastic Gradient Descent:** Estimate the gradient $\\nabla C$ by computing  $\\nabla C_x$ for a small sample of randomly chosen training examples.\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{j=1}^{m} \\nabla C_{x_j} \\approx \\frac{1}{n} \\sum_{x} \\nabla C_{x} = \\nabla C\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_k \\rightarrow w_k' = w_k - \\frac{\\eta}{m} \\sum_j \\frac{\\partial C_{X_j}}{\\partial w_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_l \\rightarrow b_l' = b_l - \\frac{\\eta}{m} \\sum_j \\frac{\\partial C_{X_j}}{\\partial b_l}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59feec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcabdd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"Initialize the network with a list of sizes.\"\"\"\n",
    "        self.num_layers = len(sizes) # number of layers in the network\n",
    "        self.sizes = sizes # number of neurons in each layer\n",
    "        self.biases = [np.random.randn(cur, 1) for cur in sizes[1:]] # biases for each layer (except the input layer)\n",
    "        self.weights = [np.random.randn(prev, cur) for prev, cur in zip(sizes[:-1], sizes[1:])] # weights for each layer (except the input layer)\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network given input a.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"Train the network using mini batch stochastic gradient descent (SGD). The \n",
    "        training_data is a list of tuples (x, y) where x is the input and y is the expected output.\n",
    "        The test_data is a list of tuples (x, y) for evaluating the performance of the network. Other parameters\n",
    "        are epochs (number of iterations), mini_batch_size (size of each mini batch), and eta (learning rate).\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient descent using a single mini batch.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w - (eta/len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta/len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "    \n",
    "    \n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2235c9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.54174811,  0.44301886],\n",
       "        [-1.17677569, -2.97927487],\n",
       "        [-0.33045123, -0.28177142]]),\n",
       " array([[ 0.24636485, -0.45324362, -0.58637459]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([2, 3, 1]) # Example: a network with 2 input neurons, 3 hidden neurons, and 1 output neuron\n",
    "net.weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
