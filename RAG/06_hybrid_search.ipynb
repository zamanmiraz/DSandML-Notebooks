{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNT92lWUwmv+W22I92cEh8d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6407deefe3d94b67ad882ed5783229ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62edf500d4ea416e9f5eb7354810ba81",
              "IPY_MODEL_7174c1b389bc4853ace61b6072eae02c",
              "IPY_MODEL_027fb88babe64859a303e587363673d2"
            ],
            "layout": "IPY_MODEL_79e9876cce1f486b8ae1f99e8d4dff6c"
          }
        },
        "62edf500d4ea416e9f5eb7354810ba81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b06ad6f19b734ceeaa626388c567d803",
            "placeholder": "​",
            "style": "IPY_MODEL_787d75ecfa0848f989c24117bdbb92f4",
            "value": "Tokenize texts:   0%"
          }
        },
        "7174c1b389bc4853ace61b6072eae02c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_576fd526cd2749efb232ed05427fe639",
            "max": 46,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44c0de496875474c860a0bdc265060ed",
            "value": 46
          }
        },
        "027fb88babe64859a303e587363673d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50424a21fe3242af87074508b2e95276",
            "placeholder": "​",
            "style": "IPY_MODEL_55be1f24ea754b42bcaa14b5bc4e6698",
            "value": " 0/46 [00:00&lt;?, ?it/s]"
          }
        },
        "79e9876cce1f486b8ae1f99e8d4dff6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "b06ad6f19b734ceeaa626388c567d803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "787d75ecfa0848f989c24117bdbb92f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "576fd526cd2749efb232ed05427fe639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44c0de496875474c860a0bdc265060ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50424a21fe3242af87074508b2e95276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55be1f24ea754b42bcaa14b5bc4e6698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2173524d90f481fa496be7181105b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0f0ac6d6463478c97887836d2f96a6e",
              "IPY_MODEL_6305945f5d41487f8291d6c10eb75e87",
              "IPY_MODEL_d0d901fd7b6e4d688040cfda0f28c0a7"
            ],
            "layout": "IPY_MODEL_3f7e1b33e85e4edaab7cac0af2a0910e"
          }
        },
        "e0f0ac6d6463478c97887836d2f96a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edee5db4714f49a3b6b584eebe47413d",
            "placeholder": "​",
            "style": "IPY_MODEL_6910f52ee938459185731e53004b6e2e",
            "value": "BM25S Create Vocab:   0%"
          }
        },
        "6305945f5d41487f8291d6c10eb75e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a65f81dd89c47f194fffcaf3e38d2c4",
            "max": 46,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c3e2d8a6f6c451c86edd6a0ff12c790",
            "value": 46
          }
        },
        "d0d901fd7b6e4d688040cfda0f28c0a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05a4df81d1544802aac36c5f6c443a99",
            "placeholder": "​",
            "style": "IPY_MODEL_7898c3d8301946c4a7a142d0d2249696",
            "value": " 0/46 [00:00&lt;?, ?it/s]"
          }
        },
        "3f7e1b33e85e4edaab7cac0af2a0910e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "edee5db4714f49a3b6b584eebe47413d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6910f52ee938459185731e53004b6e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a65f81dd89c47f194fffcaf3e38d2c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c3e2d8a6f6c451c86edd6a0ff12c790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05a4df81d1544802aac36c5f6c443a99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7898c3d8301946c4a7a142d0d2249696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5557056c597c4c6e827383945f7b0892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_998f031b44c14320a66906893aa49b10",
              "IPY_MODEL_5bee82e64db642a88c49e32cada8a260",
              "IPY_MODEL_c6c71637fd624d8b9485dcafbfc05fe3"
            ],
            "layout": "IPY_MODEL_220bc9da159642298e6463785aec03e0"
          }
        },
        "998f031b44c14320a66906893aa49b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2607c5109ed4d57a87d6d8ab382998c",
            "placeholder": "​",
            "style": "IPY_MODEL_ceb9c9c7ab164c91ac7997ca1280a3f0",
            "value": "BM25S Convert tokens to indices:   0%"
          }
        },
        "5bee82e64db642a88c49e32cada8a260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_025e358e1f184a629778ebcfcd6279ad",
            "max": 46,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14afe357ede84a609f3aaa05f1f98c6f",
            "value": 46
          }
        },
        "c6c71637fd624d8b9485dcafbfc05fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_240c098ee87d45918e64d67781ff3842",
            "placeholder": "​",
            "style": "IPY_MODEL_e98d5c57d0a0452aa6fbb49976e31956",
            "value": " 0/46 [00:00&lt;?, ?it/s]"
          }
        },
        "220bc9da159642298e6463785aec03e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "e2607c5109ed4d57a87d6d8ab382998c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceb9c9c7ab164c91ac7997ca1280a3f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "025e358e1f184a629778ebcfcd6279ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14afe357ede84a609f3aaa05f1f98c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "240c098ee87d45918e64d67781ff3842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e98d5c57d0a0452aa6fbb49976e31956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "423dd0113c864b43b1d58857c49fc95d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36c5f6a310c644958b2b7afd32d5b0eb",
              "IPY_MODEL_73003365ab80448c9fe1bbbe6ee5999b",
              "IPY_MODEL_1ee309c81883431e9594b9e01c5e5a3a"
            ],
            "layout": "IPY_MODEL_45b376666cf34c03b20d0ef570638901"
          }
        },
        "36c5f6a310c644958b2b7afd32d5b0eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ef323f91c65415ab0c26e745562af0b",
            "placeholder": "​",
            "style": "IPY_MODEL_3570c82f68a64e4595bad6ad09928a94",
            "value": "BM25S Count Tokens:   0%"
          }
        },
        "73003365ab80448c9fe1bbbe6ee5999b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f1fafdca711439c9bd9b8f994a50efe",
            "max": 46,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4105e698dd184492b1722e8ebc10c220",
            "value": 46
          }
        },
        "1ee309c81883431e9594b9e01c5e5a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e71f8875e6ed4060b28d3121b64b76f6",
            "placeholder": "​",
            "style": "IPY_MODEL_d98c03f0fb0741f9aea1a19dfa4e1642",
            "value": " 0/46 [00:00&lt;?, ?it/s]"
          }
        },
        "45b376666cf34c03b20d0ef570638901": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "7ef323f91c65415ab0c26e745562af0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3570c82f68a64e4595bad6ad09928a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f1fafdca711439c9bd9b8f994a50efe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4105e698dd184492b1722e8ebc10c220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e71f8875e6ed4060b28d3121b64b76f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d98c03f0fb0741f9aea1a19dfa4e1642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "192908f85fcd466cb7e362a92353c9ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77d6e192defa4b7a9903bec7fea2257f",
              "IPY_MODEL_79ef5a13983242c2b7fb10af3958ac91",
              "IPY_MODEL_3af2a7a6212f44df9a2dfece056ba43f"
            ],
            "layout": "IPY_MODEL_0150cd532e89485488b69c9cc1f4a824"
          }
        },
        "77d6e192defa4b7a9903bec7fea2257f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc680664a2c84074863b7ac80607465d",
            "placeholder": "​",
            "style": "IPY_MODEL_6f945c6747eb405db6b58a9ba08b9e69",
            "value": "BM25S Compute Scores:   0%"
          }
        },
        "79ef5a13983242c2b7fb10af3958ac91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a7823ba9558431ea8fe1edf8d9e07b3",
            "max": 46,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75aba9663d46459bb985c70475a42ef1",
            "value": 46
          }
        },
        "3af2a7a6212f44df9a2dfece056ba43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8215f9e9240c4ad8a7fda0e6e3063c85",
            "placeholder": "​",
            "style": "IPY_MODEL_f8ab3237bfd947bf8c97793257a5120d",
            "value": " 0/46 [00:00&lt;?, ?it/s]"
          }
        },
        "0150cd532e89485488b69c9cc1f4a824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "fc680664a2c84074863b7ac80607465d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f945c6747eb405db6b58a9ba08b9e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a7823ba9558431ea8fe1edf8d9e07b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75aba9663d46459bb985c70475a42ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8215f9e9240c4ad8a7fda0e6e3063c85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8ab3237bfd947bf8c97793257a5120d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92e54f7b260b4d91b510016aaf9f42e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e39ed21749b4410b8ff2d1945f13314",
              "IPY_MODEL_f08fa9b4f20b4dfd85770370ba56475c",
              "IPY_MODEL_464da948d97d4f588630f29dfbc95ea5"
            ],
            "layout": "IPY_MODEL_a5b0872cc6d5441ba55f9f79a4a11d88"
          }
        },
        "4e39ed21749b4410b8ff2d1945f13314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a0bb405ae174df485851b9de570683f",
            "placeholder": "​",
            "style": "IPY_MODEL_b5f9fe26d5e5487bb7b1f32d04690a6e",
            "value": "Tokenize texts:   0%"
          }
        },
        "f08fa9b4f20b4dfd85770370ba56475c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60f5c257fec84f57aeab5c72036e3099",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00aa09feb10f48219272efc95a6ba44e",
            "value": 1
          }
        },
        "464da948d97d4f588630f29dfbc95ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1399a6c69d0e4c5c9149083177c9eab6",
            "placeholder": "​",
            "style": "IPY_MODEL_984fca3966a24d4da5b80de4abbd2cce",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "a5b0872cc6d5441ba55f9f79a4a11d88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "1a0bb405ae174df485851b9de570683f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f9fe26d5e5487bb7b1f32d04690a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60f5c257fec84f57aeab5c72036e3099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00aa09feb10f48219272efc95a6ba44e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1399a6c69d0e4c5c9149083177c9eab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "984fca3966a24d4da5b80de4abbd2cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b999216a2fc04d2494e997fe88938cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a91b72d8958448bbaf2805836a341b0c",
              "IPY_MODEL_bf9d3323451940de9a5b5917cbb1ace8",
              "IPY_MODEL_e8b949bac6d442b1963bac757e9576fb"
            ],
            "layout": "IPY_MODEL_a717a141daac4531b9e1032faa012869"
          }
        },
        "a91b72d8958448bbaf2805836a341b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b78fb9528d344939277df2e10d12b71",
            "placeholder": "​",
            "style": "IPY_MODEL_6c2a05c3f6d24d3d95cb7e7c2e69e956",
            "value": "BM25S Retrieve:   0%"
          }
        },
        "bf9d3323451940de9a5b5917cbb1ace8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_411a09855675424fa78fdf9436f497e5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4ad42365a17471f861c8cfeb2e034bf",
            "value": 1
          }
        },
        "e8b949bac6d442b1963bac757e9576fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_897b85c876a64f6fa67c81e4aadde479",
            "placeholder": "​",
            "style": "IPY_MODEL_cad2bf5da62c4c5cbbce4116f6c21409",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "a717a141daac4531b9e1032faa012869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "9b78fb9528d344939277df2e10d12b71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c2a05c3f6d24d3d95cb7e7c2e69e956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "411a09855675424fa78fdf9436f497e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4ad42365a17471f861c8cfeb2e034bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "897b85c876a64f6fa67c81e4aadde479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cad2bf5da62c4c5cbbce4116f6c21409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zamanmiraz/DSandML-Notebooks/blob/main/RAG/06_hybrid_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4pO-yZxnOTfM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61e67f89-cb95-41d3-a935-b7a9740a6a98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'advanced-rag'...\n",
            "remote: Enumerating objects: 281, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 281 (delta 4), reused 10 (delta 3), pack-reused 268 (from 1)\u001b[K\n",
            "Receiving objects: 100% (281/281), 18.84 MiB | 15.85 MiB/s, done.\n",
            "Resolving deltas: 100% (167/167), done.\n",
            "/content/advanced-rag\n",
            "Collecting accelerate==1.0.0 (from -r requirements.txt (line 3))\n",
            "  Downloading accelerate-1.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting aiohappyeyeballs==2.4.2 (from -r requirements.txt (line 5))\n",
            "  Downloading aiohappyeyeballs-2.4.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting aiohttp==3.10.5 (from -r requirements.txt (line 7))\n",
            "  Downloading aiohttp-3.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting aiosignal==1.3.1 (from -r requirements.txt (line 12))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting altair==5.4.1 (from -r requirements.txt (line 14))\n",
            "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (0.7.0)\n",
            "Collecting anthropic==0.39.0 (from -r requirements.txt (line 20))\n",
            "  Downloading anthropic-0.39.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting anyio==4.6.0 (from -r requirements.txt (line 22))\n",
            "  Downloading anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting attrs==24.2.0 (from -r requirements.txt (line 27))\n",
            "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting backoff==2.2.1 (from -r requirements.txt (line 32))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting bm25s==0.2.2 (from -r requirements.txt (line 34))\n",
            "  Downloading bm25s-0.2.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting certifi==2024.8.30 (from -r requirements.txt (line 36))\n",
            "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting charset-normalizer==3.3.2 (from -r requirements.txt (line 41))\n",
            "  Downloading charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting cohere==4.57 (from -r requirements.txt (line 43))\n",
            "  Downloading cohere-4.57-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting colorama==0.4.6 (from -r requirements.txt (line 47))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting colorlog==6.8.2 (from -r requirements.txt (line 51))\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting colpali-engine==0.3.1 (from -r requirements.txt (line 55))\n",
            "  Downloading colpali_engine-0.3.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting contourpy==1.3.0 (from -r requirements.txt (line 57))\n",
            "  Downloading contourpy-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 59)) (0.12.1)\n",
            "Collecting datasets==2.19.0 (from -r requirements.txt (line 61))\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: dill==0.3.8 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 63)) (0.3.8)\n",
            "Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 67)) (1.9.0)\n",
            "Collecting einops==0.8.0 (from -r requirements.txt (line 71))\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting fastavro==1.9.7 (from -r requirements.txt (line 73))\n",
            "  Downloading fastavro-1.9.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting filelock==3.16.1 (from -r requirements.txt (line 75))\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fonttools==4.54.1 (from -r requirements.txt (line 81))\n",
            "  Downloading fonttools-4.54.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.7/163.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist==1.4.1 (from -r requirements.txt (line 83))\n",
            "  Downloading frozenlist-1.4.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting fsspec==2024.3.1 (from -r requirements.txt (line 87))\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting gputil==1.4.0 (from -r requirements.txt (line 92))\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting grpcio==1.66.1 (from -r requirements.txt (line 94))\n",
            "  Downloading grpcio-1.66.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting grpcio-tools==1.62.3 (from -r requirements.txt (line 98))\n",
            "  Downloading grpcio_tools-1.62.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting h11==0.14.0 (from -r requirements.txt (line 100))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting h2==4.1.0 (from -r requirements.txt (line 102))\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting hf-transfer==0.1.8 (from -r requirements.txt (line 104))\n",
            "  Downloading hf_transfer-0.1.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting hpack==4.0.0 (from -r requirements.txt (line 106))\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting httpcore==1.0.5 (from -r requirements.txt (line 108))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting httpx==0.27.2 (from -r requirements.txt (line 110))\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub==0.25.1 (from -r requirements.txt (line 115))\n",
            "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting hyperframe==6.0.1 (from -r requirements.txt (line 124))\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 126)) (3.10)\n",
            "Collecting importlib-metadata==6.11.0 (from -r requirements.txt (line 132))\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting jinja2==3.1.4 (from -r requirements.txt (line 134))\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting jiter==0.5.0 (from -r requirements.txt (line 138))\n",
            "  Downloading jiter-0.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting joblib==1.4.2 (from -r requirements.txt (line 142))\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting jsonschema==4.23.0 (from -r requirements.txt (line 144))\n",
            "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting jsonschema-specifications==2023.12.1 (from -r requirements.txt (line 146))\n",
            "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting kiwisolver==1.4.7 (from -r requirements.txt (line 148))\n",
            "  Downloading kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting markdown-it-py==3.0.0 (from -r requirements.txt (line 150))\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting markupsafe==2.1.5 (from -r requirements.txt (line 152))\n",
            "  Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting matplotlib==3.9.2 (from -r requirements.txt (line 154))\n",
            "  Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 159)) (0.1.2)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 161)) (1.3.0)\n",
            "Collecting multidict==6.1.0 (from -r requirements.txt (line 163))\n",
            "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: multiprocess==0.70.16 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 167)) (0.70.16)\n",
            "Collecting narwhals==1.9.3 (from -r requirements.txt (line 169))\n",
            "  Downloading narwhals-1.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting networkx==3.3 (from -r requirements.txt (line 171))\n",
            "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting numpy==1.26.4 (from -r requirements.txt (line 173))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==1.48.0 (from -r requirements.txt (line 193))\n",
            "  Downloading openai-1.48.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting packaging==23.2 (from -r requirements.txt (line 197))\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 206)) (2.2.2)\n",
            "Collecting pdf2image==1.17.0 (from -r requirements.txt (line 211))\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting peft==0.11.1 (from -r requirements.txt (line 213))\n",
            "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pillow==10.4.0 (from -r requirements.txt (line 215))\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting portalocker==2.10.1 (from -r requirements.txt (line 221))\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting protobuf==4.25.5 (from -r requirements.txt (line 223))\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting psutil==6.0.0 (from -r requirements.txt (line 227))\n",
            "  Downloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting pyarrow==17.0.0 (from -r requirements.txt (line 232))\n",
            "  Downloading pyarrow-17.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting pyarrow-hotfix==0.6 (from -r requirements.txt (line 236))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting pydantic==2.8.2 (from -r requirements.txt (line 238))\n",
            "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.20.1 (from -r requirements.txt (line 245))\n",
            "  Downloading pydantic_core-2.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting pygments==2.18.0 (from -r requirements.txt (line 247))\n",
            "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pymupdf==1.24.13 (from -r requirements.txt (line 249))\n",
            "  Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pymupdf4llm==0.0.17 (from -r requirements.txt (line 251))\n",
            "  Downloading pymupdf4llm-0.0.17-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pyparsing==3.1.4 (from -r requirements.txt (line 253))\n",
            "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pystemmer==2.2.0.3 (from -r requirements.txt (line 255))\n",
            "  Downloading PyStemmer-2.2.0.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 257)) (2.9.0.post0)\n",
            "Collecting python-dotenv==1.0.1 (from -r requirements.txt (line 261))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting pytz==2024.2 (from -r requirements.txt (line 263))\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pyyaml==6.0.2 (from -r requirements.txt (line 265))\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting qdrant-client==1.11.3 (from -r requirements.txt (line 273))\n",
            "  Downloading qdrant_client-1.11.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting referencing==0.35.1 (from -r requirements.txt (line 275))\n",
            "  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting regex==2023.12.25 (from -r requirements.txt (line 279))\n",
            "  Downloading regex-2023.12.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests==2.32.3 (from -r requirements.txt (line 285))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting requests-mock==1.12.1 (from -r requirements.txt (line 294))\n",
            "  Downloading requests_mock-1.12.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting rich==13.8.1 (from -r requirements.txt (line 298))\n",
            "  Downloading rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting rich-theme-manager==0.11.0 (from -r requirements.txt (line 302))\n",
            "  Downloading rich_theme_manager-0.11.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting rpds-py==0.20.0 (from -r requirements.txt (line 304))\n",
            "  Downloading rpds_py-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting safetensors==0.4.5 (from -r requirements.txt (line 308))\n",
            "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting scikit-learn==1.5.2 (from -r requirements.txt (line 313))\n",
            "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting scipy==1.14.1 (from -r requirements.txt (line 317))\n",
            "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 322)) (0.13.2)\n",
            "Collecting semantic-chunkers==0.0.4 (from -r requirements.txt (line 324))\n",
            "  Downloading semantic_chunkers-0.0.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting semantic-router==0.0.37 (from -r requirements.txt (line 326))\n",
            "  Downloading semantic_router-0.0.37-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting sentence-transformers==3.0.1 (from -r requirements.txt (line 330))\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting setuptools==75.1.0 (from -r requirements.txt (line 332))\n",
            "  Downloading setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting six==1.16.0 (from -r requirements.txt (line 334))\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 336)) (1.3.1)\n",
            "Collecting stamina==24.3.0 (from -r requirements.txt (line 342))\n",
            "  Downloading stamina-24.3.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 344)) (1.13.3)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 346)) (0.9.0)\n",
            "Requirement already satisfied: tenacity==8.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 348)) (8.5.0)\n",
            "Collecting threadpoolctl==3.5.0 (from -r requirements.txt (line 350))\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken==0.6.0 (from -r requirements.txt (line 352))\n",
            "  Downloading tiktoken-0.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting tokenizers==0.20.0 (from -r requirements.txt (line 356))\n",
            "  Downloading tokenizers-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting torch==2.4.1 (from -r requirements.txt (line 358))\n",
            "  Downloading torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting tqdm==4.66.5 (from -r requirements.txt (line 364))\n",
            "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.45.1 (from -r requirements.txt (line 372))\n",
            "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions==4.12.2 (from -r requirements.txt (line 378))\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting tzdata==2024.2 (from -r requirements.txt (line 387))\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting urllib3==2.2.3 (from -r requirements.txt (line 389))\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting vegafusion==1.6.9 (from -r requirements.txt (line 394))\n",
            "  Downloading vegafusion-1.6.9-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting vegafusion-python-embed==1.6.9 (from -r requirements.txt (line 396))\n",
            "  Downloading vegafusion_python_embed-1.6.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (394 bytes)\n",
            "Collecting vl-convert-python==1.7.0 (from -r requirements.txt (line 398))\n",
            "  Downloading vl_convert_python-1.7.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting xxhash==3.5.0 (from -r requirements.txt (line 400))\n",
            "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl==1.13.0 (from -r requirements.txt (line 402))\n",
            "  Downloading yarl-1.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zipp==3.20.2 (from -r requirements.txt (line 404))\n",
            "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->-r requirements.txt (line 358)) (12.6.85)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'aiohappyeyeballs' candidate (version 2.4.2 at https://files.pythonhosted.org/packages/13/64/40165ff77ade5203284e3015cf88e11acb07d451f6bf83fff71192912a0d/aiohappyeyeballs-2.4.2-py3-none-any.whl (from https://pypi.org/simple/aiohappyeyeballs/) (requires-python:>=3.8))\n",
            "Reason for being yanked: Regression: https://github.com/aio-libs/aiohappyeyeballs/issues/100\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading accelerate-1.0.0-py3-none-any.whl (330 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading aiohttp-3.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anthropic-0.39.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.4/198.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.6.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bm25s-0.2.2-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-4.57-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.9/52.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading colpali_engine-0.3.1-py3-none-any.whl (32 kB)\n",
            "Downloading contourpy-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.5/320.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading fonttools-4.54.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.4.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.66.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.62.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.0/319.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
            "Downloading kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading narwhals-1.9.3-py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.3/185.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.48.0-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.1/376.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.5/290.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf4llm-0.0.17-py3-none-any.whl (26 kB)\n",
            "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyStemmer-2.2.0.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (683 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.11.3-py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.9/258.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
            "Downloading regex-2023.12.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (789 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.1/789.1 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_mock-1.12.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading rich-13.8.1-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_theme_manager-0.11.0-py3-none-any.whl (14 kB)\n",
            "Downloading rpds_py-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (357 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.0/358.0 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.8/434.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_chunkers-0.0.4-py3-none-any.whl (13 kB)\n",
            "Downloading semantic_router-0.0.37-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.1/66.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading stamina-24.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading tiktoken-0.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl (797.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vegafusion-1.6.9-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vegafusion_python_embed-1.6.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.1/25.1 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vl_convert_python-1.7.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.1/30.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.1/490.1 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
            "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=df0e18e118d95f2f76c8685cb1442d9be3ac5697957c0290d5fae621d620638c\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/a8/b7/d8a067c31a74de9ca252bbe53dea5f896faabd25d55f541037\n",
            "Successfully built gputil\n",
            "Installing collected packages: vegafusion-python-embed, pytz, pystemmer, gputil, zipp, xxhash, vl-convert-python, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, stamina, six, setuptools, safetensors, rpds-py, regex, pyyaml, python-dotenv, pyparsing, pymupdf, pygments, pyarrow-hotfix, psutil, protobuf, portalocker, pillow, packaging, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, narwhals, multidict, markupsafe, markdown-it-py, kiwisolver, joblib, jiter, hyperframe, hpack, hf-transfer, h11, grpcio, fsspec, frozenlist, fonttools, filelock, fastavro, einops, colorlog, colorama, charset-normalizer, certifi, backoff, attrs, anyio, aiohappyeyeballs, yarl, triton, scipy, rich, requests, referencing, pymupdf4llm, pydantic-core, pyarrow, pdf2image, nvidia-cusolver-cu12, nvidia-cudnn-cu12, jinja2, importlib-metadata, httpcore, h2, grpcio-tools, contourpy, aiosignal, torch, tiktoken, scikit-learn, rich-theme-manager, requests-mock, pydantic, matplotlib, jsonschema-specifications, huggingface-hub, httpx, bm25s, aiohttp, tokenizers, openai, jsonschema, cohere, anthropic, accelerate, transformers, semantic-router, qdrant-client, datasets, altair, vegafusion, sentence-transformers, semantic-chunkers, peft, colpali-engine\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.23.0\n",
            "    Uninstalling zipp-3.23.0:\n",
            "      Successfully uninstalled zipp-3.23.0\n",
            "  Attempting uninstall: xxhash\n",
            "    Found existing installation: xxhash 3.6.0\n",
            "    Uninstalling xxhash-3.6.0:\n",
            "      Successfully uninstalled xxhash-3.6.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.6.2\n",
            "    Uninstalling safetensors-0.6.2:\n",
            "      Successfully uninstalled safetensors-0.6.2\n",
            "  Attempting uninstall: rpds-py\n",
            "    Found existing installation: rpds-py 0.27.1\n",
            "    Uninstalling rpds-py-0.27.1:\n",
            "      Successfully uninstalled rpds-py-0.27.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.3\n",
            "    Uninstalling PyYAML-6.0.3:\n",
            "      Successfully uninstalled PyYAML-6.0.3\n",
            "  Attempting uninstall: python-dotenv\n",
            "    Found existing installation: python-dotenv 1.1.1\n",
            "    Uninstalling python-dotenv-1.1.1:\n",
            "      Successfully uninstalled python-dotenv-1.1.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.5\n",
            "    Uninstalling pyparsing-3.2.5:\n",
            "      Successfully uninstalled pyparsing-3.2.5\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.19.2\n",
            "    Uninstalling Pygments-2.19.2:\n",
            "      Successfully uninstalled Pygments-2.19.2\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "  Attempting uninstall: narwhals\n",
            "    Found existing installation: narwhals 2.7.0\n",
            "    Uninstalling narwhals-2.7.0:\n",
            "      Successfully uninstalled narwhals-2.7.0\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.7.0\n",
            "    Uninstalling multidict-6.7.0:\n",
            "      Successfully uninstalled multidict-6.7.0\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 4.0.0\n",
            "    Uninstalling markdown-it-py-4.0.0:\n",
            "      Successfully uninstalled markdown-it-py-4.0.0\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.9\n",
            "    Uninstalling kiwisolver-1.4.9:\n",
            "      Successfully uninstalled kiwisolver-1.4.9\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.2\n",
            "    Uninstalling joblib-1.5.2:\n",
            "      Successfully uninstalled joblib-1.5.2\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.11.0\n",
            "    Uninstalling jiter-0.11.0:\n",
            "      Successfully uninstalled jiter-0.11.0\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: hf-transfer\n",
            "    Found existing installation: hf_transfer 0.1.9\n",
            "    Uninstalling hf_transfer-0.1.9:\n",
            "      Successfully uninstalled hf_transfer-0.1.9\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.75.1\n",
            "    Uninstalling grpcio-1.75.1:\n",
            "      Successfully uninstalled grpcio-1.75.1\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.8.0\n",
            "    Uninstalling frozenlist-1.8.0:\n",
            "      Successfully uninstalled frozenlist-1.8.0\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.60.1\n",
            "    Uninstalling fonttools-4.60.1:\n",
            "      Successfully uninstalled fonttools-4.60.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.20.0\n",
            "    Uninstalling filelock-3.20.0:\n",
            "      Successfully uninstalled filelock-3.20.0\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.1\n",
            "    Uninstalling einops-0.8.1:\n",
            "      Successfully uninstalled einops-0.8.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.3\n",
            "    Uninstalling charset-normalizer-3.4.3:\n",
            "      Successfully uninstalled charset-normalizer-3.4.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.10.5\n",
            "    Uninstalling certifi-2025.10.5:\n",
            "      Successfully uninstalled certifi-2025.10.5\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.11.0\n",
            "    Uninstalling anyio-4.11.0:\n",
            "      Successfully uninstalled anyio-4.11.0\n",
            "  Attempting uninstall: aiohappyeyeballs\n",
            "    Found existing installation: aiohappyeyeballs 2.6.1\n",
            "    Uninstalling aiohappyeyeballs-2.6.1:\n",
            "      Successfully uninstalled aiohappyeyeballs-2.6.1\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.22.0\n",
            "    Uninstalling yarl-1.22.0:\n",
            "      Successfully uninstalled yarl-1.22.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.4\n",
            "    Uninstalling rich-13.9.4:\n",
            "      Successfully uninstalled rich-13.9.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: referencing\n",
            "    Found existing installation: referencing 0.36.2\n",
            "    Uninstalling referencing-0.36.2:\n",
            "      Successfully uninstalled referencing-0.36.2\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.3.0\n",
            "    Uninstalling h2-4.3.0:\n",
            "      Successfully uninstalled h2-4.3.0\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.3\n",
            "    Uninstalling contourpy-1.3.3:\n",
            "      Successfully uninstalled contourpy-1.3.3\n",
            "  Attempting uninstall: aiosignal\n",
            "    Found existing installation: aiosignal 1.4.0\n",
            "    Uninstalling aiosignal-1.4.0:\n",
            "      Successfully uninstalled aiosignal-1.4.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.12.0\n",
            "    Uninstalling tiktoken-0.12.0:\n",
            "      Successfully uninstalled tiktoken-0.12.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.10\n",
            "    Uninstalling pydantic-2.11.10:\n",
            "      Successfully uninstalled pydantic-2.11.10\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: jsonschema-specifications\n",
            "    Found existing installation: jsonschema-specifications 2025.9.1\n",
            "    Uninstalling jsonschema-specifications-2025.9.1:\n",
            "      Successfully uninstalled jsonschema-specifications-2025.9.1\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.35.3\n",
            "    Uninstalling huggingface-hub-0.35.3:\n",
            "      Successfully uninstalled huggingface-hub-0.35.3\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.13.0\n",
            "    Uninstalling aiohttp-3.13.0:\n",
            "      Successfully uninstalled aiohttp-3.13.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.25.1\n",
            "    Uninstalling jsonschema-4.25.1:\n",
            "      Successfully uninstalled jsonschema-4.25.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.10.1\n",
            "    Uninstalling accelerate-1.10.1:\n",
            "      Successfully uninstalled accelerate-1.10.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 5.5.0\n",
            "    Uninstalling altair-5.5.0:\n",
            "      Successfully uninstalled altair-5.5.0\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 5.1.1\n",
            "    Uninstalling sentence-transformers-5.1.1:\n",
            "      Successfully uninstalled sentence-transformers-5.1.1\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.17.1\n",
            "    Uninstalling peft-0.17.1:\n",
            "      Successfully uninstalled peft-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\n",
            "google-genai 1.41.0 requires anyio<5.0.0,>=4.8.0, but you have anyio 4.6.0 which is incompatible.\n",
            "google-genai 1.41.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "bokeh 3.7.3 requires narwhals>=1.13, but you have narwhals 1.9.3 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "gradio 5.49.0 requires huggingface-hub<2.0,>=0.33.5, but you have huggingface-hub 0.25.1 which is incompatible.\n",
            "google-adk 1.14.1 requires anyio<5.0.0,>=4.9.0; python_version >= \"3.10\", but you have anyio 4.6.0 which is incompatible.\n",
            "google-adk 1.14.1 requires requests<3.0.0,>=2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "diffusers 0.35.1 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.25.1 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.4.1 which is incompatible.\n",
            "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.12.2 which is incompatible.\n",
            "sse-starlette 3.0.2 requires anyio>=4.7.0, but you have anyio 4.6.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "mcp 1.16.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.8.2 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "albumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 2.8.2 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.4.1 which is incompatible.\n",
            "xarray 2025.10.1 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.5 which is incompatible.\n",
            "grpcio-status 1.71.2 requires grpcio>=1.71.2, but you have grpcio 1.66.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.0.0 aiohappyeyeballs-2.4.2 aiohttp-3.10.5 aiosignal-1.3.1 altair-5.4.1 anthropic-0.39.0 anyio-4.6.0 attrs-24.2.0 backoff-2.2.1 bm25s-0.2.2 certifi-2024.8.30 charset-normalizer-3.3.2 cohere-4.57 colorama-0.4.6 colorlog-6.8.2 colpali-engine-0.3.1 contourpy-1.3.0 datasets-2.19.0 einops-0.8.0 fastavro-1.9.7 filelock-3.16.1 fonttools-4.54.1 frozenlist-1.4.1 fsspec-2024.3.1 gputil-1.4.0 grpcio-1.66.1 grpcio-tools-1.62.3 h11-0.14.0 h2-4.1.0 hf-transfer-0.1.8 hpack-4.0.0 httpcore-1.0.5 httpx-0.27.2 huggingface-hub-0.25.1 hyperframe-6.0.1 importlib-metadata-6.11.0 jinja2-3.1.4 jiter-0.5.0 joblib-1.4.2 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 kiwisolver-1.4.7 markdown-it-py-3.0.0 markupsafe-2.1.5 matplotlib-3.9.2 multidict-6.1.0 narwhals-1.9.3 networkx-3.3 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 openai-1.48.0 packaging-23.2 pdf2image-1.17.0 peft-0.11.1 pillow-10.4.0 portalocker-2.10.1 protobuf-4.25.5 psutil-6.0.0 pyarrow-17.0.0 pyarrow-hotfix-0.6 pydantic-2.8.2 pydantic-core-2.20.1 pygments-2.18.0 pymupdf-1.24.13 pymupdf4llm-0.0.17 pyparsing-3.1.4 pystemmer-2.2.0.3 python-dotenv-1.0.1 pytz-2024.2 pyyaml-6.0.2 qdrant-client-1.11.3 referencing-0.35.1 regex-2023.12.25 requests-2.32.3 requests-mock-1.12.1 rich-13.8.1 rich-theme-manager-0.11.0 rpds-py-0.20.0 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 semantic-chunkers-0.0.4 semantic-router-0.0.37 sentence-transformers-3.0.1 setuptools-75.1.0 six-1.16.0 stamina-24.3.0 threadpoolctl-3.5.0 tiktoken-0.6.0 tokenizers-0.20.0 torch-2.4.1 tqdm-4.66.5 transformers-4.45.1 triton-3.0.0 typing-extensions-4.12.2 tzdata-2024.2 urllib3-2.2.3 vegafusion-1.6.9 vegafusion-python-embed-1.6.9 vl-convert-python-1.7.0 xxhash-3.5.0 yarl-1.13.0 zipp-3.20.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_distutils_hack",
                  "certifi",
                  "google",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "packaging",
                  "psutil",
                  "pyparsing",
                  "six"
                ]
              },
              "id": "1e913127c9c34fa0bb3db5d7aaf6de43"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/guyernest/advanced-rag.git\n",
        "%cd advanced-rag\n",
        "!pip install --upgrade -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision==0.18.0\n",
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZjrMbeIV77oN",
        "outputId": "1534fe2a-646c-451a-d11f-b3493d0e5642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.18.0\n",
            "  Downloading torchvision-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.0) (1.26.4)\n",
            "Collecting torch==2.3.0 (from torchvision==0.18.0)\n",
            "  Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.0) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->torchvision==0.18.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision==0.18.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.0->torchvision==0.18.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.0->torchvision==0.18.0) (1.3.0)\n",
            "Downloading torchvision-0.18.0-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m738.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m688.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cudnn-cu12, torch, torchvision\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1\n",
            "    Uninstalling torch-2.4.1:\n",
            "      Successfully uninstalled torch-2.4.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-8.9.2.26 torch-2.3.0 torchvision-0.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "functorch",
                  "nvidia",
                  "torch",
                  "torchgen"
                ]
              },
              "id": "803b8032493f424d82238c062f491ac3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "import pathlib\n",
        "from rich_theme_manager import Theme, ThemeManager\n",
        "\n",
        "THEMES = [\n",
        "    Theme(\n",
        "        name=\"dark\",\n",
        "        description=\"Dark mode theme\",\n",
        "        tags=[\"dark\"],\n",
        "        styles={\n",
        "            \"repr.own\": Style(color=\"#e87d3e\", bold=True),      # Class names\n",
        "            \"repr.tag_name\": \"dim cyan\",                        # Adjust tag names\n",
        "            \"repr.call\": \"bright_yellow\",                       # Function calls and other symbols\n",
        "            \"repr.str\": \"bright_green\",                         # String representation\n",
        "            \"repr.number\": \"bright_red\",                        # Numbers\n",
        "            \"repr.none\": \"dim white\",                           # None\n",
        "            \"repr.attrib_name\": Style(color=\"#e87d3e\", bold=True),    # Attribute names\n",
        "            \"repr.attrib_value\": \"bright_blue\",                 # Attribute values\n",
        "            \"default\": \"bright_white on black\"                  # Default text and background\n",
        "        },\n",
        "    ),\n",
        "    Theme(\n",
        "        name=\"light\",\n",
        "        description=\"Light mode theme\",\n",
        "        styles={\n",
        "            \"repr.own\": Style(color=\"#22863a\", bold=True),          # Class names\n",
        "            \"repr.tag_name\": Style(color=\"#00bfff\", bold=True),     # Adjust tag names\n",
        "            \"repr.call\": Style(color=\"#ffff00\", bold=True),         # Function calls and other symbols\n",
        "            \"repr.str\": Style(color=\"#008080\", bold=True),          # String representation\n",
        "            \"repr.number\": Style(color=\"#ff6347\", bold=True),       # Numbers\n",
        "            \"repr.none\": Style(color=\"#808080\", bold=True),         # None\n",
        "            \"repr.attrib_name\": Style(color=\"#ffff00\", bold=True),  # Attribute names\n",
        "            \"repr.attrib_value\": Style(color=\"#008080\", bold=True), # Attribute values\n",
        "            \"default\": Style(color=\"#000000\", bgcolor=\"#ffffff\"),   # Default text and background\n",
        "        },\n",
        "    ),\n",
        "]\n",
        "\n",
        "theme_dir = pathlib.Path(\"themes\").expanduser()\n",
        "theme_dir.expanduser().mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "theme_manager = ThemeManager(theme_dir=theme_dir, themes=THEMES)\n",
        "theme_manager.list_themes()\n",
        "\n",
        "dark = theme_manager.get(\"dark\")\n",
        "theme_manager.preview_theme(dark)\n",
        "light = theme_manager.get(\"light\")\n",
        "\n",
        "console = Console(theme=light)"
      ],
      "metadata": {
        "id": "YWlbOezDLi2U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "4c1a592a-ad19-451b-b385-fc597c6ff58a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m \u001b[0m\u001b[1mTheme\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mDescription     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mTags\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mPath              \u001b[0m\u001b[1m \u001b[0m\n",
              " dark   Dark mode theme   dark  themes/dark.theme  \n",
              " light  Light mode theme        themes/light.theme \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Theme  Description       Tags  Path               </span>\n",
              " dark   Dark mode theme   dark  themes/dark.theme  \n",
              " light  Light mode theme        themes/light.theme \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                                      Theme: dark - themes/dark.theme                                      \u001b[0m\n",
              "┌───────────────────┬───────────────┬───────┬─────────┬─────────┬────────────────┬────────────────────────┐\n",
              "│\u001b[1m \u001b[0m\u001b[1mstyle            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mcolor        \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mcolor\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mbgcolor\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mbgcolor\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mattributes    \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mexample               \u001b[0m\u001b[1m \u001b[0m│\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ default           │ bright_white  │ \u001b[97m█████\u001b[0m │ black   │ \u001b[30m█████  \u001b[0m │ -------------- │ \u001b[97;40mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.attrib_name  │ #e87d3e       │ \u001b[38;2;232;125;62m█████\u001b[0m │ None    │         │ \u001b[1mb\u001b[0m------------- │ \u001b[1;38;2;232;125;62mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.attrib_value │ bright_blue   │ \u001b[94m█████\u001b[0m │ None    │         │ -------------- │ \u001b[94mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.call         │ bright_yellow │ \u001b[93m█████\u001b[0m │ None    │         │ -------------- │ \u001b[93mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.none         │ white         │ \u001b[37m█████\u001b[0m │ None    │         │ -\u001b[1md\u001b[0m------------ │ \u001b[2;37mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.number       │ bright_red    │ \u001b[91m█████\u001b[0m │ None    │         │ -------------- │ \u001b[91mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.own          │ #e87d3e       │ \u001b[38;2;232;125;62m█████\u001b[0m │ None    │         │ \u001b[1mb\u001b[0m------------- │ \u001b[1;38;2;232;125;62mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.str          │ bright_green  │ \u001b[92m█████\u001b[0m │ None    │         │ -------------- │ \u001b[92mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.tag_name     │ cyan          │ \u001b[36m█████\u001b[0m │ None    │         │ -\u001b[1md\u001b[0m------------ │ \u001b[2;36mThe quick brown fox...\u001b[0m │\n",
              "└───────────────────┴───────────────┴───────┴─────────┴─────────┴────────────────┴────────────────────────┘\n",
              "┌─ attributes legend ──────────────────────────────────────────────────────────────────┐\n",
              "│  \u001b[1mb\u001b[0m: bold, \u001b[1md\u001b[0m: dim, \u001b[1mi\u001b[0m: italic, \u001b[1mu\u001b[0m: underline, \u001b[1mU\u001b[0m: double underline, \u001b[1mB\u001b[0m: blink, \u001b[1m2\u001b[0m: blink2  │\n",
              "│  \u001b[1mr\u001b[0m: reverse, \u001b[1mc\u001b[0m: conceal, \u001b[1ms\u001b[0m: strike, \u001b[1mf\u001b[0m: frame, \u001b[1me\u001b[0m: encircle, \u001b[1mo\u001b[0m: overline, \u001b[1mL\u001b[0m: Link      │\n",
              "└──────────────────────────────────────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                      Theme: dark - themes/dark.theme                                      </span>\n",
              "┌───────────────────┬───────────────┬───────┬─────────┬─────────┬────────────────┬────────────────────────┐\n",
              "│<span style=\"font-weight: bold\"> style             </span>│<span style=\"font-weight: bold\"> color         </span>│<span style=\"font-weight: bold\"> color </span>│<span style=\"font-weight: bold\"> bgcolor </span>│<span style=\"font-weight: bold\"> bgcolor </span>│<span style=\"font-weight: bold\"> attributes     </span>│<span style=\"font-weight: bold\"> example                </span>│\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ default           │ bright_white  │ <span style=\"color: #ffffff; text-decoration-color: #ffffff\">█████</span> │ black   │ <span style=\"color: #000000; text-decoration-color: #000000\">█████  </span> │ -------------- │ <span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #000000\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.attrib_name  │ #e87d3e       │ <span style=\"color: #e87d3e; text-decoration-color: #e87d3e\">█████</span> │ None    │         │ <span style=\"font-weight: bold\">b</span>------------- │ <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.attrib_value │ bright_blue   │ <span style=\"color: #0000ff; text-decoration-color: #0000ff\">█████</span> │ None    │         │ -------------- │ <span style=\"color: #0000ff; text-decoration-color: #0000ff\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.call         │ bright_yellow │ <span style=\"color: #ffff00; text-decoration-color: #ffff00\">█████</span> │ None    │         │ -------------- │ <span style=\"color: #ffff00; text-decoration-color: #ffff00\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.none         │ white         │ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">█████</span> │ None    │         │ -<span style=\"font-weight: bold\">d</span>------------ │ <span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.number       │ bright_red    │ <span style=\"color: #ff0000; text-decoration-color: #ff0000\">█████</span> │ None    │         │ -------------- │ <span style=\"color: #ff0000; text-decoration-color: #ff0000\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.own          │ #e87d3e       │ <span style=\"color: #e87d3e; text-decoration-color: #e87d3e\">█████</span> │ None    │         │ <span style=\"font-weight: bold\">b</span>------------- │ <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.str          │ bright_green  │ <span style=\"color: #00ff00; text-decoration-color: #00ff00\">█████</span> │ None    │         │ -------------- │ <span style=\"color: #00ff00; text-decoration-color: #00ff00\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.tag_name     │ cyan          │ <span style=\"color: #008080; text-decoration-color: #008080\">█████</span> │ None    │         │ -<span style=\"font-weight: bold\">d</span>------------ │ <span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">The quick brown fox...</span> │\n",
              "└───────────────────┴───────────────┴───────┴─────────┴─────────┴────────────────┴────────────────────────┘\n",
              "┌─ attributes legend ──────────────────────────────────────────────────────────────────┐\n",
              "│  <span style=\"font-weight: bold\">b</span>: bold, <span style=\"font-weight: bold\">d</span>: dim, <span style=\"font-weight: bold\">i</span>: italic, <span style=\"font-weight: bold\">u</span>: underline, <span style=\"font-weight: bold\">U</span>: double underline, <span style=\"font-weight: bold\">B</span>: blink, <span style=\"font-weight: bold\">2</span>: blink2  │\n",
              "│  <span style=\"font-weight: bold\">r</span>: reverse, <span style=\"font-weight: bold\">c</span>: conceal, <span style=\"font-weight: bold\">s</span>: strike, <span style=\"font-weight: bold\">f</span>: frame, <span style=\"font-weight: bold\">e</span>: encircle, <span style=\"font-weight: bold\">o</span>: overline, <span style=\"font-weight: bold\">L</span>: Link      │\n",
              "└──────────────────────────────────────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "C-VBIv57Lp2C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bm25s\n",
        "from bm25s.tokenization import Tokenizer, Tokenized\n",
        "import Stemmer"
      ],
      "metadata": {
        "id": "MMVApFVmLsJ4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "corpus_json = json.load(open('advanced-rag/data/corpus.json'))"
      ],
      "metadata": {
        "id": "-yTCojpKBa3E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_text = [doc['text'] for doc in corpus_json]\n",
        "# optional: create a stemmer\n",
        "english_stemmer = Stemmer.Stemmer(\"english\")\n",
        "# Initialize the Tokenizer with the stemmer\n",
        "sparse_tokenizer = Tokenizer(\n",
        "    stemmer=english_stemmer,\n",
        "    lower=True, # lowercase the tokens\n",
        "    stopwords=\"english\",  # or pass a list of stopwords\n",
        "    splitter=r\"\\w+\",  # by default r\"(?u)\\b\\w\\w+\\b\", can also be a function\n",
        ")"
      ],
      "metadata": {
        "id": "voMGdQvxBub3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "console.print(sparse_tokenizer.stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "Xjlqif1Tu_Yu",
        "outputId": "b51b7857-15cf-49a5-b007-ac0b7b3ae460"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m(\u001b[0m\n",
              "    \u001b[1;38;2;0;128;128m'a'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'an'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'and'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'are'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'as'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'at'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'be'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'but'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'by'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'for'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'if'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'in'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'into'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'is'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'it'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'no'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'not'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'of'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'on'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'or'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'such'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'that'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'the'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'their'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'then'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'there'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'these'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'they'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'this'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'to'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'was'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'will'\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'with'\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'a'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'an'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'and'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'are'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'as'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'at'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'be'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'but'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'by'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'for'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'if'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'in'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'into'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'is'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'it'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'no'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'not'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'of'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'on'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'or'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'such'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'that'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'the'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'their'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'then'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'there'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'these'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'they'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'this'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'to'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'was'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'will'</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'with'</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the corpus and only keep the ids (faster and saves memory)\n",
        "corpus_sparse_tokens = (\n",
        "    sparse_tokenizer\n",
        "    .tokenize(\n",
        "        corpus_text,\n",
        "        update_vocab=True, # update the vocab as we tokenize\n",
        "        return_as=\"ids\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create the BM25 retriever and attach your corpus_json to it\n",
        "sparse_index = bm25s.BM25(corpus=corpus_json)\n",
        "# Now, index the corpus_tokens (the corpus_json is not used yet)\n",
        "sparse_index.index(corpus_sparse_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "6407deefe3d94b67ad882ed5783229ed",
            "62edf500d4ea416e9f5eb7354810ba81",
            "7174c1b389bc4853ace61b6072eae02c",
            "027fb88babe64859a303e587363673d2",
            "79e9876cce1f486b8ae1f99e8d4dff6c",
            "b06ad6f19b734ceeaa626388c567d803",
            "787d75ecfa0848f989c24117bdbb92f4",
            "576fd526cd2749efb232ed05427fe639",
            "44c0de496875474c860a0bdc265060ed",
            "50424a21fe3242af87074508b2e95276",
            "55be1f24ea754b42bcaa14b5bc4e6698",
            "d2173524d90f481fa496be7181105b71",
            "e0f0ac6d6463478c97887836d2f96a6e",
            "6305945f5d41487f8291d6c10eb75e87",
            "d0d901fd7b6e4d688040cfda0f28c0a7",
            "3f7e1b33e85e4edaab7cac0af2a0910e",
            "edee5db4714f49a3b6b584eebe47413d",
            "6910f52ee938459185731e53004b6e2e",
            "2a65f81dd89c47f194fffcaf3e38d2c4",
            "4c3e2d8a6f6c451c86edd6a0ff12c790",
            "05a4df81d1544802aac36c5f6c443a99",
            "7898c3d8301946c4a7a142d0d2249696",
            "5557056c597c4c6e827383945f7b0892",
            "998f031b44c14320a66906893aa49b10",
            "5bee82e64db642a88c49e32cada8a260",
            "c6c71637fd624d8b9485dcafbfc05fe3",
            "220bc9da159642298e6463785aec03e0",
            "e2607c5109ed4d57a87d6d8ab382998c",
            "ceb9c9c7ab164c91ac7997ca1280a3f0",
            "025e358e1f184a629778ebcfcd6279ad",
            "14afe357ede84a609f3aaa05f1f98c6f",
            "240c098ee87d45918e64d67781ff3842",
            "e98d5c57d0a0452aa6fbb49976e31956",
            "423dd0113c864b43b1d58857c49fc95d",
            "36c5f6a310c644958b2b7afd32d5b0eb",
            "73003365ab80448c9fe1bbbe6ee5999b",
            "1ee309c81883431e9594b9e01c5e5a3a",
            "45b376666cf34c03b20d0ef570638901",
            "7ef323f91c65415ab0c26e745562af0b",
            "3570c82f68a64e4595bad6ad09928a94",
            "5f1fafdca711439c9bd9b8f994a50efe",
            "4105e698dd184492b1722e8ebc10c220",
            "e71f8875e6ed4060b28d3121b64b76f6",
            "d98c03f0fb0741f9aea1a19dfa4e1642",
            "192908f85fcd466cb7e362a92353c9ca",
            "77d6e192defa4b7a9903bec7fea2257f",
            "79ef5a13983242c2b7fb10af3958ac91",
            "3af2a7a6212f44df9a2dfece056ba43f",
            "0150cd532e89485488b69c9cc1f4a824",
            "fc680664a2c84074863b7ac80607465d",
            "6f945c6747eb405db6b58a9ba08b9e69",
            "9a7823ba9558431ea8fe1edf8d9e07b3",
            "75aba9663d46459bb985c70475a42ef1",
            "8215f9e9240c4ad8a7fda0e6e3063c85",
            "f8ab3237bfd947bf8c97793257a5120d"
          ]
        },
        "id": "mlbdqLU52tw9",
        "outputId": "9846818d-58a1-4d50-953d-02ecc9f9049f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenize texts:   0%|          | 0/46 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6407deefe3d94b67ad882ed5783229ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:bm25s:Building index from tokens\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "BM25S Create Vocab:   0%|          | 0/46 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2173524d90f481fa496be7181105b71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "BM25S Convert tokens to indices:   0%|          | 0/46 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5557056c597c4c6e827383945f7b0892"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "BM25S Count Tokens:   0%|          | 0/46 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "423dd0113c864b43b1d58857c49fc95d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "BM25S Compute Scores:   0%|          | 0/46 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "192908f85fcd466cb7e362a92353c9ca"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict = sparse_tokenizer.get_vocab_dict()\n",
        "console.print(f\"The tokenizer vocabulary includes {len(vocab_dict)} tokens/terms\")\n",
        "\n",
        "focus_token = 'context'\n",
        "focus_token_index = vocab_dict.get(focus_token)\n",
        "console.print(f\"The index of the {focus_token} is {focus_token_index}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "GLNyxppc4N8M",
        "outputId": "bca5b407-4918-4416-a786-e21ec07cec95"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "The tokenizer vocabulary includes \u001b[1;38;2;255;99;71m1689\u001b[0m tokens/terms\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The tokenizer vocabulary includes <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1689</span> tokens/terms\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "The index of the context is \u001b[1;38;2;255;99;71m127\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The index of the context is <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">127</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "console.print(sparse_tokenizer.decode([[focus_token_index]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cDjUJsUk4S1b",
        "outputId": "9c96d72c-c8ee-4da9-83b4-528dd5853424"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'context'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'context'</span><span style=\"font-weight: bold\">]]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "console.print(sparse_index.scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "M16mzWOL4X1l",
        "outputId": "4fa0cb37-547b-4c7b-e9d5-933e6a56a330"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[1;38;2;0;128;128m'data'\u001b[0m: \u001b[1;38;2;255;255;0marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;38;2;255;99;71m0.738586\u001b[0m  , \u001b[1;38;2;255;99;71m0.75115275\u001b[0m, \u001b[1;38;2;255;99;71m1.0805568\u001b[0m , \u001b[33m...\u001b[0m, \u001b[1;38;2;255;99;71m1.6571838\u001b[0m , \u001b[1;38;2;255;99;71m1.6571838\u001b[0m ,\n",
              "       \u001b[1;38;2;255;99;71m1.6571838\u001b[0m \u001b[1m]\u001b[0m, \u001b[1;38;2;255;255;0mdtype\u001b[0m=\u001b[1;38;2;0;128;128mfloat32\u001b[0m\u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'indices'\u001b[0m: \u001b[1;38;2;255;255;0marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;38;2;255;99;71m0\u001b[0m,  \u001b[1;38;2;255;99;71m9\u001b[0m, \u001b[1;38;2;255;99;71m10\u001b[0m, \u001b[33m...\u001b[0m, \u001b[1;38;2;255;99;71m45\u001b[0m, \u001b[1;38;2;255;99;71m45\u001b[0m, \u001b[1;38;2;255;99;71m45\u001b[0m\u001b[1m]\u001b[0m, \u001b[1;38;2;255;255;0mdtype\u001b[0m=\u001b[1;38;2;0;128;128mint32\u001b[0m\u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'indptr'\u001b[0m: \u001b[1;38;2;255;255;0marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m   \u001b[1;38;2;255;99;71m0\u001b[0m,   \u001b[1;38;2;255;99;71m12\u001b[0m,   \u001b[1;38;2;255;99;71m33\u001b[0m, \u001b[33m...\u001b[0m, \u001b[1;38;2;255;99;71m4037\u001b[0m, \u001b[1;38;2;255;99;71m4038\u001b[0m, \u001b[1;38;2;255;99;71m4039\u001b[0m\u001b[1m]\u001b[0m, \u001b[1;38;2;255;255;0mdtype\u001b[0m=\u001b[1;38;2;0;128;128mint32\u001b[0m\u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;0;128;128m'num_docs'\u001b[0m: \u001b[1;38;2;255;99;71m46\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'data'</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">array</span><span style=\"font-weight: bold\">([</span><span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.738586</span>  , <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.75115275</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.0805568</span> , <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.6571838</span> , <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.6571838</span> ,\n",
              "       <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.6571838</span> <span style=\"font-weight: bold\">]</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">dtype</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">float32</span><span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'indices'</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">array</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,  <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">9</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">10</span>, <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">45</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">45</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">45</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">dtype</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">int32</span><span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'indptr'</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">array</span><span style=\"font-weight: bold\">([</span>   <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,   <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">12</span>,   <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">33</span>, <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">4037</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">4038</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">4039</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">dtype</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">int32</span><span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'num_docs'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">46</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.table import Table\n",
        "from rich.style import Style\n",
        "\n",
        "token_index = vocab_dict.get(focus_token)\n",
        "console.print(f\"Index of the token `{focus_token}` in the BM25 retriever: {token_index}\")\n",
        "score_index = sparse_index.scores.get('indptr')[token_index]\n",
        "next_score_index = sparse_index.scores.get('indptr')[token_index+1]\n",
        "\n",
        "table = Table(title=f\"Document Scores for `{focus_token}`\")\n",
        "\n",
        "table.add_column(\"Document ID\", justify=\"right\", style=\"cyan\", no_wrap=True)\n",
        "table.add_column(\"Score\", justify=\"right\", style=\"bright_green\")\n",
        "\n",
        "max_score = max(sparse_index.scores['data'][score_index:next_score_index])\n",
        "# Define styles for specific rows\n",
        "highlight_style = Style(bgcolor=\"yellow\")\n",
        "\n",
        "for i in range(score_index, next_score_index):\n",
        "    doc_id = sparse_index.scores['indices'][i]\n",
        "    doc_score = sparse_index.scores['data'][i]\n",
        "    if doc_score == max_score:\n",
        "        table.add_row(\n",
        "            str(doc_id),\n",
        "            str(doc_score), style=highlight_style\n",
        "        )\n",
        "    else:\n",
        "        table.add_row(\n",
        "            str(doc_id),\n",
        "            str(doc_score)\n",
        "        )\n",
        "\n",
        "console.print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "7JAP86EP6Fv9",
        "outputId": "d50a067d-a96a-45c5-b0a7-6fb00625fba1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Index of the token `context` in the BM25 retriever: \u001b[1;38;2;255;99;71m127\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Index of the token `context` in the BM25 retriever: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">127</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m    Document Scores for     \u001b[0m\n",
              "\u001b[3m         `context`          \u001b[0m\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mDocument ID\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Score\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m          0\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.4434834\u001b[0m\u001b[92m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m          2\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.7355117\u001b[0m\u001b[92m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m          3\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m0.53710693\u001b[0m\u001b[92m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m          4\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m0.90847206\u001b[0m\u001b[92m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m         13\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.5116058\u001b[0m\u001b[92m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m         14\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.9670208\u001b[0m\u001b[92m \u001b[0m│\n",
              "│\u001b[36;43m \u001b[0m\u001b[36;43m         15\u001b[0m\u001b[36;43m \u001b[0m│\u001b[92;43m \u001b[0m\u001b[92;43m 1.1056415\u001b[0m\u001b[92;43m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m         30\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.7444794\u001b[0m\u001b[92m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m         37\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.6708646\u001b[0m\u001b[92m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m         41\u001b[0m\u001b[36m \u001b[0m│\u001b[92m \u001b[0m\u001b[92m 0.7355117\u001b[0m\u001b[92m \u001b[0m│\n",
              "└─────────────┴────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">    Document Scores for     </span>\n",
              "<span style=\"font-style: italic\">         `context`          </span>\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Document ID </span>┃<span style=\"font-weight: bold\">      Score </span>┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">           0 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.4434834 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">           2 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.7355117 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">           3 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 0.53710693 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">           4 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 0.90847206 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          13 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.5116058 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          14 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.9670208 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080; background-color: #808000\">          15 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; background-color: #808000\">  1.1056415 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          30 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.7444794 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          37 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.6708646 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          41 </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00\">  0.7355117 </span>│\n",
              "└─────────────┴────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the corpus\n",
        "query = \"What is context size of Mixtral?\"\n",
        "query_tokens = (\n",
        "    sparse_tokenizer\n",
        "    .tokenize(\n",
        "        [query],\n",
        "        update_vocab=False,\n",
        "        return_as=\"ids\"\n",
        "    )\n",
        ")\n",
        "\n",
        "console.print(query_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "92e54f7b260b4d91b510016aaf9f42e7",
            "4e39ed21749b4410b8ff2d1945f13314",
            "f08fa9b4f20b4dfd85770370ba56475c",
            "464da948d97d4f588630f29dfbc95ea5",
            "a5b0872cc6d5441ba55f9f79a4a11d88",
            "1a0bb405ae174df485851b9de570683f",
            "b5f9fe26d5e5487bb7b1f32d04690a6e",
            "60f5c257fec84f57aeab5c72036e3099",
            "00aa09feb10f48219272efc95a6ba44e",
            "1399a6c69d0e4c5c9149083177c9eab6",
            "984fca3966a24d4da5b80de4abbd2cce"
          ]
        },
        "id": "r8IHCy7O6KT3",
        "outputId": "c393e4f1-f1af-4f73-e7d1-2faa6733f7f6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenize texts:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92e54f7b260b4d91b510016aaf9f42e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;38;2;255;99;71m127\u001b[0m, \u001b[1;38;2;255;99;71m128\u001b[0m, \u001b[1;38;2;255;99;71m15\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">127</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">128</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">15</span><span style=\"font-weight: bold\">]]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the corpus\n",
        "sparse_results, sparse_scores = sparse_index.retrieve(query_tokens, k=10)\n",
        "\n",
        "for i in range(sparse_results.shape[1]):\n",
        "    doc, score = sparse_results[0, i], sparse_scores[0, i]\n",
        "    console.print(f\"Rank {i+1} (score: {score:.2f}): {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b999216a2fc04d2494e997fe88938cf3",
            "a91b72d8958448bbaf2805836a341b0c",
            "bf9d3323451940de9a5b5917cbb1ace8",
            "e8b949bac6d442b1963bac757e9576fb",
            "a717a141daac4531b9e1032faa012869",
            "9b78fb9528d344939277df2e10d12b71",
            "6c2a05c3f6d24d3d95cb7e7c2e69e956",
            "411a09855675424fa78fdf9436f497e5",
            "b4ad42365a17471f861c8cfeb2e034bf",
            "897b85c876a64f6fa67c81e4aadde479",
            "cad2bf5da62c4c5cbbce4116f6c21409"
          ]
        },
        "id": "eCudAm-J6eYs",
        "outputId": "596184bd-b10c-4957-e00a-efa101a4c580"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b999216a2fc04d2494e997fe88938cf3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Rank \u001b[1;38;2;255;99;71m1\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[1;38;2;255;99;71m1.99\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m2\u001b[0m, \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'expertsâ \u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m to process the token and combine their output additively. This \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mtechnique increases the number of parameters of a model while controlling cost and latency, as the model only uses \u001b[0m\n",
              "\u001b[1;38;2;0;128;128ma fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msize of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mbenchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdescribes the key architectural details of the Mixtral model, a sparse mixture-of-experts language model that \u001b[0m\n",
              "\u001b[1;38;2;0;128;128moutperforms larger models like Llama 2 70B and GPT-3.5 on various benchmarks.'\u001b[0m, \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mExperts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.99</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'expertsâ ) to process the token and combine their output additively. This </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">technique increases the number of parameters of a model while controlling cost and latency, as the model only uses </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">describes the key architectural details of the Mixtral model, a sparse mixture-of-experts language model that </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">outperforms larger models like Llama 2 70B and GPT-3.5 on various benchmarks.'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Rank \u001b[1;38;2;255;99;71m2\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[1;38;2;255;99;71m1.86\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m14\u001b[0m, \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mArc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4%\u001b[0m\n",
              "\u001b[1;38;2;0;128;128m70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0% Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mItalian. # 3.2 Long range performance To assess the capabilities of Mixtral to tackle long context, we evaluate it \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mon the passkey retrieval task introduced in \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m23\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m, a synthetic task designed to measure the ability of the model to \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mretrieve a passkey inserted randomly in a long prompt. Results in Figure 4 \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mLeft\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m show that Mixtral achieves a 100%\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mretrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mRight\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mshows that the perplexity of Mixtral on a subset of the proof-pile dataset \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m2\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m decreases monotonically as the size \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mof the context increases. Passkey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLoc\\n\\nThe chunk discusses Mixtral's performance on multilingual benchmarks and its ability to handle long-range \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcontext, demonstrating its strong capabilities in these areas.\"\u001b[0m, \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \n",
              "\u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">2</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.86</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4%</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0% Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Italian. # 3.2 Long range performance To assess the capabilities of Mixtral to tackle long context, we evaluate it </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">on the passkey retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a 100%</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">retrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 (Right) </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">of the context increases. Passkey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Loc\\n\\nThe chunk discusses Mixtral's performance on multilingual benchmarks and its ability to handle long-range </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">context, demonstrating its strong capabilities in these areas.\"</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Rank \u001b[1;38;2;255;99;71m3\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[1;38;2;255;99;71m1.46\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m1\u001b[0m, \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'chat model on human bench- marks. Both the base and instruct models are \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mreleased under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mhttps://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mof experts model \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSMoE\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mon most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mspeed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mnetwork. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mparameters. At every layer, for every token, a router network chooses two of these groups \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mthe â\\n\\nThis chunk \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mintroduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmost benchmarks. It describes the key architectural details of Mixtral, including its use of a sparse \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmixture-of-experts network, and mentions that the base and instruct models are released under the Apache 2.0 \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mlicense.'\u001b[0m, \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">3</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.46</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'chat model on human bench- marks. Both the base and instruct models are </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">parameters. At every layer, for every token, a router network chooses two of these groups (the â\\n\\nThis chunk </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">most benchmarks. It describes the key architectural details of Mixtral, including its use of a sparse </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mixture-of-experts network, and mentions that the base and instruct models are released under the Apache 2.0 </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">license.'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Rank \u001b[1;38;2;255;99;71m4\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[1;38;2;255;99;71m1.33\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m15\u001b[0m, \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m 228 fos a 2.0 0 5k 10k 15k 20k 25k \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m30k Context length Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m 228 04 fos 0.2 a 2.0 0.0 OK \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral. \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mLeft\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m Mixtral has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mand length of the input sequence. \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mRight\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m The perplexity of Mixtral on the proof-pile dataset decreases \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmonotonically as the context length increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmodel, demonstrating its ability to retrieve a passkey regardless of its location in a long input sequence, and \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mshowing that the model's perplexity on the proof-pile dataset decreases as the context length increases.\"\u001b[0m, \n",
              "\u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">4</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.33</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">15</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"3.8 â Mixtral_8x7B 3.5 32 &gt; $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30k Context length Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 &gt; 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">and length of the input sequence. (Right) The perplexity of Mixtral on the proof-pile dataset decreases </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">monotonically as the context length increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">model, demonstrating its ability to retrieve a passkey regardless of its location in a long input sequence, and </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">showing that the model's perplexity on the proof-pile dataset decreases as the context length increases.\"</span>, \n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Rank \u001b[1;38;2;255;99;71m5\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[1;38;2;255;99;71m1.32\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m0\u001b[0m, \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'4 2 0 2 n a J 8 \u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m G L . s c \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mBour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mSubramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSMoE\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m language model. \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mblocks \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mi.e. experts\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. For every token, at each layer, a router network selects two experts to process the current \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mstate and combine their outputs. Even though each token only sees two experts, the selected experts can be \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdifferent at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mparameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLlama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow \u001b[0m\n",
              "\u001b[1;38;2;0;128;128minstructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mâ\\n\\nThis chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mand GPT-3.5 on various benchmarks. It also describes the model architecture and the fine-tuned Mixtral 8x7B - \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mInstruct model.'\u001b[0m, \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \n",
              "\u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">5</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.32</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">state and combine their outputs. Even though each token only sees two experts, the selected experts can be </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">and GPT-3.5 on various benchmarks. It also describes the model architecture and the fine-tuned Mixtral 8x7B - </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Instruct model.'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: \n",
              "<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Rank \u001b[1;38;2;255;99;71m6\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[1;38;2;255;99;71m1.24\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m12\u001b[0m, \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Size and Efficiency. We compare our performance to the Llama 2 family, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128maiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128msee Figure 3\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. As a sparse \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters,\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral is able to outperform Llama 2 70B across most categories. Note that this analysis focuses on the active \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mparameter count \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128msee Section 2.1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m, which is directly proportional to the inference compute cost, but does not \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mconsider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mSMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mrunning more than one expert per device. They are more suitable for batched workloads where one can reach a good \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdegree of arithmetic intensity. Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmodels. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m47B tokens \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcompared to 70B\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mgpt-3.5-turbo-1106. 2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n\\nThis chunk \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdiscusses the size and efficiency of the Mixtral model, comparing its performance to the Llama 2 family of models. \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mIt highlights that Mixtral, as a sparse mixture-of-experts model, uses significantly fewer active parameters than \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLlama 2 70B while outperforming it across most benchmarks. The chunk also compares the performance of Mixtral 8x7B \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mto Llama 2 70B and GPT-3.5.'\u001b[0m, \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \n",
              "\u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">6</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.24</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">12</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Size and Efficiency. We compare our performance to the Llama 2 family, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters,</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral is able to outperform Llama 2 70B across most categories. Note that this analysis focuses on the active </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">running more than one expert per device. They are more suitable for batched workloads where one can reach a good </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">degree of arithmetic intensity. Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">gpt-3.5-turbo-1106. 2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n\\nThis chunk </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">discusses the size and efficiency of the Mixtral model, comparing its performance to the Llama 2 family of models. </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">It highlights that Mixtral, as a sparse mixture-of-experts model, uses significantly fewer active parameters than </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Llama 2 70B while outperforming it across most benchmarks. The chunk also compares the performance of Mixtral 8x7B </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">to Llama 2 70B and GPT-3.5.'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: \n",
              "<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Rank \u001b[1;38;2;255;99;71m7\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[1;38;2;255;99;71m1.12\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m4\u001b[0m, \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"Instruct under the Apache 2.0 license1, free for academic and commercial \u001b[0m\n",
              "\u001b[1;38;2;0;128;128musage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwith a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mfor efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mArchitectural details Mixtral is based on a transformer architecture \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m31\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m and uses the same modifications as \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdescribed in \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m18\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m, with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mand the feed- forward blocks are replaced by Mixture-of-Expert layers \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSection 2.1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. The model architecture \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mparameters are summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmodel, including its use of a transformer architecture with a 32k token context length and mixture-of-expert \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mlayers. It also mentions the model's open-source licensing and deployment options.\"\u001b[0m, \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mof Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">7</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.12</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"Instruct under the Apache 2.0 license1, free for academic and commercial </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">parameters are summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">model, including its use of a transformer architecture with a 32k token context length and mixture-of-expert </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">layers. It also mentions the model's open-source licensing and deployment options.\"</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Rank \u001b[1;38;2;255;99;71m8\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[1;38;2;255;99;71m0.89\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m41\u001b[0m, \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Hellaswag: Can a machine really finish your sentence? arXiv preprint \u001b[0m\n",
              "\u001b[1;38;2;0;128;128marXiv:1905.07830, 2019. \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m33\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mpreprint arXiv:2306.05685, 2023.\\n\\nThe chunk discusses two references related to language model benchmarking, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mincluding the Hellaswag dataset and the MT-Bench and Chatbot Arena benchmarks. This is situated within the broader \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcontext of the paper, which introduces the Mixtral language model and evaluates its performance on various \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mbenchmarks.'\u001b[0m, \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">8</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.89</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">41</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Hellaswag: Can a machine really finish your sentence? arXiv preprint </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">arXiv:1905.07830, 2019. [33] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">preprint arXiv:2306.05685, 2023.\\n\\nThe chunk discusses two references related to language model benchmarking, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">including the Hellaswag dataset and the MT-Bench and Chatbot Arena benchmarks. This is situated within the broader </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">context of the paper, which introduces the Mixtral language model and evaluates its performance on various </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">benchmarks.'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Rank \u001b[1;38;2;255;99;71m9\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[1;38;2;255;99;71m0.80\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m3\u001b[0m, \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msuperior capabilities in mathematics, code generation, and tasks that require multilingual understanding, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msignificantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mretrieve information from its context window of 32k tokens, regardless of the sequence length and the location of \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe information in the sequence. We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow \u001b[0m\n",
              "\u001b[1;38;2;0;128;128minstructions using supervised fine-tuning and Direct Preference Optimization \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m25\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m. Its performance notably \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msurpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mbenchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mbenchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â\\n\\nThis chunk describes the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixture of Experts layer architecture used in the Mixtral model, as well as the superior performance of Mixtral \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcompared to other models on various benchmarks, including mathematics, code generation, and multilingual tasks. It \u001b[0m\n",
              "\u001b[1;38;2;0;128;128malso introduces the Mixtral 8x7B - Instruct model, which is fine-tuned to follow instructions and outperforms other\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mchat models on human evaluation benchmarks.'\u001b[0m, \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m,\n",
              "\u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">9</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.80</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the information in the sequence. We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â\\n\\nThis chunk describes the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixture of Experts layer architecture used in the Mixtral model, as well as the superior performance of Mixtral </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">compared to other models on various benchmarks, including mathematics, code generation, and multilingual tasks. It </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">also introduces the Mixtral 8x7B - Instruct model, which is fine-tuned to follow instructions and outperforms other</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">chat models on human evaluation benchmarks.'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>,\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Rank \u001b[1;38;2;255;99;71m10\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[1;38;2;255;99;71m0.75\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m13\u001b[0m, \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"4 LLaMA 2 70B GPT-3.5 MMLU \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mMCQ in 57 subjects\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m 69.9% 70.0% 70.6% \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mHellaSwag \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m10-shot\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m 87.1% 85.5% 86.7% ARC Challenge \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m25-shot\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m 85.1% 85.2% 85.8% WinoGrande \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m5-shot\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m 83.2% 81.6% \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m81.2% MBPP \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mpass@1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m 49.8% 52.2% 60.7% GSM-8K \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m5-shot\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m 53.6% 57.1% 58.4% MT Bench \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mfor Instruct Models\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m 6.86 8.32 \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m8.30 # Mixtral 8x7B Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLlama 2 70B and GPT-3.5 performance on most metrics. Evaluation Differences. On some benchmarks, there are some \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdifferences between our evaluation protocol and the one reported in the Llama 2 paper: 1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m on MBPP, we use the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mhand-verified subset 2\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m on TriviaQA, we do not provide Wikipedia contexts. # 3.1 Multilingual benchmarks Compared \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mto Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mallows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mparticular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mTable 4.\\n\\nThis chunk presents a comparison of the performance of Mixtral 8x7B, Llama 2 70B, and GPT-3.5 on \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mvarious benchmarks, as well as an analysis of Mixtral's performance on multilingual benchmarks.\"\u001b[0m, \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \n",
              "\u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">10</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.75</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">13</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"4 LLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.30 # Mixtral 8x7B Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Llama 2 70B and GPT-3.5 performance on most metrics. Evaluation Differences. On some benchmarks, there are some </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts. # 3.1 Multilingual benchmarks Compared </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Table 4.\\n\\nThis chunk presents a comparison of the performance of Mixtral 8x7B, Llama 2 70B, and GPT-3.5 on </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">various benchmarks, as well as an analysis of Mixtral's performance on multilingual benchmarks.\"</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: \n",
              "<span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    \":memory:\"\n",
        ")\n",
        "\n",
        "# Create the embedding encoder\n",
        "dense_encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings"
      ],
      "metadata": {
        "id": "Q-Qvcj8r7m_C"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_name = \"hybrid_search\"\n",
        "\n",
        "dense_index = qdrant_client.recreate_collection(\n",
        "    collection_name=collection_name,\n",
        "        vectors_config=models.VectorParams(\n",
        "        size=dense_encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
        "        distance=models.Distance.COSINE\n",
        "    )\n",
        ")\n",
        "print(dense_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI-BJFZz7p6h",
        "outputId": "a94d373a-f051-4c75-e3f7-683c5733e03e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize!\n",
        "qdrant_client.upload_points(\n",
        "    collection_name=collection_name,\n",
        "    points=[\n",
        "        models.PointStruct(\n",
        "            id=idx,\n",
        "            vector=dense_encoder.encode(doc[\"text\"]).tolist(),\n",
        "            payload=doc\n",
        "        ) for idx, doc in enumerate(corpus_json) # data is the variable holding all the enriched texts\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Ler6Ehmf9VuP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = dense_encoder.encode(query).tolist()"
      ],
      "metadata": {
        "id": "aHBBamXr7zL4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_results = qdrant_client.search(\n",
        "    collection_name=collection_name,\n",
        "    query_vector=query_vector,\n",
        "    limit=10\n",
        ")"
      ],
      "metadata": {
        "id": "hSrwDuFh88Rb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "console.print(dense_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tpf6Qxrg8-JF",
        "outputId": "312b7a2b-dd68-47bc-b152-5fd221312694"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[1;38;2;255;255;0mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1;38;2;255;255;0mid\u001b[0m=\u001b[1;38;2;255;99;71m15\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mversion\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mscore\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.6180976543997941\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m15\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mPasskey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mLeft\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m Mixtral\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mhas 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msequence. \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mRight\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mincreases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mretrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe proof-pile dataset decreases as the context length increases.\"\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mvector\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mshard_key\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0morder_value\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;255;255;0mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1;38;2;255;255;0mid\u001b[0m=\u001b[1;38;2;255;99;71m4\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mversion\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mscore\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.49721741586617196\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m4\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad\u001b[0m\n",
              "\u001b[1;38;2;0;128;128maccessibility and potential for diverse applications. To enable the community to run Mixtral with a fully \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mopen-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient\u001b[0m\n",
              "\u001b[1;38;2;0;128;128minference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdetails Mixtral is based on a transformer architecture \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m31\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m and uses the same modifications as described in \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m18\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwith the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mforward blocks are replaced by Mixture-of-Expert layers \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSection 2.1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. The model architecture parameters are \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msummarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mits use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmentions the model's open-source licensing and deployment options.\"\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mvector\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mshard_key\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0morder_value\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;255;255;0mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1;38;2;255;255;0mid\u001b[0m=\u001b[1;38;2;255;99;71m2\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mversion\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mscore\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.44239128745163936\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m2\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'expertsâ \u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m to process the token and combine their output additively. This technique increases \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mtotal set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens.\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mIt either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdetails of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m2 70B and GPT-3.5 on various benchmarks.'\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mvector\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mshard_key\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0morder_value\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;255;255;0mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1;38;2;255;255;0mid\u001b[0m=\u001b[1;38;2;255;99;71m6\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mversion\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mscore\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.4404993624382003\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m6\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Table 1: Model architecture. # j nâ G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi Â· Ei\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. \u001b[0m\u001b[1;38;2;0;128;128mi\u001b[0m\u001b[1;38;2;0;128;128m=\u001b[0m\u001b[1;38;2;0;128;128m0\u001b[0m\u001b[1;38;2;0;128;128m Here, G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi denotes the n-dimensional \u001b[0m\n",
              "\u001b[1;38;2;0;128;128moutput of the gating network for the i-th expert, and Ei\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m is the output of the i-th expert network. If the gating\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mvector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple \u001b[0m\n",
              "\u001b[1;38;2;0;128;128malternative ways of implementing G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m6, 15, 35\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m, but a simple and performant one is implemented by taking the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msoftmax over the Top-K logits of a linear layer \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m28\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m.\\n\\nThe chunk describes the architectural details of the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral model, specifically the Sparse Mixture of Experts \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSMoE\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m layer that is used in the model.'\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mvector\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mshard_key\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0morder_value\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;255;255;0mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1;38;2;255;255;0mid\u001b[0m=\u001b[1;38;2;255;99;71m7\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mversion\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mscore\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.42453507176734634\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m7\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'We use G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m := Softmax\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mTopK\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx Â· Wg\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m, where \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mTopK\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mâ \u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi := â i if â i is among the top-K \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcoordinates of logits â â Rn and \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mTopK\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mâ \u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi := â â otherwise. The value of K â the number of experts used per \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mtoken â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwhile keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmechanism used in the Mixture of Experts \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mMoE\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m layer of the Mixtral model. It explains how the router network \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mselects the top-K experts to process each token, and how this allows the model to increase its parameter count \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwhile keeping the computational cost constant.'\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mvector\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mshard_key\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0morder_value\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;255;255;0mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1;38;2;255;255;0mid\u001b[0m=\u001b[1;38;2;255;99;71m42\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mversion\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mscore\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.3848539617202079\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m42\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'10 \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m34\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint \u001b[0m\n",
              "\u001b[1;38;2;0;128;128marXiv:2304.06364, 2023. \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m35\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mQuoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwhich is relevant to the overall topic of the document discussing the Mixtral language model.'\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mvector\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mshard_key\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0morder_value\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;255;255;0mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1;38;2;255;255;0mid\u001b[0m=\u001b[1;38;2;255;99;71m45\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mversion\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mscore\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.3803753938976532\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m45\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mStackExchange â e-â Wikipedia \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128men\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated \u001b[0m\n",
              "\u001b[1;38;2;0;128;128massignments occur a lot more often than they would with uniform assignments \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mmaterialized by the dashed lines\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mPatterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128manalysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mlocality in the expert selection, especially at higher layers of the model. This analysis provides insights into \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe behavior of the sparse mixture-of-experts architecture used in Mixtral.'\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mvector\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mshard_key\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0morder_value\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;255;255;0mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1;38;2;255;255;0mid\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mversion\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mscore\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.3677009450563554\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'4 2 0 2 n a J 8 \u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m G L . s c \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mQ. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mChaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mAntoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mAbstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSMoE\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m language model. Mixtral has the same \u001b[0m\n",
              "\u001b[1;38;2;0;128;128marchitecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mi.e. experts\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m.\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mFor every token, at each layer, a router network selects two experts to process the current state and combine their\u001b[0m\n",
              "\u001b[1;38;2;0;128;128moutputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mresult, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mtrained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mevaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmultilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msurpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mvector\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mshard_key\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0morder_value\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;255;255;0mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1;38;2;255;255;0mid\u001b[0m=\u001b[1;38;2;255;99;71m5\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mversion\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mscore\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.36711109045473134\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m5\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mnum_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mlayer \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mFigure 1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. For a more in-depth overview, see \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m12\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m. The output of the MoE module for a given input x is \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdetermined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mnetworkâ s output. i.e. given n expert networks \u001b[0m\u001b[1;38;2;0;128;128m{\u001b[0m\u001b[1;38;2;0;128;128mE0, Ei, ..., Enâ 1\u001b[0m\u001b[1;38;2;0;128;128m}\u001b[0m\u001b[1;38;2;0;128;128m, the output of the expert layer is given \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mby:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mExperts layer that is a key component of the model.'\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mvector\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mshard_key\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0morder_value\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1;38;2;255;255;0mScoredPoint\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1;38;2;255;255;0mid\u001b[0m=\u001b[1;38;2;255;99;71m11\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mversion\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mscore\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.3540553621859746\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mpayload\u001b[0m=\u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m11\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138\u001b[0m\n",
              "\u001b[1;38;2;0;128;128m348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mâ e LlaMA2 78 \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m138 348 70B 7B \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mResults on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m7B/8x7B\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m vs Llama 2 \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m7B/13B/70B\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcomprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mand math. Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2.\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mFigure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama\u001b[0m\n",
              "\u001b[1;38;2;0;128;128m2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mbenchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models \u001b[0m\n",
              "\u001b[1;38;2;0;128;128magainst the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mreading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmetrics while using significantly fewer active parameters.'\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mvector\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0mshard_key\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m,\n",
              "        \u001b[1;38;2;255;255;0morder_value\u001b[0m=\u001b[1;38;2;128;128;128mNone\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">id</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">15</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">version</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">score</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.6180976543997941</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">15</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"3.8 â Mixtral_8x7B 3.5 32 &gt; $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 &gt; 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. (Left) Mixtral</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">sequence. (Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the proof-pile dataset decreases as the context length increases.\"</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">vector</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">shard_key</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">order_value</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">id</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">4</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">version</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">score</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.49721741586617196</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">4</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mentions the model's open-source licensing and deployment options.\"</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">vector</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">shard_key</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">order_value</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">id</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">2</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">version</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">score</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.44239128745163936</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">2</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'expertsâ ) to process the token and combine their output additively. This technique increases </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens.</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2 70B and GPT-3.5 on various benchmarks.'</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">vector</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">shard_key</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">order_value</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">id</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">6</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">version</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">score</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.4404993624382003</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">6</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Table 1: Model architecture. # j nâ G(x)i Â· Ei(x). i=0 Here, G(x)i denotes the n-dimensional </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network. If the gating</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">softmax over the Top-K logits of a linear layer [28].\\n\\nThe chunk describes the architectural details of the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral model, specifically the Sparse Mixture of Experts (SMoE) layer that is used in the model.'</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">vector</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">shard_key</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">order_value</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">id</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">7</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">version</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">score</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.42453507176734634</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">7</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'We use G(x) := Softmax(TopK(x Â· Wg)), where (TopK(â ))i := â i if â i is among the top-K </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">coordinates of logits â â Rn and (TopK(â ))i := â â otherwise. The value of K â the number of experts used per </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">token â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">while keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mechanism used in the Mixture of Experts (MoE) layer of the Mixtral model. It explains how the router network </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">selects the top-K experts to process each token, and how this allows the model to increase its parameter count </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">while keeping the computational cost constant.'</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">vector</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">shard_key</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">order_value</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">id</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">42</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">version</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">score</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.3848539617202079</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">42</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'10 [34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">arXiv:2304.06364, 2023. [35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Quoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">which is relevant to the overall topic of the document discussing the Mixtral language model.'</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">vector</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">shard_key</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">order_value</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">id</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">45</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">version</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">score</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.3803753938976532</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">45</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">StackExchange â e-â Wikipedia (en) # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">assignments occur a lot more often than they would with uniform assignments (materialized by the dashed lines). </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Patterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">analysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">locality in the expert selection, especially at higher layers of the model. This analysis provides insights into </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the behavior of the sparse mixture-of-experts architecture used in Mixtral.'</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">vector</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">shard_key</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">order_value</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">id</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">version</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">score</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.3677009450563554</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts).</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">For every token, at each layer, a router network selects two experts to process the current state and combine their</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">vector</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">shard_key</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">order_value</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">id</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">5</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">version</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">score</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.36711109045473134</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">5</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">by:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Experts layer that is a key component of the model.'</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">vector</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">shard_key</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">order_value</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">ScoredPoint</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">id</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">11</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">version</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">score</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.3540553621859746</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">payload</span>=<span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">11</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â e LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">and math. Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2.</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">benchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">against the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">reading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">metrics while using significantly fewer active parameters.'</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">vector</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">shard_key</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>,\n",
              "        <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">order_value</span>=<span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">None</span>\n",
              "    <span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents_with_scores = []\n",
        "for hit in dense_results:\n",
        "    doc_id = hit.payload[\"id\"]\n",
        "    doc_text = next((doc for doc in corpus_json if doc[\"id\"] == doc_id), None)[\"text\"]\n",
        "    doc_dense_score = hit.score\n",
        "    documents_with_scores.append({\n",
        "        \"id\": doc_id,\n",
        "        \"text\": doc_text,\n",
        "        \"dense_score\": doc_dense_score\n",
        "    })\n",
        "\n",
        "for i, result in enumerate(sparse_results[0]):\n",
        "    doc_id = result[\"id\"]\n",
        "    doc_text = next((doc for doc in corpus_json if doc[\"id\"] == doc_id), None)[\"text\"]\n",
        "    doc_sparse_score = sparse_scores[0][i]\n",
        "    for doc in documents_with_scores:\n",
        "        if doc[\"id\"] == doc_id:\n",
        "            doc[\"sparse_score\"] = doc_sparse_score\n",
        "            break"
      ],
      "metadata": {
        "id": "XYsjnLvi9qN5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "console.print(documents_with_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3751Ubh79vEs",
        "outputId": "045fdaa8-1fee-4872-bb45-14ac4643e9cf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m15\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mPerformance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mLeft\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m Mixtral has 100% \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mretrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mRight\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mincreases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mretrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe proof-pile dataset decreases as the context length increases.\"\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.6180976543997941\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'sparse_score'\u001b[0m: \u001b[1;38;2;255;99;71m1.333729\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m4\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad \u001b[0m\n",
              "\u001b[1;38;2;0;128;128maccessibility and potential for diverse applications. To enable the community to run Mixtral with a fully \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mopen-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient\u001b[0m\n",
              "\u001b[1;38;2;0;128;128minference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdetails Mixtral is based on a transformer architecture \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m31\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m and uses the same modifications as described in \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m18\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwith the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mforward blocks are replaced by Mixture-of-Expert layers \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSection 2.1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. The model architecture parameters are \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msummarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mits use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmentions the model's open-source licensing and deployment options.\"\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.49721741586617196\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'sparse_score'\u001b[0m: \u001b[1;38;2;255;99;71m1.1242076\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m2\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'expertsâ \u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m to process the token and combine their output additively. This technique increases the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mnumber of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mset of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It \u001b[0m\n",
              "\u001b[1;38;2;0;128;128meither matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdetails of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m2 70B and GPT-3.5 on various benchmarks.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.44239128745163936\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'sparse_score'\u001b[0m: \u001b[1;38;2;255;99;71m1.9919165\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m6\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Table 1: Model architecture. # j nâ G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi Â· Ei\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. \u001b[0m\u001b[1;38;2;0;128;128mi\u001b[0m\u001b[1;38;2;0;128;128m=\u001b[0m\u001b[1;38;2;0;128;128m0\u001b[0m\u001b[1;38;2;0;128;128m Here, G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi denotes the n-dimensional \u001b[0m\n",
              "\u001b[1;38;2;0;128;128moutput of the gating network for the i-th expert, and Ei\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m is the output of the i-th expert network. If the gating\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mvector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple \u001b[0m\n",
              "\u001b[1;38;2;0;128;128malternative ways of implementing G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m6, 15, 35\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m, but a simple and performant one is implemented by taking the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msoftmax over the Top-K logits of a linear layer \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m28\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m.\\n\\nThe chunk describes the architectural details of the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral model, specifically the Sparse Mixture of Experts \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSMoE\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m layer that is used in the model.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.4404993624382003\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m7\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'We use G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m := Softmax\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mTopK\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx Â· Wg\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m, where \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mTopK\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mâ \u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi := â i if â i is among the top-K \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcoordinates of logits â â Rn and \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mTopK\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mâ \u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi := â â otherwise. The value of K â the number of experts used per \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mtoken â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwhile keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmechanism used in the Mixture of Experts \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mMoE\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m layer of the Mixtral model. It explains how the router network \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mselects the top-K experts to process each token, and how this allows the model to increase its parameter count \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwhile keeping the computational cost constant.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.42453507176734634\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m42\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'10 \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m34\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint \u001b[0m\n",
              "\u001b[1;38;2;0;128;128marXiv:2304.06364, 2023. \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m35\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mQuoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwhich is relevant to the overall topic of the document discussing the Mixtral language model.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.3848539617202079\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m45\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mStackExchange â e-â Wikipedia \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128men\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated \u001b[0m\n",
              "\u001b[1;38;2;0;128;128massignments occur a lot more often than they would with uniform assignments \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mmaterialized by the dashed lines\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mPatterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128manalysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mlocality in the expert selection, especially at higher layers of the model. This analysis provides insights into \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe behavior of the sparse mixture-of-experts architecture used in Mixtral.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.3803753938976532\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'4 2 0 2 n a J 8 \u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m G L . s c \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mJiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mDiego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mRenard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mAntoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mAbstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSMoE\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m language model. Mixtral has the same \u001b[0m\n",
              "\u001b[1;38;2;0;128;128marchitecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mi.e. experts\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m.\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mFor every token, at each layer, a router network selects two experts to process the current state and combine their\u001b[0m\n",
              "\u001b[1;38;2;0;128;128moutputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mresult, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mtrained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mevaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmultilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msurpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.3677009450563554\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'sparse_score'\u001b[0m: \u001b[1;38;2;255;99;71m1.3166082\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m5\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mnum_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mlayer \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mFigure 1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. For a more in-depth overview, see \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m12\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m. The output of the MoE module for a given input x is \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdetermined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mnetworkâ s output. i.e. given n expert networks \u001b[0m\u001b[1;38;2;0;128;128m{\u001b[0m\u001b[1;38;2;0;128;128mE0, Ei, ..., Enâ 1\u001b[0m\u001b[1;38;2;0;128;128m}\u001b[0m\u001b[1;38;2;0;128;128m, the output of the expert layer is given \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mby:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mExperts layer that is a key component of the model.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.36711109045473134\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m11\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348\u001b[0m\n",
              "\u001b[1;38;2;0;128;128m70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLlaMA2 78 \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m138 348 70B 7B \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mon MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m7B/8x7B\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m vs \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLlama 2 \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m7B/13B/70B\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mbenchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mDetailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcompares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B \u001b[0m\n",
              "\u001b[1;38;2;0;128;128macross most metrics. In particular, Mixtral displays a superior performance in code and mathematics \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mbenchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models \u001b[0m\n",
              "\u001b[1;38;2;0;128;128magainst the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mreading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmetrics while using significantly fewer active parameters.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.3540553621859746\u001b[0m\n",
              "    \u001b[1m}\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">15</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"3.8 â Mixtral_8x7B 3.5 32 &gt; $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 &gt; 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100% </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the proof-pile dataset decreases as the context length increases.\"</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.6180976543997941</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'sparse_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.333729</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">4</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mentions the model's open-source licensing and deployment options.\"</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.49721741586617196</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'sparse_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.1242076</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">2</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'expertsâ ) to process the token and combine their output additively. This technique increases the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2 70B and GPT-3.5 on various benchmarks.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.44239128745163936</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'sparse_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.9919165</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">6</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Table 1: Model architecture. # j nâ G(x)i Â· Ei(x). i=0 Here, G(x)i denotes the n-dimensional </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network. If the gating</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">softmax over the Top-K logits of a linear layer [28].\\n\\nThe chunk describes the architectural details of the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral model, specifically the Sparse Mixture of Experts (SMoE) layer that is used in the model.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.4404993624382003</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">7</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'We use G(x) := Softmax(TopK(x Â· Wg)), where (TopK(â ))i := â i if â i is among the top-K </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">coordinates of logits â â Rn and (TopK(â ))i := â â otherwise. The value of K â the number of experts used per </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">token â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">while keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mechanism used in the Mixture of Experts (MoE) layer of the Mixtral model. It explains how the router network </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">selects the top-K experts to process each token, and how this allows the model to increase its parameter count </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">while keeping the computational cost constant.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.42453507176734634</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">42</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'10 [34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">arXiv:2304.06364, 2023. [35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Quoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">which is relevant to the overall topic of the document discussing the Mixtral language model.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.3848539617202079</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">45</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">StackExchange â e-â Wikipedia (en) # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">assignments occur a lot more often than they would with uniform assignments (materialized by the dashed lines). </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Patterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">analysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">locality in the expert selection, especially at higher layers of the model. This analysis provides insights into </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the behavior of the sparse mixture-of-experts architecture used in Mixtral.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.3803753938976532</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts).</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">For every token, at each layer, a router network selects two experts to process the current state and combine their</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.3677009450563554</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'sparse_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.3166082</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">5</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">by:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Experts layer that is a key component of the model.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.36711109045473134</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">11</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">across most metrics. In particular, Mixtral displays a superior performance in code and mathematics </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">benchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">against the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">reading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">metrics while using significantly fewer active parameters.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.3540553621859746</span>\n",
              "    <span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Normalize the two types of scores\n",
        "dense_scores = np.array([doc.get(\"dense_score\", 0) for doc in documents_with_scores])\n",
        "sparse_scores = np.array([doc.get(\"sparse_score\", 0) for doc in documents_with_scores])\n",
        "\n",
        "dense_scores_normalized = (dense_scores - np.min(dense_scores)) / (np.max(dense_scores) - np.min(dense_scores))\n",
        "sparse_scores_normalized = (sparse_scores - np.min(sparse_scores)) / (np.max(sparse_scores) - np.min(sparse_scores))\n",
        "\n",
        "# Calculate a weighted score with alpha of 0.2 to the sparse score\n",
        "alpha = 0.2\n",
        "weighted_scores = (1 - alpha) * dense_scores_normalized + alpha * sparse_scores_normalized\n",
        "\n",
        "# Pick up the top 3 documents with the weighted score\n",
        "top_docs = sorted(\n",
        "    zip(\n",
        "        documents_with_scores,\n",
        "        weighted_scores\n",
        "    ),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")[:3]"
      ],
      "metadata": {
        "id": "RnSBgiYV9l1X"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "console.print(top_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "whYK8m7L9zeZ",
        "outputId": "d0cc4c67-5188-4926-860a-513726ecd46f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m15\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mPasskey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mLeft\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m Mixtral\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mhas 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msequence. \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mRight\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mincreases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mretrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe proof-pile dataset decreases as the context length increases.\"\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.6180976543997941\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'sparse_score'\u001b[0m: \u001b[1;38;2;255;99;71m1.333729\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;99;71m0.9339141478808913\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m4\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad\u001b[0m\n",
              "\u001b[1;38;2;0;128;128maccessibility and potential for diverse applications. To enable the community to run Mixtral with a fully \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mopen-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient\u001b[0m\n",
              "\u001b[1;38;2;0;128;128minference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdetails Mixtral is based on a transformer architecture \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m31\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m and uses the same modifications as described in \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m18\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwith the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mforward blocks are replaced by Mixture-of-Expert layers \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSection 2.1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. The model architecture parameters are \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msummarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mits use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmentions the model's open-source licensing and deployment options.\"\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.49721741586617196\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'sparse_score'\u001b[0m: \u001b[1;38;2;255;99;71m1.1242076\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;99;71m0.5466318985315719\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[1m{\u001b[0m\n",
              "            \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m2\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'expertsâ \u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m to process the token and combine their output additively. This technique increases \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mtotal set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens.\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mIt either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdetails of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m2 70B and GPT-3.5 on various benchmarks.'\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'dense_score'\u001b[0m: \u001b[1;38;2;255;99;71m0.44239128745163936\u001b[0m,\n",
              "            \u001b[1;38;2;0;128;128m'sparse_score'\u001b[0m: \u001b[1;38;2;255;99;71m1.9919165\u001b[0m\n",
              "        \u001b[1m}\u001b[0m,\n",
              "        \u001b[1;38;2;255;99;71m0.46764174640365863\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">15</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"3.8 â Mixtral_8x7B 3.5 32 &gt; $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 &gt; 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. (Left) Mixtral</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">sequence. (Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the proof-pile dataset decreases as the context length increases.\"</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.6180976543997941</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'sparse_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.333729</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.9339141478808913</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">4</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mentions the model's open-source licensing and deployment options.\"</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.49721741586617196</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'sparse_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.1242076</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.5466318985315719</span>\n",
              "    <span style=\"font-weight: bold\">)</span>,\n",
              "    <span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">2</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'expertsâ ) to process the token and combine their output additively. This technique increases </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens.</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2 70B and GPT-3.5 on various benchmarks.'</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'dense_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.44239128745163936</span>,\n",
              "            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'sparse_score'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.9919165</span>\n",
              "        <span style=\"font-weight: bold\">}</span>,\n",
              "        <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.46764174640365863</span>\n",
              "    <span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a variable to hold the search results for the generation model\n",
        "search_results = [doc[0]['text'] for doc in top_docs]"
      ],
      "metadata": {
        "id": "tqslTY9fAncU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now time to connect to the large language model\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from rich.text import Text\n",
        "\n",
        "# Configure the Gemini API\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialize the Generative Model\n",
        "# You can choose a different model if needed\n",
        "gemini_model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "# Create the prompt for the Gemini model\n",
        "prompt = f\"\"\"You are chatbot, an research expert. Your top priority is to help guide users to understand research papers.\n",
        "\n",
        "Based on the following search results, answer the user's query:\n",
        "\n",
        "Search results:\n",
        "{search_results}\n",
        "\n",
        "User query:\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "# Generate the response using the Gemini model\n",
        "response = gemini_model.generate_content(prompt)\n",
        "response_text = Text(response.text)\n",
        "\n",
        "# Display the response (optional, you might want to print it directly)\n",
        "# console.print(response_text)"
      ],
      "metadata": {
        "id": "qiiAVu25Asx2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.panel import Panel\n",
        "\n",
        "panel = Panel(response_text, title=f\"Hybrid Search Reply to \\\"{query}\\\"\")\n",
        "console.print(panel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "D7ALsZBpBtra",
        "outputId": "78391e16-88e6-4fe2-9038-3c950df461bf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────── Hybrid Search Reply to \"What is context size of Mixtral?\" ───────────────────────────╮\n",
              "│ Based on the search results, Mixtral supports a fully dense context length of **32k tokens**. It was also       │\n",
              "│ pretrained using this 32k token context size.                                                                   │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────── Hybrid Search Reply to \"What is context size of Mixtral?\" ───────────────────────────╮\n",
              "│ Based on the search results, Mixtral supports a fully dense context length of **32k tokens**. It was also       │\n",
              "│ pretrained using this 32k token context size.                                                                   │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}