{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ6iWenRBNPVeY7wqttwNV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fe6a25a7aa734fd68195ee549f925ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6700ca64aab4cc98b33d62a555ad930",
              "IPY_MODEL_ed98776d6a014baea216132d8e26a598",
              "IPY_MODEL_b8fd6582e66a46d1868db90a59b4ade9"
            ],
            "layout": "IPY_MODEL_92325bc6694c4d188c2b57aa46eeb0d3"
          }
        },
        "d6700ca64aab4cc98b33d62a555ad930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_716faa679c5c41d7859372b0aa9ab5d5",
            "placeholder": "​",
            "style": "IPY_MODEL_db822d6374884d0882e0e05d6717e5bb",
            "value": "config.json: 100%"
          }
        },
        "ed98776d6a014baea216132d8e26a598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb2f3eb941d54e0f91c80298ab7a7b69",
            "max": 794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_688ee1fafc464668af0361a71d1a9497",
            "value": 794
          }
        },
        "b8fd6582e66a46d1868db90a59b4ade9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6182914b34734308a65e51bab15770ee",
            "placeholder": "​",
            "style": "IPY_MODEL_b0b17cb769894dc98584d2ac6e6a24a7",
            "value": " 794/794 [00:00&lt;00:00, 70.1kB/s]"
          }
        },
        "92325bc6694c4d188c2b57aa46eeb0d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "716faa679c5c41d7859372b0aa9ab5d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db822d6374884d0882e0e05d6717e5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb2f3eb941d54e0f91c80298ab7a7b69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "688ee1fafc464668af0361a71d1a9497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6182914b34734308a65e51bab15770ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0b17cb769894dc98584d2ac6e6a24a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d893c18bcd2c46ae969f82a8bd142990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6f10bf30e6d4956b92eaebcfd74979f",
              "IPY_MODEL_afa4bcd23a2642a7b8cef9300d5fecc8",
              "IPY_MODEL_2665b7d5999c43df8ee559c2c2c70370"
            ],
            "layout": "IPY_MODEL_88a3b1de5d154f46ac61623819edb9d6"
          }
        },
        "b6f10bf30e6d4956b92eaebcfd74979f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd985801a0174af5a4f941be05c8254a",
            "placeholder": "​",
            "style": "IPY_MODEL_7b4afe8b6e4b45ae8083f2c47da59acc",
            "value": "model.safetensors: 100%"
          }
        },
        "afa4bcd23a2642a7b8cef9300d5fecc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95c438e722b2405f8c43a7836bf1cb09",
            "max": 90870598,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d6d82bd853d4ed8a4751616f0b8a09b",
            "value": 90870598
          }
        },
        "2665b7d5999c43df8ee559c2c2c70370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83c4c3802a244034ac6dd6127390934d",
            "placeholder": "​",
            "style": "IPY_MODEL_d8bbb0b01aeb41f2badb3f7deb94157e",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 179MB/s]"
          }
        },
        "88a3b1de5d154f46ac61623819edb9d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd985801a0174af5a4f941be05c8254a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b4afe8b6e4b45ae8083f2c47da59acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95c438e722b2405f8c43a7836bf1cb09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d6d82bd853d4ed8a4751616f0b8a09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83c4c3802a244034ac6dd6127390934d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8bbb0b01aeb41f2badb3f7deb94157e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c57de25c55784e36aa6ace3adeef7358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa978d562bfb4120b14bbaf1229bb98f",
              "IPY_MODEL_589093a94e204c7893c7a372b65868c1",
              "IPY_MODEL_c9401f3fe56f4d8bb4c405adeb49ae9c"
            ],
            "layout": "IPY_MODEL_582cd571162e4099b9b3fdaf8271399c"
          }
        },
        "aa978d562bfb4120b14bbaf1229bb98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed129d59fd649ac92adbac069ad5612",
            "placeholder": "​",
            "style": "IPY_MODEL_623171362f3b4e89a4069ff6a2552881",
            "value": "tokenizer_config.json: "
          }
        },
        "589093a94e204c7893c7a372b65868c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a0ac3ddd4e443399cd8d58feda62b72",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a91dfe1a88d4c6eae1f336ca90c24bf",
            "value": 1
          }
        },
        "c9401f3fe56f4d8bb4c405adeb49ae9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cfa7ba4307648b9860238accf97eefa",
            "placeholder": "​",
            "style": "IPY_MODEL_ea260eb0171f4e85a804831e81149de9",
            "value": " 1.33k/? [00:00&lt;00:00, 90.4kB/s]"
          }
        },
        "582cd571162e4099b9b3fdaf8271399c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ed129d59fd649ac92adbac069ad5612": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623171362f3b4e89a4069ff6a2552881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a0ac3ddd4e443399cd8d58feda62b72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1a91dfe1a88d4c6eae1f336ca90c24bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cfa7ba4307648b9860238accf97eefa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea260eb0171f4e85a804831e81149de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e221dd5d757d42efb2462ea9a3ccb5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73d4fccaa3ea4bf7a0756de4fd71a2db",
              "IPY_MODEL_75d6f2b6dbc047dbad34878f2c380d30",
              "IPY_MODEL_d099e2814cf94d168a7758083e7c36d5"
            ],
            "layout": "IPY_MODEL_af18bfac0f65425a8d0eb609162691de"
          }
        },
        "73d4fccaa3ea4bf7a0756de4fd71a2db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f41e43b984c8404db915d935762a2c7c",
            "placeholder": "​",
            "style": "IPY_MODEL_b311342c41154403870520248db4531f",
            "value": "vocab.txt: "
          }
        },
        "75d6f2b6dbc047dbad34878f2c380d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a4b72ef7ced42bebd90408bf1710388",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b9a4ea31c784aeba019962373a2aa7e",
            "value": 1
          }
        },
        "d099e2814cf94d168a7758083e7c36d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09daf19781ee42debd4680fc321a93e8",
            "placeholder": "​",
            "style": "IPY_MODEL_e565fb75c4ab4a7794844c0698d8aac4",
            "value": " 232k/? [00:00&lt;00:00, 5.19MB/s]"
          }
        },
        "af18bfac0f65425a8d0eb609162691de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f41e43b984c8404db915d935762a2c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b311342c41154403870520248db4531f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a4b72ef7ced42bebd90408bf1710388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1b9a4ea31c784aeba019962373a2aa7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09daf19781ee42debd4680fc321a93e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e565fb75c4ab4a7794844c0698d8aac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb2f6a456124467c8263745becc2e6a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_506388a899f74ff59ceed4a027866c30",
              "IPY_MODEL_fec1a014b9de4a8586d729fb79c9cc0e",
              "IPY_MODEL_ef25313df9a94dfd9f5e6b3a9e7957b9"
            ],
            "layout": "IPY_MODEL_26c0e96159d340aebfc50814428dda21"
          }
        },
        "506388a899f74ff59ceed4a027866c30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36d5a0c6251d4b0d94d6eaee01e9c0e6",
            "placeholder": "​",
            "style": "IPY_MODEL_4f0b99703b4c47d79b7ace06749f8229",
            "value": "tokenizer.json: "
          }
        },
        "fec1a014b9de4a8586d729fb79c9cc0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05bbe1e4a2c84b44803bcf56ee042300",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_125313f4f31f429cb9bf6898e06b004c",
            "value": 1
          }
        },
        "ef25313df9a94dfd9f5e6b3a9e7957b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_493e770dfedc4ebeb0c2ae0758d735f7",
            "placeholder": "​",
            "style": "IPY_MODEL_d2c53910f90c4a458f2e9e38f6c7c689",
            "value": " 711k/? [00:00&lt;00:00, 20.9MB/s]"
          }
        },
        "26c0e96159d340aebfc50814428dda21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36d5a0c6251d4b0d94d6eaee01e9c0e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f0b99703b4c47d79b7ace06749f8229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05bbe1e4a2c84b44803bcf56ee042300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "125313f4f31f429cb9bf6898e06b004c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "493e770dfedc4ebeb0c2ae0758d735f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c53910f90c4a458f2e9e38f6c7c689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "360e26bdd44244e7af97581242213cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b4fbdd235724c16a6fd257bf6383d14",
              "IPY_MODEL_b865d7f9ac864becb03542148a6b5192",
              "IPY_MODEL_cde6f478963749d88755913132cec8e1"
            ],
            "layout": "IPY_MODEL_c59872038f3c4ddda6df453b83f35c88"
          }
        },
        "7b4fbdd235724c16a6fd257bf6383d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_740ba5b5fdf2483a9db8cb87b1223636",
            "placeholder": "​",
            "style": "IPY_MODEL_08faff87057c48cfa2f4ecab7554c059",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "b865d7f9ac864becb03542148a6b5192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f64c4d3bc3041e7b42bcea21ac9d658",
            "max": 132,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81c56fde0a6d4494a3dfbb479e9391b5",
            "value": 132
          }
        },
        "cde6f478963749d88755913132cec8e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ba32c389f4d47abafba6ebb58d86cb9",
            "placeholder": "​",
            "style": "IPY_MODEL_c41c7f5f4b9645ccb59463bc35dbf401",
            "value": " 132/132 [00:00&lt;00:00, 6.91kB/s]"
          }
        },
        "c59872038f3c4ddda6df453b83f35c88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "740ba5b5fdf2483a9db8cb87b1223636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08faff87057c48cfa2f4ecab7554c059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f64c4d3bc3041e7b42bcea21ac9d658": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81c56fde0a6d4494a3dfbb479e9391b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ba32c389f4d47abafba6ebb58d86cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c41c7f5f4b9645ccb59463bc35dbf401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zamanmiraz/DSandML-Notebooks/blob/main/RAG/07_reranking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UZJtKppKGQYP",
        "outputId": "fb8b610e-bc74-45f6-a084-a4907c7ed0e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'advanced-rag'...\n",
            "remote: Enumerating objects: 281, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 281 (delta 4), reused 10 (delta 3), pack-reused 268 (from 1)\u001b[K\n",
            "Receiving objects: 100% (281/281), 18.84 MiB | 11.60 MiB/s, done.\n",
            "Resolving deltas: 100% (167/167), done.\n",
            "/content/advanced-rag\n",
            "Collecting accelerate==1.0.0 (from -r requirements.txt (line 3))\n",
            "  Downloading accelerate-1.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting aiohappyeyeballs==2.4.2 (from -r requirements.txt (line 5))\n",
            "  Downloading aiohappyeyeballs-2.4.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting aiohttp==3.10.5 (from -r requirements.txt (line 7))\n",
            "  Downloading aiohttp-3.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting aiosignal==1.3.1 (from -r requirements.txt (line 12))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting altair==5.4.1 (from -r requirements.txt (line 14))\n",
            "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (0.7.0)\n",
            "Collecting anthropic==0.39.0 (from -r requirements.txt (line 20))\n",
            "  Downloading anthropic-0.39.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting anyio==4.6.0 (from -r requirements.txt (line 22))\n",
            "  Downloading anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting attrs==24.2.0 (from -r requirements.txt (line 27))\n",
            "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting backoff==2.2.1 (from -r requirements.txt (line 32))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting bm25s==0.2.2 (from -r requirements.txt (line 34))\n",
            "  Downloading bm25s-0.2.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting certifi==2024.8.30 (from -r requirements.txt (line 36))\n",
            "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting charset-normalizer==3.3.2 (from -r requirements.txt (line 41))\n",
            "  Downloading charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting cohere==4.57 (from -r requirements.txt (line 43))\n",
            "  Downloading cohere-4.57-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting colorama==0.4.6 (from -r requirements.txt (line 47))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting colorlog==6.8.2 (from -r requirements.txt (line 51))\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting colpali-engine==0.3.1 (from -r requirements.txt (line 55))\n",
            "  Downloading colpali_engine-0.3.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting contourpy==1.3.0 (from -r requirements.txt (line 57))\n",
            "  Downloading contourpy-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 59)) (0.12.1)\n",
            "Collecting datasets==2.19.0 (from -r requirements.txt (line 61))\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: dill==0.3.8 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 63)) (0.3.8)\n",
            "Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 67)) (1.9.0)\n",
            "Collecting einops==0.8.0 (from -r requirements.txt (line 71))\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting fastavro==1.9.7 (from -r requirements.txt (line 73))\n",
            "  Downloading fastavro-1.9.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting filelock==3.16.1 (from -r requirements.txt (line 75))\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fonttools==4.54.1 (from -r requirements.txt (line 81))\n",
            "  Downloading fonttools-4.54.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.7/163.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist==1.4.1 (from -r requirements.txt (line 83))\n",
            "  Downloading frozenlist-1.4.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting fsspec==2024.3.1 (from -r requirements.txt (line 87))\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting gputil==1.4.0 (from -r requirements.txt (line 92))\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting grpcio==1.66.1 (from -r requirements.txt (line 94))\n",
            "  Downloading grpcio-1.66.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting grpcio-tools==1.62.3 (from -r requirements.txt (line 98))\n",
            "  Downloading grpcio_tools-1.62.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting h11==0.14.0 (from -r requirements.txt (line 100))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting h2==4.1.0 (from -r requirements.txt (line 102))\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting hf-transfer==0.1.8 (from -r requirements.txt (line 104))\n",
            "  Downloading hf_transfer-0.1.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting hpack==4.0.0 (from -r requirements.txt (line 106))\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting httpcore==1.0.5 (from -r requirements.txt (line 108))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting httpx==0.27.2 (from -r requirements.txt (line 110))\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub==0.25.1 (from -r requirements.txt (line 115))\n",
            "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting hyperframe==6.0.1 (from -r requirements.txt (line 124))\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 126)) (3.10)\n",
            "Collecting importlib-metadata==6.11.0 (from -r requirements.txt (line 132))\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting jinja2==3.1.4 (from -r requirements.txt (line 134))\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting jiter==0.5.0 (from -r requirements.txt (line 138))\n",
            "  Downloading jiter-0.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting joblib==1.4.2 (from -r requirements.txt (line 142))\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting jsonschema==4.23.0 (from -r requirements.txt (line 144))\n",
            "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting jsonschema-specifications==2023.12.1 (from -r requirements.txt (line 146))\n",
            "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting kiwisolver==1.4.7 (from -r requirements.txt (line 148))\n",
            "  Downloading kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting markdown-it-py==3.0.0 (from -r requirements.txt (line 150))\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting markupsafe==2.1.5 (from -r requirements.txt (line 152))\n",
            "  Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting matplotlib==3.9.2 (from -r requirements.txt (line 154))\n",
            "  Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 159)) (0.1.2)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 161)) (1.3.0)\n",
            "Collecting multidict==6.1.0 (from -r requirements.txt (line 163))\n",
            "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: multiprocess==0.70.16 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 167)) (0.70.16)\n",
            "Collecting narwhals==1.9.3 (from -r requirements.txt (line 169))\n",
            "  Downloading narwhals-1.9.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting networkx==3.3 (from -r requirements.txt (line 171))\n",
            "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting numpy==1.26.4 (from -r requirements.txt (line 173))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==1.48.0 (from -r requirements.txt (line 193))\n",
            "  Downloading openai-1.48.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting packaging==23.2 (from -r requirements.txt (line 197))\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 206)) (2.2.2)\n",
            "Collecting pdf2image==1.17.0 (from -r requirements.txt (line 211))\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting peft==0.11.1 (from -r requirements.txt (line 213))\n",
            "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pillow==10.4.0 (from -r requirements.txt (line 215))\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting portalocker==2.10.1 (from -r requirements.txt (line 221))\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting protobuf==4.25.5 (from -r requirements.txt (line 223))\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting psutil==6.0.0 (from -r requirements.txt (line 227))\n",
            "  Downloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting pyarrow==17.0.0 (from -r requirements.txt (line 232))\n",
            "  Downloading pyarrow-17.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting pyarrow-hotfix==0.6 (from -r requirements.txt (line 236))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting pydantic==2.8.2 (from -r requirements.txt (line 238))\n",
            "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.20.1 (from -r requirements.txt (line 245))\n",
            "  Downloading pydantic_core-2.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting pygments==2.18.0 (from -r requirements.txt (line 247))\n",
            "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pymupdf==1.24.13 (from -r requirements.txt (line 249))\n",
            "  Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pymupdf4llm==0.0.17 (from -r requirements.txt (line 251))\n",
            "  Downloading pymupdf4llm-0.0.17-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pyparsing==3.1.4 (from -r requirements.txt (line 253))\n",
            "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pystemmer==2.2.0.3 (from -r requirements.txt (line 255))\n",
            "  Downloading PyStemmer-2.2.0.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 257)) (2.9.0.post0)\n",
            "Collecting python-dotenv==1.0.1 (from -r requirements.txt (line 261))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting pytz==2024.2 (from -r requirements.txt (line 263))\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pyyaml==6.0.2 (from -r requirements.txt (line 265))\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting qdrant-client==1.11.3 (from -r requirements.txt (line 273))\n",
            "  Downloading qdrant_client-1.11.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting referencing==0.35.1 (from -r requirements.txt (line 275))\n",
            "  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting regex==2023.12.25 (from -r requirements.txt (line 279))\n",
            "  Downloading regex-2023.12.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests==2.32.3 (from -r requirements.txt (line 285))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting requests-mock==1.12.1 (from -r requirements.txt (line 294))\n",
            "  Downloading requests_mock-1.12.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting rich==13.8.1 (from -r requirements.txt (line 298))\n",
            "  Downloading rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting rich-theme-manager==0.11.0 (from -r requirements.txt (line 302))\n",
            "  Downloading rich_theme_manager-0.11.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting rpds-py==0.20.0 (from -r requirements.txt (line 304))\n",
            "  Downloading rpds_py-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting safetensors==0.4.5 (from -r requirements.txt (line 308))\n",
            "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting scikit-learn==1.5.2 (from -r requirements.txt (line 313))\n",
            "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting scipy==1.14.1 (from -r requirements.txt (line 317))\n",
            "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 322)) (0.13.2)\n",
            "Collecting semantic-chunkers==0.0.4 (from -r requirements.txt (line 324))\n",
            "  Downloading semantic_chunkers-0.0.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting semantic-router==0.0.37 (from -r requirements.txt (line 326))\n",
            "  Downloading semantic_router-0.0.37-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting sentence-transformers==3.0.1 (from -r requirements.txt (line 330))\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting setuptools==75.1.0 (from -r requirements.txt (line 332))\n",
            "  Downloading setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting six==1.16.0 (from -r requirements.txt (line 334))\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 336)) (1.3.1)\n",
            "Collecting stamina==24.3.0 (from -r requirements.txt (line 342))\n",
            "  Downloading stamina-24.3.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 344)) (1.13.3)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 346)) (0.9.0)\n",
            "Requirement already satisfied: tenacity==8.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 348)) (8.5.0)\n",
            "Collecting threadpoolctl==3.5.0 (from -r requirements.txt (line 350))\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken==0.6.0 (from -r requirements.txt (line 352))\n",
            "  Downloading tiktoken-0.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting tokenizers==0.20.0 (from -r requirements.txt (line 356))\n",
            "  Downloading tokenizers-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting torch==2.4.1 (from -r requirements.txt (line 358))\n",
            "  Downloading torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting tqdm==4.66.5 (from -r requirements.txt (line 364))\n",
            "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.45.1 (from -r requirements.txt (line 372))\n",
            "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions==4.12.2 (from -r requirements.txt (line 378))\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting tzdata==2024.2 (from -r requirements.txt (line 387))\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting urllib3==2.2.3 (from -r requirements.txt (line 389))\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting vegafusion==1.6.9 (from -r requirements.txt (line 394))\n",
            "  Downloading vegafusion-1.6.9-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting vegafusion-python-embed==1.6.9 (from -r requirements.txt (line 396))\n",
            "  Downloading vegafusion_python_embed-1.6.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (394 bytes)\n",
            "Collecting vl-convert-python==1.7.0 (from -r requirements.txt (line 398))\n",
            "  Downloading vl_convert_python-1.7.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting xxhash==3.5.0 (from -r requirements.txt (line 400))\n",
            "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl==1.13.0 (from -r requirements.txt (line 402))\n",
            "  Downloading yarl-1.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zipp==3.20.2 (from -r requirements.txt (line 404))\n",
            "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.1->-r requirements.txt (line 358))\n",
            "  Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->-r requirements.txt (line 358)) (12.6.85)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'aiohappyeyeballs' candidate (version 2.4.2 at https://files.pythonhosted.org/packages/13/64/40165ff77ade5203284e3015cf88e11acb07d451f6bf83fff71192912a0d/aiohappyeyeballs-2.4.2-py3-none-any.whl (from https://pypi.org/simple/aiohappyeyeballs/) (requires-python:>=3.8))\n",
            "Reason for being yanked: Regression: https://github.com/aio-libs/aiohappyeyeballs/issues/100\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading accelerate-1.0.0-py3-none-any.whl (330 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading aiohttp-3.10.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anthropic-0.39.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.4/198.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.6.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bm25s-0.2.2-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-4.57-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.9/52.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading colpali_engine-0.3.1-py3-none-any.whl (32 kB)\n",
            "Downloading contourpy-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.5/320.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading fonttools-4.54.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.4.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.66.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.62.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.0/319.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
            "Downloading kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading narwhals-1.9.3-py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.3/185.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.48.0-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.1/376.1 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.5/290.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf4llm-0.0.17-py3-none-any.whl (26 kB)\n",
            "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyStemmer-2.2.0.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (683 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.11.3-py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.9/258.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
            "Downloading regex-2023.12.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (789 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.1/789.1 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_mock-1.12.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading rich-13.8.1-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_theme_manager-0.11.0-py3-none-any.whl (14 kB)\n",
            "Downloading rpds_py-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (357 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.0/358.0 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.8/434.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_chunkers-0.0.4-py3-none-any.whl (13 kB)\n",
            "Downloading semantic_router-0.0.37-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.1/66.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading stamina-24.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading tiktoken-0.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl (797.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 MB\u001b[0m \u001b[31m732.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vegafusion-1.6.9-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vegafusion_python_embed-1.6.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.1/25.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vl_convert_python-1.7.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.1/30.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.1/490.1 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
            "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m906.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=98618ea3b1f3baad107fc5b17a72f2bdda93da1e19079ca5de304f1f14005ee2\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/a8/b7/d8a067c31a74de9ca252bbe53dea5f896faabd25d55f541037\n",
            "Successfully built gputil\n",
            "Installing collected packages: vegafusion-python-embed, pytz, pystemmer, gputil, zipp, xxhash, vl-convert-python, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, stamina, six, setuptools, safetensors, rpds-py, regex, pyyaml, python-dotenv, pyparsing, pymupdf, pygments, pyarrow-hotfix, psutil, protobuf, portalocker, pillow, packaging, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, narwhals, multidict, markupsafe, markdown-it-py, kiwisolver, joblib, jiter, hyperframe, hpack, hf-transfer, h11, grpcio, fsspec, frozenlist, fonttools, filelock, fastavro, einops, colorlog, colorama, charset-normalizer, certifi, backoff, attrs, anyio, aiohappyeyeballs, yarl, triton, scipy, rich, requests, referencing, pymupdf4llm, pydantic-core, pyarrow, pdf2image, nvidia-cusolver-cu12, nvidia-cudnn-cu12, jinja2, importlib-metadata, httpcore, h2, grpcio-tools, contourpy, aiosignal, torch, tiktoken, scikit-learn, rich-theme-manager, requests-mock, pydantic, matplotlib, jsonschema-specifications, huggingface-hub, httpx, bm25s, aiohttp, tokenizers, openai, jsonschema, cohere, anthropic, accelerate, transformers, semantic-router, qdrant-client, datasets, altair, vegafusion, sentence-transformers, semantic-chunkers, peft, colpali-engine\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.23.0\n",
            "    Uninstalling zipp-3.23.0:\n",
            "      Successfully uninstalled zipp-3.23.0\n",
            "  Attempting uninstall: xxhash\n",
            "    Found existing installation: xxhash 3.6.0\n",
            "    Uninstalling xxhash-3.6.0:\n",
            "      Successfully uninstalled xxhash-3.6.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.6.2\n",
            "    Uninstalling safetensors-0.6.2:\n",
            "      Successfully uninstalled safetensors-0.6.2\n",
            "  Attempting uninstall: rpds-py\n",
            "    Found existing installation: rpds-py 0.27.1\n",
            "    Uninstalling rpds-py-0.27.1:\n",
            "      Successfully uninstalled rpds-py-0.27.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.3\n",
            "    Uninstalling PyYAML-6.0.3:\n",
            "      Successfully uninstalled PyYAML-6.0.3\n",
            "  Attempting uninstall: python-dotenv\n",
            "    Found existing installation: python-dotenv 1.1.1\n",
            "    Uninstalling python-dotenv-1.1.1:\n",
            "      Successfully uninstalled python-dotenv-1.1.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.5\n",
            "    Uninstalling pyparsing-3.2.5:\n",
            "      Successfully uninstalled pyparsing-3.2.5\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.19.2\n",
            "    Uninstalling Pygments-2.19.2:\n",
            "      Successfully uninstalled Pygments-2.19.2\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "  Attempting uninstall: narwhals\n",
            "    Found existing installation: narwhals 2.7.0\n",
            "    Uninstalling narwhals-2.7.0:\n",
            "      Successfully uninstalled narwhals-2.7.0\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.7.0\n",
            "    Uninstalling multidict-6.7.0:\n",
            "      Successfully uninstalled multidict-6.7.0\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 4.0.0\n",
            "    Uninstalling markdown-it-py-4.0.0:\n",
            "      Successfully uninstalled markdown-it-py-4.0.0\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.9\n",
            "    Uninstalling kiwisolver-1.4.9:\n",
            "      Successfully uninstalled kiwisolver-1.4.9\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.2\n",
            "    Uninstalling joblib-1.5.2:\n",
            "      Successfully uninstalled joblib-1.5.2\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.11.0\n",
            "    Uninstalling jiter-0.11.0:\n",
            "      Successfully uninstalled jiter-0.11.0\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: hf-transfer\n",
            "    Found existing installation: hf_transfer 0.1.9\n",
            "    Uninstalling hf_transfer-0.1.9:\n",
            "      Successfully uninstalled hf_transfer-0.1.9\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.75.1\n",
            "    Uninstalling grpcio-1.75.1:\n",
            "      Successfully uninstalled grpcio-1.75.1\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.8.0\n",
            "    Uninstalling frozenlist-1.8.0:\n",
            "      Successfully uninstalled frozenlist-1.8.0\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.60.1\n",
            "    Uninstalling fonttools-4.60.1:\n",
            "      Successfully uninstalled fonttools-4.60.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.20.0\n",
            "    Uninstalling filelock-3.20.0:\n",
            "      Successfully uninstalled filelock-3.20.0\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.1\n",
            "    Uninstalling einops-0.8.1:\n",
            "      Successfully uninstalled einops-0.8.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.3\n",
            "    Uninstalling charset-normalizer-3.4.3:\n",
            "      Successfully uninstalled charset-normalizer-3.4.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.10.5\n",
            "    Uninstalling certifi-2025.10.5:\n",
            "      Successfully uninstalled certifi-2025.10.5\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.11.0\n",
            "    Uninstalling anyio-4.11.0:\n",
            "      Successfully uninstalled anyio-4.11.0\n",
            "  Attempting uninstall: aiohappyeyeballs\n",
            "    Found existing installation: aiohappyeyeballs 2.6.1\n",
            "    Uninstalling aiohappyeyeballs-2.6.1:\n",
            "      Successfully uninstalled aiohappyeyeballs-2.6.1\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.22.0\n",
            "    Uninstalling yarl-1.22.0:\n",
            "      Successfully uninstalled yarl-1.22.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.4\n",
            "    Uninstalling rich-13.9.4:\n",
            "      Successfully uninstalled rich-13.9.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: referencing\n",
            "    Found existing installation: referencing 0.36.2\n",
            "    Uninstalling referencing-0.36.2:\n",
            "      Successfully uninstalled referencing-0.36.2\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.3.0\n",
            "    Uninstalling h2-4.3.0:\n",
            "      Successfully uninstalled h2-4.3.0\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.3\n",
            "    Uninstalling contourpy-1.3.3:\n",
            "      Successfully uninstalled contourpy-1.3.3\n",
            "  Attempting uninstall: aiosignal\n",
            "    Found existing installation: aiosignal 1.4.0\n",
            "    Uninstalling aiosignal-1.4.0:\n",
            "      Successfully uninstalled aiosignal-1.4.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: tiktoken\n",
            "    Found existing installation: tiktoken 0.12.0\n",
            "    Uninstalling tiktoken-0.12.0:\n",
            "      Successfully uninstalled tiktoken-0.12.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.10\n",
            "    Uninstalling pydantic-2.11.10:\n",
            "      Successfully uninstalled pydantic-2.11.10\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: jsonschema-specifications\n",
            "    Found existing installation: jsonschema-specifications 2025.9.1\n",
            "    Uninstalling jsonschema-specifications-2025.9.1:\n",
            "      Successfully uninstalled jsonschema-specifications-2025.9.1\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.35.3\n",
            "    Uninstalling huggingface-hub-0.35.3:\n",
            "      Successfully uninstalled huggingface-hub-0.35.3\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.13.0\n",
            "    Uninstalling aiohttp-3.13.0:\n",
            "      Successfully uninstalled aiohttp-3.13.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.25.1\n",
            "    Uninstalling jsonschema-4.25.1:\n",
            "      Successfully uninstalled jsonschema-4.25.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.10.1\n",
            "    Uninstalling accelerate-1.10.1:\n",
            "      Successfully uninstalled accelerate-1.10.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 5.5.0\n",
            "    Uninstalling altair-5.5.0:\n",
            "      Successfully uninstalled altair-5.5.0\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 5.1.1\n",
            "    Uninstalling sentence-transformers-5.1.1:\n",
            "      Successfully uninstalled sentence-transformers-5.1.1\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.17.1\n",
            "    Uninstalling peft-0.17.1:\n",
            "      Successfully uninstalled peft-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\n",
            "google-genai 1.41.0 requires anyio<5.0.0,>=4.8.0, but you have anyio 4.6.0 which is incompatible.\n",
            "google-genai 1.41.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "bokeh 3.7.3 requires narwhals>=1.13, but you have narwhals 1.9.3 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "gradio 5.49.0 requires huggingface-hub<2.0,>=0.33.5, but you have huggingface-hub 0.25.1 which is incompatible.\n",
            "google-adk 1.14.1 requires anyio<5.0.0,>=4.9.0; python_version >= \"3.10\", but you have anyio 4.6.0 which is incompatible.\n",
            "google-adk 1.14.1 requires requests<3.0.0,>=2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "diffusers 0.35.1 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.25.1 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.4.1 which is incompatible.\n",
            "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.12.2 which is incompatible.\n",
            "sse-starlette 3.0.2 requires anyio>=4.7.0, but you have anyio 4.6.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "mcp 1.16.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.8.2 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "albumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 2.8.2 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.4.1 which is incompatible.\n",
            "xarray 2025.10.1 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.5 which is incompatible.\n",
            "grpcio-status 1.71.2 requires grpcio>=1.71.2, but you have grpcio 1.66.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.0.0 aiohappyeyeballs-2.4.2 aiohttp-3.10.5 aiosignal-1.3.1 altair-5.4.1 anthropic-0.39.0 anyio-4.6.0 attrs-24.2.0 backoff-2.2.1 bm25s-0.2.2 certifi-2024.8.30 charset-normalizer-3.3.2 cohere-4.57 colorama-0.4.6 colorlog-6.8.2 colpali-engine-0.3.1 contourpy-1.3.0 datasets-2.19.0 einops-0.8.0 fastavro-1.9.7 filelock-3.16.1 fonttools-4.54.1 frozenlist-1.4.1 fsspec-2024.3.1 gputil-1.4.0 grpcio-1.66.1 grpcio-tools-1.62.3 h11-0.14.0 h2-4.1.0 hf-transfer-0.1.8 hpack-4.0.0 httpcore-1.0.5 httpx-0.27.2 huggingface-hub-0.25.1 hyperframe-6.0.1 importlib-metadata-6.11.0 jinja2-3.1.4 jiter-0.5.0 joblib-1.4.2 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 kiwisolver-1.4.7 markdown-it-py-3.0.0 markupsafe-2.1.5 matplotlib-3.9.2 multidict-6.1.0 narwhals-1.9.3 networkx-3.3 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 openai-1.48.0 packaging-23.2 pdf2image-1.17.0 peft-0.11.1 pillow-10.4.0 portalocker-2.10.1 protobuf-4.25.5 psutil-6.0.0 pyarrow-17.0.0 pyarrow-hotfix-0.6 pydantic-2.8.2 pydantic-core-2.20.1 pygments-2.18.0 pymupdf-1.24.13 pymupdf4llm-0.0.17 pyparsing-3.1.4 pystemmer-2.2.0.3 python-dotenv-1.0.1 pytz-2024.2 pyyaml-6.0.2 qdrant-client-1.11.3 referencing-0.35.1 regex-2023.12.25 requests-2.32.3 requests-mock-1.12.1 rich-13.8.1 rich-theme-manager-0.11.0 rpds-py-0.20.0 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 semantic-chunkers-0.0.4 semantic-router-0.0.37 sentence-transformers-3.0.1 setuptools-75.1.0 six-1.16.0 stamina-24.3.0 threadpoolctl-3.5.0 tiktoken-0.6.0 tokenizers-0.20.0 torch-2.4.1 tqdm-4.66.5 transformers-4.45.1 triton-3.0.0 typing-extensions-4.12.2 tzdata-2024.2 urllib3-2.2.3 vegafusion-1.6.9 vegafusion-python-embed-1.6.9 vl-convert-python-1.7.0 xxhash-3.5.0 yarl-1.13.0 zipp-3.20.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_distutils_hack",
                  "certifi",
                  "google",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "packaging",
                  "psutil",
                  "pyparsing",
                  "six"
                ]
              },
              "id": "39a98ea512d74548a9b626aebd26495b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/guyernest/advanced-rag.git\n",
        "%cd advanced-rag\n",
        "!pip install --upgrade -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision==0.18.0\n",
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMsSBOGHGZaJ",
        "outputId": "b5191fa2-cf57-4d4e-b600-700970f0a2a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.18.0\n",
            "  Using cached torchvision-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.0) (1.26.4)\n",
            "Collecting torch==2.3.0 (from torchvision==0.18.0)\n",
            "  Using cached torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.0) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->torchvision==0.18.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision==0.18.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.0->torchvision==0.18.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.0->torchvision==0.18.0) (1.3.0)\n",
            "Using cached torchvision-0.18.0-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
            "Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cudnn-cu12, torch, torchvision\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1\n",
            "    Uninstalling torch-2.4.1:\n",
            "      Successfully uninstalled torch-2.4.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-8.9.2.26 torch-2.3.0 torchvision-0.18.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-tools 1.62.3 requires protobuf<5.0dev,>=4.21.6, but you have protobuf 5.29.5 which is incompatible.\n",
            "google-adk 1.14.1 requires anyio<5.0.0,>=4.9.0; python_version >= \"3.10\", but you have anyio 4.6.0 which is incompatible.\n",
            "google-adk 1.14.1 requires requests<3.0.0,>=2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.5 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "import pathlib\n",
        "from rich_theme_manager import Theme, ThemeManager\n",
        "\n",
        "THEMES = [\n",
        "    Theme(\n",
        "        name=\"dark\",\n",
        "        description=\"Dark mode theme\",\n",
        "        tags=[\"dark\"],\n",
        "        styles={\n",
        "            \"repr.own\": Style(color=\"#e87d3e\", bold=True),      # Class names\n",
        "            \"repr.tag_name\": \"dim cyan\",                        # Adjust tag names\n",
        "            \"repr.call\": \"bright_yellow\",                       # Function calls and other symbols\n",
        "            \"repr.str\": \"bright_green\",                         # String representation\n",
        "            \"repr.number\": \"bright_red\",                        # Numbers\n",
        "            \"repr.none\": \"dim white\",                           # None\n",
        "            \"repr.attrib_name\": Style(color=\"#e87d3e\", bold=True),    # Attribute names\n",
        "            \"repr.attrib_value\": \"bright_blue\",                 # Attribute values\n",
        "            \"default\": \"bright_white on black\"                  # Default text and background\n",
        "        },\n",
        "    ),\n",
        "    Theme(\n",
        "        name=\"light\",\n",
        "        description=\"Light mode theme\",\n",
        "        styles={\n",
        "            \"repr.own\": Style(color=\"#22863a\", bold=True),          # Class names\n",
        "            \"repr.tag_name\": Style(color=\"#00bfff\", bold=True),     # Adjust tag names\n",
        "            \"repr.call\": Style(color=\"#ffff00\", bold=True),         # Function calls and other symbols\n",
        "            \"repr.str\": Style(color=\"#008080\", bold=True),          # String representation\n",
        "            \"repr.number\": Style(color=\"#ff6347\", bold=True),       # Numbers\n",
        "            \"repr.none\": Style(color=\"#808080\", bold=True),         # None\n",
        "            \"repr.attrib_name\": Style(color=\"#ffff00\", bold=True),  # Attribute names\n",
        "            \"repr.attrib_value\": Style(color=\"#008080\", bold=True), # Attribute values\n",
        "            \"default\": Style(color=\"#000000\", bgcolor=\"#ffffff\"),   # Default text and background\n",
        "        },\n",
        "    ),\n",
        "]\n",
        "\n",
        "theme_dir = pathlib.Path(\"themes\").expanduser()\n",
        "theme_dir.expanduser().mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "theme_manager = ThemeManager(theme_dir=theme_dir, themes=THEMES)\n",
        "theme_manager.list_themes()\n",
        "\n",
        "dark = theme_manager.get(\"dark\")\n",
        "theme_manager.preview_theme(dark)\n",
        "light = theme_manager.get(\"light\")\n",
        "\n",
        "console = Console(theme=light)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "Eat_nyFnIOFX",
        "outputId": "1f5b8c73-2dfe-4916-830a-2aecd502f468"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m \u001b[0m\u001b[1mTheme\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mDescription     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mTags\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mPath              \u001b[0m\u001b[1m \u001b[0m\n",
              " dark   Dark mode theme   dark  themes/dark.theme  \n",
              " light  Light mode theme        themes/light.theme \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Theme  Description       Tags  Path               </span>\n",
              " dark   Dark mode theme   dark  themes/dark.theme  \n",
              " light  Light mode theme        themes/light.theme \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                                      Theme: dark - themes/dark.theme                                      \u001b[0m\n",
              "┌───────────────────┬───────────────┬───────┬─────────┬─────────┬────────────────┬────────────────────────┐\n",
              "│\u001b[1m \u001b[0m\u001b[1mstyle            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mcolor        \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mcolor\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mbgcolor\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mbgcolor\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mattributes    \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mexample               \u001b[0m\u001b[1m \u001b[0m│\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.own          │ #e87d3e       │ \u001b[38;2;232;125;62m█████\u001b[0m │ None    │         │ \u001b[1mb\u001b[0m------------- │ \u001b[1;38;2;232;125;62mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.tag_name     │ cyan          │ \u001b[36m█████\u001b[0m │ None    │         │ -\u001b[1md\u001b[0m------------ │ \u001b[2;36mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.call         │ bright_yellow │ \u001b[93m█████\u001b[0m │ None    │         │ -------------- │ \u001b[93mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.str          │ bright_green  │ \u001b[92m█████\u001b[0m │ None    │         │ -------------- │ \u001b[92mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.number       │ bright_red    │ \u001b[91m█████\u001b[0m │ None    │         │ -------------- │ \u001b[91mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.none         │ white         │ \u001b[37m█████\u001b[0m │ None    │         │ -\u001b[1md\u001b[0m------------ │ \u001b[2;37mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.attrib_name  │ #e87d3e       │ \u001b[38;2;232;125;62m█████\u001b[0m │ None    │         │ \u001b[1mb\u001b[0m------------- │ \u001b[1;38;2;232;125;62mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.attrib_value │ bright_blue   │ \u001b[94m█████\u001b[0m │ None    │         │ -------------- │ \u001b[94mThe quick brown fox...\u001b[0m │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ default           │ bright_white  │ \u001b[97m█████\u001b[0m │ black   │ \u001b[30m█████  \u001b[0m │ -------------- │ \u001b[97;40mThe quick brown fox...\u001b[0m │\n",
              "└───────────────────┴───────────────┴───────┴─────────┴─────────┴────────────────┴────────────────────────┘\n",
              "┌─ attributes legend ──────────────────────────────────────────────────────────────────┐\n",
              "│  \u001b[1mb\u001b[0m: bold, \u001b[1md\u001b[0m: dim, \u001b[1mi\u001b[0m: italic, \u001b[1mu\u001b[0m: underline, \u001b[1mU\u001b[0m: double underline, \u001b[1mB\u001b[0m: blink, \u001b[1m2\u001b[0m: blink2  │\n",
              "│  \u001b[1mr\u001b[0m: reverse, \u001b[1mc\u001b[0m: conceal, \u001b[1ms\u001b[0m: strike, \u001b[1mf\u001b[0m: frame, \u001b[1me\u001b[0m: encircle, \u001b[1mo\u001b[0m: overline, \u001b[1mL\u001b[0m: Link      │\n",
              "└──────────────────────────────────────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                      Theme: dark - themes/dark.theme                                      </span>\n",
              "┌───────────────────┬───────────────┬───────┬─────────┬─────────┬────────────────┬────────────────────────┐\n",
              "│<span style=\"font-weight: bold\"> style             </span>│<span style=\"font-weight: bold\"> color         </span>│<span style=\"font-weight: bold\"> color </span>│<span style=\"font-weight: bold\"> bgcolor </span>│<span style=\"font-weight: bold\"> bgcolor </span>│<span style=\"font-weight: bold\"> attributes     </span>│<span style=\"font-weight: bold\"> example                </span>│\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.own          │ #e87d3e       │ <span style=\"color: #e87d3e; text-decoration-color: #e87d3e\">█████</span> │ None    │         │ <span style=\"font-weight: bold\">b</span>------------- │ <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.tag_name     │ cyan          │ <span style=\"color: #008080; text-decoration-color: #008080\">█████</span> │ None    │         │ -<span style=\"font-weight: bold\">d</span>------------ │ <span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.call         │ bright_yellow │ <span style=\"color: #ffff00; text-decoration-color: #ffff00\">█████</span> │ None    │         │ -------------- │ <span style=\"color: #ffff00; text-decoration-color: #ffff00\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.str          │ bright_green  │ <span style=\"color: #00ff00; text-decoration-color: #00ff00\">█████</span> │ None    │         │ -------------- │ <span style=\"color: #00ff00; text-decoration-color: #00ff00\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.number       │ bright_red    │ <span style=\"color: #ff0000; text-decoration-color: #ff0000\">█████</span> │ None    │         │ -------------- │ <span style=\"color: #ff0000; text-decoration-color: #ff0000\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.none         │ white         │ <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">█████</span> │ None    │         │ -<span style=\"font-weight: bold\">d</span>------------ │ <span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.attrib_name  │ #e87d3e       │ <span style=\"color: #e87d3e; text-decoration-color: #e87d3e\">█████</span> │ None    │         │ <span style=\"font-weight: bold\">b</span>------------- │ <span style=\"color: #e87d3e; text-decoration-color: #e87d3e; font-weight: bold\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ repr.attrib_value │ bright_blue   │ <span style=\"color: #0000ff; text-decoration-color: #0000ff\">█████</span> │ None    │         │ -------------- │ <span style=\"color: #0000ff; text-decoration-color: #0000ff\">The quick brown fox...</span> │\n",
              "├───────────────────┼───────────────┼───────┼─────────┼─────────┼────────────────┼────────────────────────┤\n",
              "│ default           │ bright_white  │ <span style=\"color: #ffffff; text-decoration-color: #ffffff\">█████</span> │ black   │ <span style=\"color: #000000; text-decoration-color: #000000\">█████  </span> │ -------------- │ <span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #000000\">The quick brown fox...</span> │\n",
              "└───────────────────┴───────────────┴───────┴─────────┴─────────┴────────────────┴────────────────────────┘\n",
              "┌─ attributes legend ──────────────────────────────────────────────────────────────────┐\n",
              "│  <span style=\"font-weight: bold\">b</span>: bold, <span style=\"font-weight: bold\">d</span>: dim, <span style=\"font-weight: bold\">i</span>: italic, <span style=\"font-weight: bold\">u</span>: underline, <span style=\"font-weight: bold\">U</span>: double underline, <span style=\"font-weight: bold\">B</span>: blink, <span style=\"font-weight: bold\">2</span>: blink2  │\n",
              "│  <span style=\"font-weight: bold\">r</span>: reverse, <span style=\"font-weight: bold\">c</span>: conceal, <span style=\"font-weight: bold\">s</span>: strike, <span style=\"font-weight: bold\">f</span>: frame, <span style=\"font-weight: bold\">e</span>: encircle, <span style=\"font-weight: bold\">o</span>: overline, <span style=\"font-weight: bold\">L</span>: Link      │\n",
              "└──────────────────────────────────────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "BWeWv05BIQyM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "console.print(cross_encoder.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991,
          "referenced_widgets": [
            "fe6a25a7aa734fd68195ee549f925ca1",
            "d6700ca64aab4cc98b33d62a555ad930",
            "ed98776d6a014baea216132d8e26a598",
            "b8fd6582e66a46d1868db90a59b4ade9",
            "92325bc6694c4d188c2b57aa46eeb0d3",
            "716faa679c5c41d7859372b0aa9ab5d5",
            "db822d6374884d0882e0e05d6717e5bb",
            "eb2f3eb941d54e0f91c80298ab7a7b69",
            "688ee1fafc464668af0361a71d1a9497",
            "6182914b34734308a65e51bab15770ee",
            "b0b17cb769894dc98584d2ac6e6a24a7",
            "d893c18bcd2c46ae969f82a8bd142990",
            "b6f10bf30e6d4956b92eaebcfd74979f",
            "afa4bcd23a2642a7b8cef9300d5fecc8",
            "2665b7d5999c43df8ee559c2c2c70370",
            "88a3b1de5d154f46ac61623819edb9d6",
            "cd985801a0174af5a4f941be05c8254a",
            "7b4afe8b6e4b45ae8083f2c47da59acc",
            "95c438e722b2405f8c43a7836bf1cb09",
            "0d6d82bd853d4ed8a4751616f0b8a09b",
            "83c4c3802a244034ac6dd6127390934d",
            "d8bbb0b01aeb41f2badb3f7deb94157e",
            "c57de25c55784e36aa6ace3adeef7358",
            "aa978d562bfb4120b14bbaf1229bb98f",
            "589093a94e204c7893c7a372b65868c1",
            "c9401f3fe56f4d8bb4c405adeb49ae9c",
            "582cd571162e4099b9b3fdaf8271399c",
            "5ed129d59fd649ac92adbac069ad5612",
            "623171362f3b4e89a4069ff6a2552881",
            "4a0ac3ddd4e443399cd8d58feda62b72",
            "1a91dfe1a88d4c6eae1f336ca90c24bf",
            "1cfa7ba4307648b9860238accf97eefa",
            "ea260eb0171f4e85a804831e81149de9",
            "e221dd5d757d42efb2462ea9a3ccb5b4",
            "73d4fccaa3ea4bf7a0756de4fd71a2db",
            "75d6f2b6dbc047dbad34878f2c380d30",
            "d099e2814cf94d168a7758083e7c36d5",
            "af18bfac0f65425a8d0eb609162691de",
            "f41e43b984c8404db915d935762a2c7c",
            "b311342c41154403870520248db4531f",
            "7a4b72ef7ced42bebd90408bf1710388",
            "1b9a4ea31c784aeba019962373a2aa7e",
            "09daf19781ee42debd4680fc321a93e8",
            "e565fb75c4ab4a7794844c0698d8aac4",
            "fb2f6a456124467c8263745becc2e6a4",
            "506388a899f74ff59ceed4a027866c30",
            "fec1a014b9de4a8586d729fb79c9cc0e",
            "ef25313df9a94dfd9f5e6b3a9e7957b9",
            "26c0e96159d340aebfc50814428dda21",
            "36d5a0c6251d4b0d94d6eaee01e9c0e6",
            "4f0b99703b4c47d79b7ace06749f8229",
            "05bbe1e4a2c84b44803bcf56ee042300",
            "125313f4f31f429cb9bf6898e06b004c",
            "493e770dfedc4ebeb0c2ae0758d735f7",
            "d2c53910f90c4a458f2e9e38f6c7c689",
            "360e26bdd44244e7af97581242213cc2",
            "7b4fbdd235724c16a6fd257bf6383d14",
            "b865d7f9ac864becb03542148a6b5192",
            "cde6f478963749d88755913132cec8e1",
            "c59872038f3c4ddda6df453b83f35c88",
            "740ba5b5fdf2483a9db8cb87b1223636",
            "08faff87057c48cfa2f4ecab7554c059",
            "0f64c4d3bc3041e7b42bcea21ac9d658",
            "81c56fde0a6d4494a3dfbb479e9391b5",
            "1ba32c389f4d47abafba6ebb58d86cb9",
            "c41c7f5f4b9645ccb59463bc35dbf401"
          ]
        },
        "id": "okjOl6huJICy",
        "outputId": "d3926074-a9f8-414a-849d-18e7eae2056e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe6a25a7aa734fd68195ee549f925ca1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d893c18bcd2c46ae969f82a8bd142990"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c57de25c55784e36aa6ace3adeef7358"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e221dd5d757d42efb2462ea9a3ccb5b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb2f6a456124467c8263745becc2e6a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "360e26bdd44244e7af97581242213cc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;255;255;0mBertForSequenceClassification\u001b[0m\u001b[1m(\u001b[0m\n",
              "  \u001b[1m(\u001b[0mbert\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mBertModel\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[1m(\u001b[0membeddings\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mBertEmbeddings\u001b[0m\u001b[1m(\u001b[0m\n",
              "      \u001b[1m(\u001b[0mword_embeddings\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;99;71m30522\u001b[0m, \u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mpadding_idx\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1m)\u001b[0m\n",
              "      \u001b[1m(\u001b[0mposition_embeddings\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;99;71m512\u001b[0m, \u001b[1;38;2;255;99;71m384\u001b[0m\u001b[1m)\u001b[0m\n",
              "      \u001b[1m(\u001b[0mtoken_type_embeddings\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;99;71m2\u001b[0m, \u001b[1;38;2;255;99;71m384\u001b[0m\u001b[1m)\u001b[0m\n",
              "      \u001b[1m(\u001b[0mLayerNorm\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;99;71m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[1;38;2;255;255;0meps\u001b[0m=\u001b[1;38;2;255;99;71m1e\u001b[0m\u001b[1;38;2;255;99;71m-12\u001b[0m, \u001b[1;38;2;255;255;0melementwise_affine\u001b[0m=\u001b[1;3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
              "      \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0mp\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.1\u001b[0m, \u001b[1;38;2;255;255;0minplace\u001b[0m=\u001b[1;3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "    \u001b[1m(\u001b[0mencoder\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mBertEncoder\u001b[0m\u001b[1m(\u001b[0m\n",
              "      \u001b[1m(\u001b[0mlayer\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1m(\u001b[0m\u001b[1;38;2;255;99;71m0\u001b[0m-\u001b[1;38;2;255;99;71m5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;38;2;255;99;71m6\u001b[0m x \u001b[1;38;2;255;255;0mBertLayer\u001b[0m\u001b[1m(\u001b[0m\n",
              "          \u001b[1m(\u001b[0mattention\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mBertAttention\u001b[0m\u001b[1m(\u001b[0m\n",
              "            \u001b[1m(\u001b[0mself\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mBertSdpaSelfAttention\u001b[0m\u001b[1m(\u001b[0m\n",
              "              \u001b[1m(\u001b[0mquery\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0min_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mout_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mbias\u001b[0m=\u001b[1;3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
              "              \u001b[1m(\u001b[0mkey\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0min_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mout_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mbias\u001b[0m=\u001b[1;3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
              "              \u001b[1m(\u001b[0mvalue\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0min_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mout_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mbias\u001b[0m=\u001b[1;3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
              "              \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0mp\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.1\u001b[0m, \u001b[1;38;2;255;255;0minplace\u001b[0m=\u001b[1;3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
              "            \u001b[1m)\u001b[0m\n",
              "            \u001b[1m(\u001b[0moutput\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mBertSelfOutput\u001b[0m\u001b[1m(\u001b[0m\n",
              "              \u001b[1m(\u001b[0mdense\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0min_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mout_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mbias\u001b[0m=\u001b[1;3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
              "              \u001b[1m(\u001b[0mLayerNorm\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;99;71m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[1;38;2;255;255;0meps\u001b[0m=\u001b[1;38;2;255;99;71m1e\u001b[0m\u001b[1;38;2;255;99;71m-12\u001b[0m, \u001b[1;38;2;255;255;0melementwise_affine\u001b[0m=\u001b[1;3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
              "              \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0mp\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.1\u001b[0m, \u001b[1;38;2;255;255;0minplace\u001b[0m=\u001b[1;3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
              "            \u001b[1m)\u001b[0m\n",
              "          \u001b[1m)\u001b[0m\n",
              "          \u001b[1m(\u001b[0mintermediate\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mBertIntermediate\u001b[0m\u001b[1m(\u001b[0m\n",
              "            \u001b[1m(\u001b[0mdense\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0min_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mout_features\u001b[0m=\u001b[1;38;2;255;99;71m1536\u001b[0m, \u001b[1;38;2;255;255;0mbias\u001b[0m=\u001b[1;3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
              "            \u001b[1m(\u001b[0mintermediate_act_fn\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mGELUActivation\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
              "          \u001b[1m)\u001b[0m\n",
              "          \u001b[1m(\u001b[0moutput\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mBertOutput\u001b[0m\u001b[1m(\u001b[0m\n",
              "            \u001b[1m(\u001b[0mdense\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0min_features\u001b[0m=\u001b[1;38;2;255;99;71m1536\u001b[0m, \u001b[1;38;2;255;255;0mout_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mbias\u001b[0m=\u001b[1;3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
              "            \u001b[1m(\u001b[0mLayerNorm\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;99;71m384\u001b[0m,\u001b[1m)\u001b[0m, \u001b[1;38;2;255;255;0meps\u001b[0m=\u001b[1;38;2;255;99;71m1e\u001b[0m\u001b[1;38;2;255;99;71m-12\u001b[0m, \u001b[1;38;2;255;255;0melementwise_affine\u001b[0m=\u001b[1;3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
              "            \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0mp\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.1\u001b[0m, \u001b[1;38;2;255;255;0minplace\u001b[0m=\u001b[1;3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
              "          \u001b[1m)\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "      \u001b[1m)\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "    \u001b[1m(\u001b[0mpooler\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mBertPooler\u001b[0m\u001b[1m(\u001b[0m\n",
              "      \u001b[1m(\u001b[0mdense\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0min_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mout_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mbias\u001b[0m=\u001b[1;3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
              "      \u001b[1m(\u001b[0mactivation\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mTanh\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "  \u001b[1m)\u001b[0m\n",
              "  \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0mp\u001b[0m=\u001b[1;38;2;255;99;71m0\u001b[0m\u001b[1;38;2;255;99;71m.1\u001b[0m, \u001b[1;38;2;255;255;0minplace\u001b[0m=\u001b[1;3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
              "  \u001b[1m(\u001b[0mclassifier\u001b[1m)\u001b[0m: \u001b[1;38;2;255;255;0mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[1;38;2;255;255;0min_features\u001b[0m=\u001b[1;38;2;255;99;71m384\u001b[0m, \u001b[1;38;2;255;255;0mout_features\u001b[0m=\u001b[1;38;2;255;99;71m1\u001b[0m, \u001b[1;38;2;255;255;0mbias\u001b[0m=\u001b[1;3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">BertForSequenceClassification</span><span style=\"font-weight: bold\">(</span>\n",
              "  <span style=\"font-weight: bold\">(</span>bert<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">BertModel</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"font-weight: bold\">(</span>embeddings<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">BertEmbeddings</span><span style=\"font-weight: bold\">(</span>\n",
              "      <span style=\"font-weight: bold\">(</span>word_embeddings<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">30522</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">padding_idx</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>\n",
              "      <span style=\"font-weight: bold\">(</span>position_embeddings<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">512</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span><span style=\"font-weight: bold\">)</span>\n",
              "      <span style=\"font-weight: bold\">(</span>token_type_embeddings<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">2</span>, <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span><span style=\"font-weight: bold\">)</span>\n",
              "      <span style=\"font-weight: bold\">(</span>LayerNorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">eps</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1e-12</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
              "      <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">p</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.1</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">(</span>encoder<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">BertEncoder</span><span style=\"font-weight: bold\">(</span>\n",
              "      <span style=\"font-weight: bold\">(</span>layer<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">ModuleList</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"font-weight: bold\">(</span><span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>-<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">6</span> x <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">BertLayer</span><span style=\"font-weight: bold\">(</span>\n",
              "          <span style=\"font-weight: bold\">(</span>attention<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">BertAttention</span><span style=\"font-weight: bold\">(</span>\n",
              "            <span style=\"font-weight: bold\">(</span>self<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">BertSdpaSelfAttention</span><span style=\"font-weight: bold\">(</span>\n",
              "              <span style=\"font-weight: bold\">(</span>query<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">in_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">out_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
              "              <span style=\"font-weight: bold\">(</span>key<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">in_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">out_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
              "              <span style=\"font-weight: bold\">(</span>value<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">in_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">out_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
              "              <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">p</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.1</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "            <span style=\"font-weight: bold\">)</span>\n",
              "            <span style=\"font-weight: bold\">(</span>output<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">BertSelfOutput</span><span style=\"font-weight: bold\">(</span>\n",
              "              <span style=\"font-weight: bold\">(</span>dense<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">in_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">out_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
              "              <span style=\"font-weight: bold\">(</span>LayerNorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">eps</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1e-12</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
              "              <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">p</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.1</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "            <span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">(</span>intermediate<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">BertIntermediate</span><span style=\"font-weight: bold\">(</span>\n",
              "            <span style=\"font-weight: bold\">(</span>dense<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">in_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">out_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1536</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
              "            <span style=\"font-weight: bold\">(</span>intermediate_act_fn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">GELUActivation</span><span style=\"font-weight: bold\">()</span>\n",
              "          <span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">(</span>output<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">BertOutput</span><span style=\"font-weight: bold\">(</span>\n",
              "            <span style=\"font-weight: bold\">(</span>dense<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">in_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1536</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">out_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
              "            <span style=\"font-weight: bold\">(</span>LayerNorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">eps</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1e-12</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
              "            <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">p</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.1</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">)</span>\n",
              "        <span style=\"font-weight: bold\">)</span>\n",
              "      <span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">(</span>pooler<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">BertPooler</span><span style=\"font-weight: bold\">(</span>\n",
              "      <span style=\"font-weight: bold\">(</span>dense<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">in_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">out_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
              "      <span style=\"font-weight: bold\">(</span>activation<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Tanh</span><span style=\"font-weight: bold\">()</span>\n",
              "    <span style=\"font-weight: bold\">)</span>\n",
              "  <span style=\"font-weight: bold\">)</span>\n",
              "  <span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">p</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0.1</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "  <span style=\"font-weight: bold\">(</span>classifier<span style=\"font-weight: bold\">)</span>: <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">in_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">384</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">out_features</span>=<span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1</span>, <span style=\"color: #ffff00; text-decoration-color: #ffff00; font-weight: bold\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "hybrid_search_results = {}\n",
        "with open('advanced-rag/data/dense_results.json') as f:\n",
        "    dense_results = json.load(f)\n",
        "    for doc in dense_results:\n",
        "        hybrid_search_results[doc['id']] = doc\n",
        "with open('advanced-rag/data/sparse_results.json') as f:\n",
        "    sparse_results = json.load(f)\n",
        "    for doc in sparse_results:\n",
        "        hybrid_search_results[doc['id']] = doc\n",
        "console.print(hybrid_search_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fuG8FYadK2xO",
        "outputId": "30f4d4e5-b20d-4f3b-ec85-12045f8aaa54"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[1;38;2;255;99;71m15\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m15\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mPerformance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mLeft\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m Mixtral has 100% \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mretrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mRight\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mincreases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mretrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe proof-pile dataset decreases as the context length increases.\"\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m4\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m4\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad \u001b[0m\n",
              "\u001b[1;38;2;0;128;128maccessibility and potential for diverse applications. To enable the community to run Mixtral with a fully \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mopen-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient\u001b[0m\n",
              "\u001b[1;38;2;0;128;128minference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdetails Mixtral is based on a transformer architecture \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m31\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m and uses the same modifications as described in \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m18\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwith the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mforward blocks are replaced by Mixture-of-Expert layers \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSection 2.1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. The model architecture parameters are \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msummarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mits use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmentions the model's open-source licensing and deployment options.\"\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m2\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m2\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'expertsâ \u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m to process the token and combine their output additively. This technique increases the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mnumber of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mset of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It \u001b[0m\n",
              "\u001b[1;38;2;0;128;128meither matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdetails of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m2 70B and GPT-3.5 on various benchmarks.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m6\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m6\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Table 1: Model architecture. # j nâ G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi Â· Ei\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. \u001b[0m\u001b[1;38;2;0;128;128mi\u001b[0m\u001b[1;38;2;0;128;128m=\u001b[0m\u001b[1;38;2;0;128;128m0\u001b[0m\u001b[1;38;2;0;128;128m Here, G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi denotes the n-dimensional \u001b[0m\n",
              "\u001b[1;38;2;0;128;128moutput of the gating network for the i-th expert, and Ei\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m is the output of the i-th expert network. If the gating\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mvector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple \u001b[0m\n",
              "\u001b[1;38;2;0;128;128malternative ways of implementing G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m6, 15, 35\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m, but a simple and performant one is implemented by taking the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msoftmax over the Top-K logits of a linear layer \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m28\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m.\\n\\nThe chunk describes the architectural details of the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral model, specifically the Sparse Mixture of Experts \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSMoE\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m layer that is used in the model.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m7\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m7\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'We use G\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m := Softmax\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mTopK\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mx Â· Wg\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m, where \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mTopK\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mâ \u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi := â i if â i is among the top-K \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcoordinates of logits â â Rn and \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mTopK\u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mâ \u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128mi := â â otherwise. The value of K â the number of experts used per \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mtoken â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwhile keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmechanism used in the Mixture of Experts \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mMoE\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m layer of the Mixtral model. It explains how the router network \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mselects the top-K experts to process each token, and how this allows the model to increase its parameter count \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwhile keeping the computational cost constant.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m42\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m42\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'10 \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m34\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint \u001b[0m\n",
              "\u001b[1;38;2;0;128;128marXiv:2304.06364, 2023. \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m35\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mQuoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwhich is relevant to the overall topic of the document discussing the Mixtral language model.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m45\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m45\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mStackExchange â e-â Wikipedia \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128men\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated \u001b[0m\n",
              "\u001b[1;38;2;0;128;128massignments occur a lot more often than they would with uniform assignments \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mmaterialized by the dashed lines\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mPatterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128manalysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mlocality in the expert selection, especially at higher layers of the model. This analysis provides insights into \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe behavior of the sparse mixture-of-experts architecture used in Mixtral.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m0\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m0\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'4 2 0 2 n a J 8 \u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m G L . s c \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mJiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mDiego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mRenard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mAntoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mAbstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mSMoE\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m language model. Mixtral has the same \u001b[0m\n",
              "\u001b[1;38;2;0;128;128marchitecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mi.e. experts\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m.\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mFor every token, at each layer, a router network selects two experts to process the current state and combine their\u001b[0m\n",
              "\u001b[1;38;2;0;128;128moutputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mresult, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mtrained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mevaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmultilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msurpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m5\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m5\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mnum_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mlayer \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mFigure 1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. For a more in-depth overview, see \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m12\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m. The output of the MoE module for a given input x is \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mdetermined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mnetworkâ s output. i.e. given n expert networks \u001b[0m\u001b[1;38;2;0;128;128m{\u001b[0m\u001b[1;38;2;0;128;128mE0, Ei, ..., Enâ 1\u001b[0m\u001b[1;38;2;0;128;128m}\u001b[0m\u001b[1;38;2;0;128;128m, the output of the expert layer is given \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mby:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mExperts layer that is a key component of the model.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m11\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m11\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348\u001b[0m\n",
              "\u001b[1;38;2;0;128;128m70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLlaMA2 78 \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m138 348 70B 7B \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mon MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m7B/8x7B\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m vs \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLlama 2 \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m7B/13B/70B\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mbenchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mDetailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcompares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B \u001b[0m\n",
              "\u001b[1;38;2;0;128;128macross most metrics. In particular, Mixtral displays a superior performance in code and mathematics \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mbenchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models \u001b[0m\n",
              "\u001b[1;38;2;0;128;128magainst the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mreading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmetrics while using significantly fewer active parameters.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m14\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m14\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m\"Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mArc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m50.5% 74.5% 66.0% Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mHellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian. # 3.2 \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLong range performance To assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mretrieval task introduced in \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m23\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m, a synthetic task designed to measure the ability of the model to retrieve a \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mpasskey inserted randomly in a long prompt. Results in Figure 4 \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mLeft\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m show that Mixtral achieves a 100% retrieval \u001b[0m\n",
              "\u001b[1;38;2;0;128;128maccuracy regardless of the context length or the position of passkey in the sequence. Figure 4 \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128mRight\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m shows that \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mthe perplexity of Mixtral on a subset of the proof-pile dataset \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m2\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m decreases monotonically as the size of the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcontext increases. Passkey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLoc\\n\\nThe chunk discusses Mixtral's performance on multilingual benchmarks and its ability to handle long-range \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcontext, demonstrating its strong capabilities in these areas.\"\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m12\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m12\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral modelsâ efficiency in the cost-performance spectrum \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128msee Figure 3\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. As a sparse Mixture- of-Experts model, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to \u001b[0m\n",
              "\u001b[1;38;2;0;128;128moutperform Llama 2 70B across most categories. Note that this analysis focuses on the active parameter count \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128msee \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mSection 2.1\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m, which is directly proportional to the inference compute cost, but does not consider the memory costs \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mand hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B,\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mwhich is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces \u001b[0m\n",
              "\u001b[1;38;2;0;128;128madditional overhead due to the routing mechanism and due to the increased memory loads when running more than one \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mexpert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mintensity. Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mto Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral obtains a better performance, despite its significantly smaller capacity \u001b[0m\u001b[1;38;2;0;128;128m(\u001b[0m\u001b[1;38;2;0;128;128m47B tokens compared to 70B\u001b[0m\u001b[1;38;2;0;128;128m)\u001b[0m\u001b[1;38;2;0;128;128m. For \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106. 2Since Llama 2\u001b[0m\n",
              "\u001b[1;38;2;0;128;128m34B was not open-sourced, we report results for Llama 1 34B.\\n\\nThis chunk discusses the size and efficiency of the\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixtral model, comparing its performance to the Llama 2 family of models. It highlights that Mixtral, as a sparse \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmixture-of-experts model, uses significantly fewer active parameters than Llama 2 70B while outperforming it across\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mmost benchmarks. The chunk also compares the performance of Mixtral 8x7B to Llama 2 70B and GPT-3.5.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m41\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m41\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m33\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mLi, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, \u001b[0m\n",
              "\u001b[1;38;2;0;128;128m2023.\\n\\nThe chunk discusses two references related to language model benchmarking, including the Hellaswag dataset\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mand the MT-Bench and Chatbot Arena benchmarks. This is situated within the broader context of the paper, which \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mintroduces the Mixtral language model and evaluates its performance on various benchmarks.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m3\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m3\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mrouter. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mis a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcapabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly \u001b[0m\n",
              "\u001b[1;38;2;0;128;128moutperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve \u001b[0m\n",
              "\u001b[1;38;2;0;128;128minformation from its context window of 32k tokens, regardless of the sequence length and the location of the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128minformation in the sequence. We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow \u001b[0m\n",
              "\u001b[1;38;2;0;128;128minstructions using supervised fine-tuning and Direct Preference Optimization \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m25\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m. Its performance notably \u001b[0m\n",
              "\u001b[1;38;2;0;128;128msurpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mbenchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mbenchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â\\n\\nThis chunk describes the \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMixture of Experts layer architecture used in the Mixtral model, as well as the superior performance of Mixtral \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mcompared to other models on various benchmarks, including mathematics, code generation, and multilingual tasks. It \u001b[0m\n",
              "\u001b[1;38;2;0;128;128malso introduces the Mixtral 8x7B - Instruct model, which is fine-tuned to follow instructions and outperforms other\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mchat models on human evaluation benchmarks.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m30\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m30\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Quac: Question answering in context. arXiv preprint arXiv:1808.07036, 2018. \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m6\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Aidan Clark, Diego\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mDe Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mCai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International Conference on \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMachine Learning, pages 4057â 4086. PMLR, 2022. \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m7\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mMichael Collins, and Kristina Toutanova.\\n\\nThe chunk discusses references related to question answering and \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mlanguage models, which are relevant topics covered in the overall document.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m,\n",
              "    \u001b[1;38;2;255;99;71m37\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[1;38;2;0;128;128m'id'\u001b[0m: \u001b[1;38;2;255;99;71m37\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'text'\u001b[0m: \u001b[1;38;2;0;128;128m'Landmark attention: Random-access infinite context length for transformers. arXiv preprint \u001b[0m\n",
              "\u001b[1;38;2;0;128;128marXiv:2305.16300, 2023. \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m24\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mThomp- son, Phu Mon Htut, and Samuel R Bowman. Bbq: A hand-built bias benchmark for question answering. arXiv \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mpreprint arXiv:2110.08193, 2021. \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m25\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mManning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mpreprint arXiv:2305.18290, 2023. \u001b[0m\u001b[1;38;2;0;128;128m[\u001b[0m\u001b[1;38;2;0;128;128m26\u001b[0m\u001b[1;38;2;0;128;128m]\u001b[0m\u001b[1;38;2;0;128;128m Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.\\n\\nThe\u001b[0m\n",
              "\u001b[1;38;2;0;128;128mchunk discusses references related to long-range performance, bias benchmarks, and instruction fine-tuning of \u001b[0m\n",
              "\u001b[1;38;2;0;128;128mlanguage models, which are topics covered in the overall document.'\u001b[0m,\n",
              "        \u001b[1;38;2;0;128;128m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1;38;2;0;128;128m'title'\u001b[0m: \u001b[1;38;2;0;128;128m'Mixtral of Experts'\u001b[0m, \u001b[1;38;2;0;128;128m'arxiv_id'\u001b[0m: \u001b[1;38;2;0;128;128m'2401.04088'\u001b[0m, \u001b[1;38;2;0;128;128m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;38;2;0;128;128m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">15</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">15</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"3.8 â Mixtral_8x7B 3.5 32 &gt; $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 &gt; 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100% </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the proof-pile dataset decreases as the context length increases.\"</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">4</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">4</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mentions the model's open-source licensing and deployment options.\"</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">2</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">2</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'expertsâ ) to process the token and combine their output additively. This technique increases the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2 70B and GPT-3.5 on various benchmarks.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">6</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">6</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Table 1: Model architecture. # j nâ G(x)i Â· Ei(x). i=0 Here, G(x)i denotes the n-dimensional </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network. If the gating</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">softmax over the Top-K logits of a linear layer [28].\\n\\nThe chunk describes the architectural details of the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral model, specifically the Sparse Mixture of Experts (SMoE) layer that is used in the model.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">7</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">7</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'We use G(x) := Softmax(TopK(x Â· Wg)), where (TopK(â ))i := â i if â i is among the top-K </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">coordinates of logits â â Rn and (TopK(â ))i := â â otherwise. The value of K â the number of experts used per </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">token â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">while keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mechanism used in the Mixture of Experts (MoE) layer of the Mixtral model. It explains how the router network </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">selects the top-K experts to process each token, and how this allows the model to increase its parameter count </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">while keeping the computational cost constant.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">42</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">42</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'10 [34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">arXiv:2304.06364, 2023. [35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Quoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">which is relevant to the overall topic of the document discussing the Mixtral language model.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">45</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">45</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">StackExchange â e-â Wikipedia (en) # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">assignments occur a lot more often than they would with uniform assignments (materialized by the dashed lines). </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Patterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">analysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">locality in the expert selection, especially at higher layers of the model. This analysis provides insights into </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the behavior of the sparse mixture-of-experts architecture used in Mixtral.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">0</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts).</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">For every token, at each layer, a router network selects two experts to process the current state and combine their</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">5</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">5</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">by:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Experts layer that is a key component of the model.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">11</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">11</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">across most metrics. In particular, Mixtral displays a superior performance in code and mathematics </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">benchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">against the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">reading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">metrics while using significantly fewer active parameters.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">14</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">14</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">\"Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50.5% 74.5% 66.0% Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian. # 3.2 </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Long range performance To assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a 100% retrieval </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 (Right) shows that </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size of the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">context increases. Passkey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Loc\\n\\nThe chunk discusses Mixtral's performance on multilingual benchmarks and its ability to handle long-range </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">context, demonstrating its strong capabilities in these areas.\"</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">12</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">12</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">outperform Llama 2 70B across most categories. Note that this analysis focuses on the active parameter count (see </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B,</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">additional overhead due to the routing mechanism and due to the increased memory loads when running more than one </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">intensity. Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106. 2Since Llama 2</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34B was not open-sourced, we report results for Llama 1 34B.\\n\\nThis chunk discusses the size and efficiency of the</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixtral model, comparing its performance to the Llama 2 family of models. It highlights that Mixtral, as a sparse </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">mixture-of-experts model, uses significantly fewer active parameters than Llama 2 70B while outperforming it across</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">most benchmarks. The chunk also compares the performance of Mixtral 8x7B to Llama 2 70B and GPT-3.5.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">41</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">41</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [33] </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023.\\n\\nThe chunk discusses two references related to language model benchmarking, including the Hellaswag dataset</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">and the MT-Bench and Chatbot Arena benchmarks. This is situated within the broader context of the paper, which </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">introduces the Mixtral language model and evaluates its performance on various benchmarks.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">3</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">3</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">information from its context window of 32k tokens, regardless of the sequence length and the location of the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">information in the sequence. We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â\\n\\nThis chunk describes the </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Mixture of Experts layer architecture used in the Mixtral model, as well as the superior performance of Mixtral </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">compared to other models on various benchmarks, including mathematics, code generation, and multilingual tasks. It </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">also introduces the Mixtral 8x7B - Instruct model, which is fine-tuned to follow instructions and outperforms other</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">chat models on human evaluation benchmarks.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">30</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">30</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Quac: Question answering in context. arXiv preprint arXiv:1808.07036, 2018. [6] Aidan Clark, Diego</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International Conference on </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Machine Learning, pages 4057â 4086. PMLR, 2022. [7] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Michael Collins, and Kristina Toutanova.\\n\\nThe chunk discusses references related to question answering and </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">language models, which are relevant topics covered in the overall document.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">37</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'id'</span>: <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">37</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'text'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Landmark attention: Random-access infinite context length for transformers. arXiv preprint </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">arXiv:2305.16300, 2023. [24] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Thomp- son, Phu Mon Htut, and Samuel R Bowman. Bbq: A hand-built bias benchmark for question answering. arXiv </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">preprint arXiv:2110.08193, 2021. [25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">preprint arXiv:2305.18290, 2023. [26] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.\\n\\nThe</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">chunk discusses references related to long-range performance, bias benchmarks, and instruction fine-tuning of </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">language models, which are topics covered in the overall document.'</span>,\n",
              "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'Mixtral of Experts'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'arxiv_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'2401.04088'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the query that we used for the retrieval of the above documents\n",
        "query = \"What is context size of Mixtral?\""
      ],
      "metadata": {
        "id": "v-e_zLCIMiJk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [[query, doc['text']] for doc in hybrid_search_results.values()]\n",
        "scores = cross_encoder.predict(pairs)\n",
        "\n",
        "console.print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "iI82JjOsMoUY",
        "outputId": "ebfd9f2e-b3f1-4bdc-b9e1-64cd32ffa74a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m  \u001b[1;38;2;255;99;71m5.065693\u001b[0m    \u001b[1;38;2;255;99;71m3.3688302\u001b[0m   \u001b[1;38;2;255;99;71m7.1048393\u001b[0m  \u001b[1;38;2;255;99;71m-4.116105\u001b[0m   \u001b[1;38;2;255;99;71m-4.375498\u001b[0m   \u001b[1;38;2;255;99;71m-5.261078\u001b[0m\n",
              "  \u001b[1;38;2;255;99;71m-3.7225776\u001b[0m   \u001b[1;38;2;255;99;71m3.1854541\u001b[0m   \u001b[1;38;2;255;99;71m1.7966672\u001b[0m  \u001b[1;38;2;255;99;71m-2.5144255\u001b[0m   \u001b[1;38;2;255;99;71m2.5638695\u001b[0m   \u001b[1;38;2;255;99;71m2.3361566\u001b[0m\n",
              "  \u001b[1;38;2;255;99;71m-3.0395977\u001b[0m   \u001b[1;38;2;255;99;71m3.0869865\u001b[0m \u001b[1;38;2;255;99;71m-11.08604\u001b[0m    \u001b[1;38;2;255;99;71m-8.8575735\u001b[0m\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>  <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">5.065693</span>    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">3.3688302</span>   <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">7.1048393</span>  <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">-4.116105</span>   <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">-4.375498</span>   <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">-5.261078</span>\n",
              "  <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">-3.7225776</span>   <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">3.1854541</span>   <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">1.7966672</span>  <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">-2.5144255</span>   <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">2.5638695</span>   <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">2.3361566</span>\n",
              "  <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">-3.0395977</span>   <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">3.0869865</span> <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">-11.08604</span>    <span style=\"color: #ff6347; text-decoration-color: #ff6347; font-weight: bold\">-8.8575735</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine scores with corresponding document IDs\n",
        "results_with_scores = [\n",
        "    (doc_id, hybrid_search_results[doc_id]['text'], score)\n",
        "    for doc_id, score in zip(hybrid_search_results.keys(), scores)\n",
        "]\n",
        "\n",
        "# Sort results by score in descending order and take the top 3\n",
        "top_results = sorted(results_with_scores, key=lambda x: x[2], reverse=True)[:3]"
      ],
      "metadata": {
        "id": "PM9eqbnyNLyK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from rich.table import Table\n",
        "table = Table(title=\"Top 3 Documents after Reranking\", show_lines=True)\n",
        "\n",
        "table.add_column(\"ID\", justify=\"right\", style=\"cyan\", no_wrap=True)\n",
        "table.add_column(\"Score\", justify=\"right\", style=\"green\", no_wrap=True)\n",
        "table.add_column(\"Document\", style=\"#e87d3e\")\n",
        "\n",
        "# Add rows to the table with top 3 results\n",
        "for doc_id, text, score in top_results:\n",
        "    table.add_row(str(doc_id), f\"{score:.4f}\", text)\n",
        "\n",
        "console.print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "6rkk_3umNNyk",
        "outputId": "7997f641-c3a9-4c14-8609-d43e4f42042f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                                          Top 3 Documents after Reranking                                          \u001b[0m\n",
              "┏━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mID\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m Score\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDocument                                                                                         \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m 2\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m7.1048\u001b[0m\u001b[32m \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mexpertsâ ) to process the token and combine their output additively. This technique increases the\u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mnumber of parameters of a model while controlling cost and latency, as the model only uses a     \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mfraction of the total set of parameters per token. Mixtral is pretrained with multilingual data  \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62musing a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B  \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mand GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62moutputs router expert                                                                            \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62m                                                                                                 \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mThis chunk describes the key architectural details of the Mixtral model, a sparse                \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mmixture-of-experts language model that outperforms larger models like Llama 2 70B and GPT-3.5 on \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mvarious benchmarks.                                                                              \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "├────┼────────┼───────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
              "│\u001b[36m \u001b[0m\u001b[36m15\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m5.0657\u001b[0m\u001b[32m \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62m3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey\u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mPerformance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62m16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mof Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task regardless of the     \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mlocation of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on   \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mthe proof-pile dataset decreases monotonically as the context length increases.                  \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62m                                                                                                 \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to\u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mretrieve a passkey regardless of its location in a long input sequence, and showing that the     \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mmodel's perplexity on the proof-pile dataset decreases as the context length increases.          \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "├────┼────────┼───────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
              "│\u001b[36m \u001b[0m\u001b[36m 4\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m3.3688\u001b[0m\u001b[32m \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mInstruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad   \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62maccessibility and potential for diverse applications. To enable the community to run Mixtral with\u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62ma fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mCUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on   \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62many instance in the cloud. # 2 Architectural details Mixtral is based on a transformer           \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62marchitecture [31] and uses the same modifications as described in [18], with the notable         \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mexceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed-     \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mforward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture    \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mparameters are summarized in Table 1.                                                            \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62m                                                                                                 \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mThis chunk describes the architectural details of the Mixtral language model, including its use  \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62mof a transformer architecture with a 32k token context length and mixture-of-expert layers. It   \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "│\u001b[36m    \u001b[0m│\u001b[32m        \u001b[0m│\u001b[38;2;232;125;62m \u001b[0m\u001b[38;2;232;125;62malso mentions the model's open-source licensing and deployment options.                          \u001b[0m\u001b[38;2;232;125;62m \u001b[0m│\n",
              "└────┴────────┴───────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                          Top 3 Documents after Reranking                                          </span>\n",
              "┏━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> ID </span>┃<span style=\"font-weight: bold\">  Score </span>┃<span style=\"font-weight: bold\"> Document                                                                                          </span>┃\n",
              "┡━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">  2 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 7.1048 </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> expertsâ ) to process the token and combine their output additively. This technique increases the </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> number of parameters of a model while controlling cost and latency, as the model only uses a      </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> outputs router expert                                                                             </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\">                                                                                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> This chunk describes the key architectural details of the Mixtral model, a sparse                 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> mixture-of-experts language model that outperforms larger models like Llama 2 70B and GPT-3.5 on  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> various benchmarks.                                                                               </span>│\n",
              "├────┼────────┼───────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> 15 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 5.0657 </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> 3.8 â Mixtral_8x7B 3.5 32 &gt; $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 &gt; 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task regardless of the      </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> the proof-pile dataset decreases monotonically as the context length increases.                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\">                                                                                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> The chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> retrieve a passkey regardless of its location in a long input sequence, and showing that the      </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> model's perplexity on the proof-pile dataset decreases as the context length increases.           </span>│\n",
              "├────┼────────┼───────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">  4 </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 3.3688 </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> accessibility and potential for diverse applications. To enable the community to run Mixtral with </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> any instance in the cloud. # 2 Architectural details Mixtral is based on a transformer            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> architecture [31] and uses the same modifications as described in [18], with the notable          </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed-      </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> parameters are summarized in Table 1.                                                             </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\">                                                                                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> This chunk describes the architectural details of the Mixtral language model, including its use   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> of a transformer architecture with a 32k token context length and mixture-of-expert layers. It    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│<span style=\"color: #008000; text-decoration-color: #008000\">        </span>│<span style=\"color: #e87d3e; text-decoration-color: #e87d3e\"> also mentions the model's open-source licensing and deployment options.                           </span>│\n",
              "└────┴────────┴───────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a variable to hold the search results for the generation model\n",
        "search_results = [doc[1] for doc in top_results]"
      ],
      "metadata": {
        "id": "Bx21GlLrNXaA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now time to connect to the large language model\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from rich.text import Text\n",
        "\n",
        "# Configure the Gemini API\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialize the Generative Model\n",
        "# You can choose a different model if needed\n",
        "gemini_model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "# Create the prompt for the Gemini model\n",
        "prompt = f\"\"\"You are chatbot, an research expert. Your top priority is to help guide users to understand research papers.\n",
        "\n",
        "Based on the following search results, answer the user's query:\n",
        "\n",
        "Search results:\n",
        "{search_results}\n",
        "\n",
        "User query:\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "# Generate the response using the Gemini model\n",
        "response = gemini_model.generate_content(prompt)\n",
        "response_text = Text(response.text)\n",
        "\n",
        "# Display the response (optional, you might want to print it directly)\n",
        "# console.print(response_text)"
      ],
      "metadata": {
        "id": "QInbvq5ANZuc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.panel import Panel\n",
        "\n",
        "panel = Panel(response_text, title=f\"Hybrid Search with Reranking Reply to \\\"{query}\\\"\")\n",
        "console.print(panel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "JWRktrnoNkAG",
        "outputId": "a81975fe-d9ce-4a41-f9e3-8c1721001e4a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────── Hybrid Search with Reranking Reply to \"What is context size of Mixtral?\" ────────────────────╮\n",
              "│ Mixtral has a context size of **32k tokens**.                                                                   │\n",
              "│                                                                                                                 │\n",
              "│ It is pretrained with this context size and supports a fully dense context length of 32k tokens.                │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────── Hybrid Search with Reranking Reply to \"What is context size of Mixtral?\" ────────────────────╮\n",
              "│ Mixtral has a context size of **32k tokens**.                                                                   │\n",
              "│                                                                                                                 │\n",
              "│ It is pretrained with this context size and supports a fully dense context length of 32k tokens.                │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}